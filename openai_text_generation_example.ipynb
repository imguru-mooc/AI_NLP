{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI 텍스트 생성(Text Generation) API 살펴보기"
      ],
      "metadata": {
        "id": "rNSR_6WVQbCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Reference : https://platform.openai.com/docs/api-reference/chat/create"
      ],
      "metadata": {
        "id": "JpsyZUrCWMPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpqGbdvvs_hh",
        "outputId": "e52bc073-14ec-49cc-c919-9d7f0ba3a129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.6.1)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Z18qebdtHkJ",
        "outputId": "f063a980-5c7c-4885-bcda-9571dba3fb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: openai\n",
            "Version: 1.6.1\n",
            "Summary: The official Python library for the openai API\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: OpenAI <support@openai.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: anyio, distro, httpx, pydantic, sniffio, tqdm, typing-extensions\n",
            "Required-by: llmx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. seed 파라미터를 이용해서 재현가능한 텍스트 생성 결과 만들기"
      ],
      "metadata": {
        "id": "RFRISYbZtxYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference : https://cookbook.openai.com/examples/deterministic_outputs_with_the_seed_parameter"
      ],
      "metadata": {
        "id": "OlYLLRvk5t-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI API Key 설정"
      ],
      "metadata": {
        "id": "oTZCcjSECPW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = \"여러분의_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "dQz-KdSl8Kpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_KEY)"
      ],
      "metadata": {
        "id": "B_XFc0vj5YiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import pprint\n",
        "import difflib\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-1106\""
      ],
      "metadata": {
        "id": "_fv-J-bZtI2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 함수는 GPT로부터 응답 결과를 받아옵니다.\n",
        "async def get_chat_response(system_message: str, user_request: str, seed: int = None):\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_request},\n",
        "        ]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=GPT_MODEL,\n",
        "            messages=messages,\n",
        "            seed=seed,\n",
        "            max_tokens=200,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "        response_content = response.choices[0].message.content\n",
        "        system_fingerprint = response.system_fingerprint\n",
        "        prompt_tokens = response.usage.prompt_tokens\n",
        "        completion_tokens = (\n",
        "            response.usage.total_tokens - response.usage.prompt_tokens\n",
        "        )\n",
        "\n",
        "        table = f\"\"\"\n",
        "        <table>\n",
        "        <tr><th>Response</th><td>{response_content}</td></tr>\n",
        "        <tr><th>System Fingerprint</th><td>{system_fingerprint}</td></tr>\n",
        "        <tr><th>Number of prompt tokens</th><td>{prompt_tokens}</td></tr>\n",
        "        <tr><th>Number of completion tokens</th><td>{completion_tokens}</td></tr>\n",
        "        </table>\n",
        "        \"\"\"\n",
        "        display(HTML(table))\n",
        "\n",
        "        return response_content\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mhGi_g4Nto-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 함수는 두 응답을 비교하고 차이점을 표로 표시합니다.\n",
        "# 삭제된 부분은 빨간색으로, 추가된 부분은 초록색으로 강조됩니다.\n",
        "# 차이점이 없는 경우 \"차이점이 발견되지 않았습니다.\"라고 출력합니다.\n",
        "def compare_responses(previous_response: str, response: str):\n",
        "    d = difflib.Differ()\n",
        "    diff = d.compare(previous_response.splitlines(), response.splitlines())\n",
        "\n",
        "    diff_table = \"<table>\"\n",
        "    diff_exists = False\n",
        "\n",
        "    for line in diff:\n",
        "        if line.startswith(\"- \"):\n",
        "            diff_table += f\"<tr style='color: red;'><td>{line}</td></tr>\"\n",
        "            diff_exists = True\n",
        "        elif line.startswith(\"+ \"):\n",
        "            diff_table += f\"<tr style='color: green;'><td>{line}</td></tr>\"\n",
        "            diff_exists = True\n",
        "        else:\n",
        "            diff_table += f\"<tr><td>{line}</td></tr>\"\n",
        "\n",
        "    diff_table += \"</table>\"\n",
        "\n",
        "    if diff_exists:\n",
        "        display(HTML(diff_table))\n",
        "    else:\n",
        "        print(\"No differences found.\")"
      ],
      "metadata": {
        "id": "ud-VqAAx97dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seed 파라미터 설정 없이 비교하기"
      ],
      "metadata": {
        "id": "SfrQk4O3-AEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = \"가을\"\n",
        "system_message = \"You are a helpful assistant that generates short poems.\"\n",
        "user_request = f\"{topic}에 관한 짧은 시를 작성해줘.\"\n",
        "\n",
        "previous_response = await get_chat_response(\n",
        "    system_message=system_message, user_request=user_request\n",
        ")\n",
        "\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, user_request=user_request\n",
        ")\n",
        "\n",
        "# 이 함수는 두 응답을 비교하고 차이점을 표로 표시합니다.\n",
        "# 삭제된 부분은 빨간색으로, 추가된 부분은 초록색으로 강조됩니다.\n",
        "# 차이점이 없는 경우 \"차이점이 발견되지 않았습니다.\"라고 출력합니다.\n",
        "compare_responses(previous_response, response)"
      ],
      "metadata": {
        "id": "G5Vo4z_TumLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "5323b830-3e5a-450c-b0e7-628c82fb655b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <table>\n",
              "        <tr><th>Response</th><td>가을이 오면\n",
              "단풍이 물들고\n",
              "바람은 시원해지고\n",
              "가을은 참 아름다워</td></tr>\n",
              "        <tr><th>System Fingerprint</th><td>fp_772e8125bb</td></tr>\n",
              "        <tr><th>Number of prompt tokens</th><td>37</td></tr>\n",
              "        <tr><th>Number of completion tokens</th><td>39</td></tr>\n",
              "        </table>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <table>\n",
              "        <tr><th>Response</th><td>가을이 왔네\n",
              "단풍잎이 떨어지네\n",
              "하늘은 맑고\n",
              "바람은 시원하고\n",
              "가을은 아름답네</td></tr>\n",
              "        <tr><th>System Fingerprint</th><td>fp_772e8125bb</td></tr>\n",
              "        <tr><th>Number of prompt tokens</th><td>37</td></tr>\n",
              "        <tr><th>Number of completion tokens</th><td>51</td></tr>\n",
              "        </table>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><tr style='color: red;'><td>- 가을이 오면</td></tr><tr style='color: red;'><td>- 단풍이 물들고</td></tr><tr style='color: green;'><td>+ 가을이 왔네</td></tr><tr style='color: green;'><td>+ 단풍잎이 떨어지네</td></tr><tr style='color: green;'><td>+ 하늘은 맑고</td></tr><tr style='color: red;'><td>- 바람은 시원해지고</td></tr><tr><td>?       ^^\n",
              "</td></tr><tr style='color: green;'><td>+ 바람은 시원하고</td></tr><tr><td>?       ^\n",
              "</td></tr><tr style='color: red;'><td>- 가을은 참 아름다워</td></tr><tr style='color: green;'><td>+ 가을은 아름답네</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## seed 파라미터 설정후 비교하기"
      ],
      "metadata": {
        "id": "FksKxmsq-FBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 123\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, seed=SEED, user_request=user_request\n",
        ")\n",
        "previous_response = response\n",
        "response = await get_chat_response(\n",
        "    system_message=system_message, seed=SEED, user_request=user_request\n",
        ")\n",
        "\n",
        "compare_responses(previous_response, response)"
      ],
      "metadata": {
        "id": "0lr7QSjRupZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "7dccb063-d5e7-4a38-f420-f9eb14597a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <table>\n",
              "        <tr><th>Response</th><td>산 너머 갈대숲에 바람 소리\n",
              "가을이 오는 소식을 전해주네\n",
              "단풍이 노랗게 물들고\n",
              "가을의 멋진 향기가 퍼져간다</td></tr>\n",
              "        <tr><th>System Fingerprint</th><td>fp_772e8125bb</td></tr>\n",
              "        <tr><th>Number of prompt tokens</th><td>37</td></tr>\n",
              "        <tr><th>Number of completion tokens</th><td>70</td></tr>\n",
              "        </table>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <table>\n",
              "        <tr><th>Response</th><td>산 너머 갈대숲에 바람 소리\n",
              "가을이 오는 소식을 전해주네\n",
              "단풍이 노랗게 물들고\n",
              "가을의 멋진 향기가 퍼져간다.</td></tr>\n",
              "        <tr><th>System Fingerprint</th><td>fp_772e8125bb</td></tr>\n",
              "        <tr><th>Number of prompt tokens</th><td>37</td></tr>\n",
              "        <tr><th>Number of completion tokens</th><td>71</td></tr>\n",
              "        </table>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table><tr><td>  산 너머 갈대숲에 바람 소리</td></tr><tr><td>  가을이 오는 소식을 전해주네</td></tr><tr><td>  단풍이 노랗게 물들고</td></tr><tr style='color: red;'><td>- 가을의 멋진 향기가 퍼져간다</td></tr><tr style='color: green;'><td>+ 가을의 멋진 향기가 퍼져간다.</td></tr><tr><td>?                +\n",
              "</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 토큰 개수 세기"
      ],
      "metadata": {
        "id": "dL_8HFWNZXHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference : https://platform.openai.com/docs/guides/text-generation/managing-tokens"
      ],
      "metadata": {
        "id": "UAmKUIc9Ruba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "B8pifE4ARNrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
        "      num_tokens = 0\n",
        "      for message in messages:\n",
        "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "          for key, value in message.items():\n",
        "              num_tokens += len(encoding.encode(value))\n",
        "              if key == \"name\":  # if there's a name, the role is omitted\n",
        "                  num_tokens += -1  # role is always required and always 1 token\n",
        "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "      return num_tokens\n",
        "  else:\n",
        "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "      See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
      ],
      "metadata": {
        "id": "Ja4u3RHfAJx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
        "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
        "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
        "  {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
        "  {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
        "  {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
        "]\n",
        "\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "\n",
        "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n",
        "# Should show ~126 total_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN0NV9-KAJva",
        "outputId": "01939fe3-e65b-4aba-84f7-849fa4a7f85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "126 prompt tokens counted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example token count from the OpenAI API\n",
        "response = client.chat.completions.create(\n",
        "  model=model,\n",
        "  messages=messages,\n",
        "  temperature=0,\n",
        ")\n",
        "\n",
        "print(f'{response.usage.prompt_tokens} prompt tokens used.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJdPxv80AJqG",
        "outputId": "3f7b2b27-ae3c-4a78-8bab-bf82f173c212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129 prompt tokens used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. JSON mode 사용해보기"
      ],
      "metadata": {
        "id": "mqk3TE_nU9dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHEyDyKIUeRB",
        "outputId": "58bd5ef1-ee3d-4476-ce29-31050fcbd4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"winner\": \"Los Angeles Dodgers\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 JSON 출력을 만드는 조수야. 아래 내용의 감정상태가 '긍정'인지 '부정'인지 판단해줘\"},\n",
        "    {\"role\": \"user\", \"content\": \"가격이 착하고 디자인이 예쁩니다\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrQatPq-UeOx",
        "outputId": "e46d4735-d893-4a0a-96a6-50e6b8cc8128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"감정상태\": \"긍정\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 JSON 출력을 만드는 조수야. 아래 내용의 감정상태가 '긍정'인지 '부정'인지 판단해줘\"},\n",
        "    {\"role\": \"user\", \"content\": \"금액이 저렴해도 기대를 했었는데 그 값어치밖에 안되는군요 그만큼만 입겠습니다\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYAhzzNsUeMN",
        "outputId": "b80bb964-4139-433a-be1b-ff084702654c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"감정상태\": \"부정\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Temperature 값 변경해보기"
      ],
      "metadata": {
        "id": "faI5EBGcYt-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## temperature : 설정가능범위(0.0~2.0, 기본값 1.0), 낮을수록 더 정확한 답변, 높을수록 더 다양성 있는 답변을 생성"
      ],
      "metadata": {
        "id": "G41R73hYpRWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"푸른 하늘에 OO이 떠있다. OO에 들어갈 단어를 10개 추천해줘\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZuwuNCnUeKD",
        "outputId": "83e9e2e1-637e-41f4-894f-14412f58125b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불꽃, 구름, 비행기, 이끼, 기억, 별, 햇볕, 구름, 비행기, 비둘기\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"푸른 하늘에 OO이 떠있다. OO에 들어갈 단어를 10개 추천해줘\"}\n",
        "  ],\n",
        "  temperature=0.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "FHG0DhIW5hrd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a301368-8e23-4e33-b7b3-30612dd8a45a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 구름\n",
            "2. 태양\n",
            "3. 달\n",
            "4. 비행기\n",
            "5. 구름\n",
            "6. 비\n",
            "7. 별\n",
            "8. 비둘기\n",
            "9. 구름\n",
            "10. 무지개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"푸른 하늘에 OO이 떠있다. OO에 들어갈 단어를 10개 추천해줘\"}\n",
        "  ],\n",
        "  temperature=2.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbNewCeLp7s8",
        "outputId": "2afe63b3-d568-421b-8071-35ea71e88f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "달, 비행기, 구름, 별, 태양, 비, 월, 구름, 비행 로켓, 핫물 갓Teacherarezendi\"?>\n",
            "MayorAPI AudioImage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. frequency_penalty & presence_penalty 값 변경해보기"
      ],
      "metadata": {
        "id": "TdCGopEDqc0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**presence_penalty** : -2.0과 2.0 사이의 숫자입니다. (기본값 0.0) 양수 값은 지금까지의 텍스트에 나타나는 **새로운 토큰을 기반으로 벌칙을 부여**하여 모델이 새로운 주제에 대해 이야기할 가능성을 높입니다.\n",
        "\n",
        "**frequency_penalty** : -2.0과 2.0 사이의 숫자입니다. (기본값 0.0) 양수 값은 현재까지 텍스트에서의 **존재 빈도를 기준으로 새 토큰에 대해 벌칙을 부여**하며, 모델이 동일한 문장을 그대로 반복할 가능성을 감소시킵니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "etrYqxwjraWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"가을 하면 생각나는 단어를 20개 추천해줘\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWHcZ6nmp9xk",
        "outputId": "970e14e1-3933-41ce-81cd-996928ba8b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. 단풍\n",
            "2. 가을 열대과일\n",
            "3. 호박\n",
            "4. 바람\n",
            "5. 가을 축제\n",
            "6. 단호박\n",
            "7. 감귤\n",
            "8. 비올 때\n",
            "9. 꽃\n",
            "10. 따뜻한 차\n",
            "11. 나무\n",
            "12. 달콤한 사과\n",
            "13. 단맛\n",
            "14. 포근한 담요\n",
            "15. 가을 햇볕\n",
            "16. 단단한 마룬\n",
            "17. 달 보름달\n",
            "18. 버섯\n",
            "19. 단콩\n",
            "20. 메론\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"가을 하면 생각나는 단어를 20개 추천해줘\"}\n",
        "  ],\n",
        "  presence_penalty=-2.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWc4efFTtHji",
        "outputId": "543a2282-4055-449f-b401-2a236c27f477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "물들기, 단풍, 시원한 바람, 담배 연기, 단단한 껍질, 햇볕, 바람, 담배, 햇볕, 단풍, 나뭇잎, 나뭇가지, 느끼한 공기, 고요한 밤, 시들은 꽃, 땀, 담배 연기, 시들은 잎, 바람소리, 단풍잎\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"가을 하면 생각나는 단어를 20개 추천해줘\"}\n",
        "  ],\n",
        "  presence_penalty=2.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUACEFAZtHg8",
        "outputId": "eb59d449-ea9b-4b35-a927-2d1bf28418b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "좋아, 여기 가을을 상징하는 단어 20가지야:\n",
            "\n",
            "1. 단풍\n",
            "2. 감자 수확\n",
            "3. 나무 잎\n",
            "4. 가을비\n",
            "5. 케이크\n",
            "6. 향긋한 차\n",
            "7. 바람\n",
            "8. 달\n",
            "9. 현미경\n",
            "10. 가을 축제\n",
            "11. 개구리 소리\n",
            "12. 호박\n",
            "13. 사과 농장\n",
            "14. 단단한 호두\n",
            "15. 고산윗독\n",
            "16. 나귀\n",
            "17. 커피\n",
            "18. 버터스카치\n",
            "19. 벙커 계덧망\n",
            "20. 읽는 시간\n",
            "\n",
            "이 추천이 도움이 되었기를 바라며, 다른 것이 필요하면 언제든 말해줘!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"가을 하면 생각나는 단어를 20개 추천해줘\"}\n",
        "  ],\n",
        "  frequency_penalty=-2.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0439dc9-b18b-455c-9156-8a6991db3f19",
        "id": "-S_tkF0zuJvc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "좋아요, 여기 몇 가지 가을을 상징하는 단어들이에요.\n",
            "1. 단풍\n",
            "2. 가을 단풍\n",
            "3. 가을풍경\n",
            "4. 가을풍\n",
            "5. 가을 가을\n",
            "6. 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가을 가\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"너는 도움이 되는 조수야.\"},\n",
        "    {\"role\": \"user\", \"content\": \"가을 하면 생각나는 단어를 20개 추천해줘\"}\n",
        "  ],\n",
        "  frequency_penalty=2.0\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106617a5-b387-44ac-85af-8815bbfc53f8",
        "id": "UpaGgySnuJvc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "좋아요, 여기 가을과 연관된 20가지 단어에 대한 추천입니다:\n",
            "\n",
            "1. 단풍\n",
            "2. 감자\n",
            "3. 산소\n",
            "4. 청명\n",
            "5. 열매\n",
            "6. 시원한 바람 \n",
            "7. 걷기 \n",
            "8. 학교 \n",
            "9 . 왁스 베리 그래나딘 ,\n",
            "10 . 수박,\n",
            "11 . 병충해 관리,\n",
            "12 . 참외,\n",
            "13 . 슬레이트 빛깔 ,\n",
            "14 굴뚝 ,   \n",
            "15 미 서터니ть ,   \n",
            "16 펄잡운공주의 이야기     \n",
            "17 악세사케     \n",
            "18 맥문갱 여사 경       \n",
            "19 선장    \n",
            "20 백합 식물\n",
            "\n",
            "여러분들이 좋아하는 가을의 특유의 분위기와 관련된 다른 단어가 있으면 추가해 주세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Osux9-WtHcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}