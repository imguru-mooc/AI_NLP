{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 랭체인(LangChain) Query Analysis Quickstart\n",
        "## 작성자 : AISchool ( http://aischool.ai/%ec%98%a8%eb%9d%bc%ec%9d%b8-%ea%b0%95%ec%9d%98-%ec%b9%b4%ed%85%8c%ea%b3%a0%eb%a6%ac/ )\n",
        "## Reference : https://python.langchain.com/docs/use_cases/query_analysis/quickstart/"
      ],
      "metadata": {
        "id": "dR6gqM5aJ2YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 페이지에서는 기본적인 end-to-end 예제를 통해 쿼리 분석 사용 방법을 보여줄 것입니다. 이것은 간단한 검색 엔진을 만드는 것을 포함하며, **사용자의 질문을 그대로 검색에 사용했을 때 발생하는 실패 모드**를 보여줄 것입니다. **그리고 나서 쿼리 분석이 그 문제를 어떻게 해결할 수 있는지에 대한 예제를 보여줄 것**입니다. 많은 다양한 쿼리 분석 기법들이 있으며, 이 end-to-end 예제에서는 모든 기법을 보여주지 않을 것입니다.\n",
        "\n",
        "이 예제의 목적을 위해, 우리는 LangChain YouTube 동영상들에 대한 검색을 수행할 것입니다."
      ],
      "metadata": {
        "id": "pLMgL-HrZr3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 설치"
      ],
      "metadata": {
        "id": "8dFf1LyAalCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-community langchain-openai youtube-transcript-api pytube langchain-chroma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QpupPqmjThV",
        "outputId": "4a3e58d0-64e2-4af2-cea4-3e69ce7f6e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Key 설정"
      ],
      "metadata": {
        "id": "2J_c-XcBVGSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = \"여러분의_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "Te8R0rpGv8zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load documents"
      ],
      "metadata": {
        "id": "9cA5jaqFk5HP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain 비디오 대본을 로드하는 데 YouTubeLoader를 사용할 수 있습니다:"
      ],
      "metadata": {
        "id": "pzdEbnQqbGR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "urls = [\n",
        "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
        "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
        "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
        "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
        "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
        "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
        "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
        "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
        "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
        "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
        "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
        "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
        "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
        "]\n",
        "docs = []\n",
        "for url in urls:\n",
        "    docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
      ],
      "metadata": {
        "id": "Yur7We8rk6we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfLEhrask6ti",
        "outputId": "e1a7b5a7-336b-43ea-a2ec-e616165a36e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be used with other models as as we'll see um and it's an exciting way to create a a GPT store like experience if you're building a more focused platform an internal platform or any of that so we launched this a few months ago actually right when uh open AI released their GPT store and but we haven't really dove into what's going on or how to use it um and so there's several things that I want to cover in this video there's maybe two main areas one I want to talk about it as an enduser facing application so how can you interact with we we have a we have a simple research preview hosted version of this um how can you interact what can you do what the functionality in the second half of the video I want to talk about how openg gpts is built um we do not intend to uh you know monetize openg gpts we want this to be a platform that people can clone and spin up their own versions of the GPD store or an internal platform or things like that so the second half of this video will focus more on how to build this platform and some of the considerations that we made please note that I am filming this on January 30th we are going to continue working on this continue changing it so if you are watching it later the hosted version or the code may have changed so starting with how to use it um the the the first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is using open aai tool calling under the hood so we can also ask it to look up multiple things and see multiple tool calls in parallel what about in LA and in New York so we can see that it now calls two things two things it C it call calls the tavil search Json for weather in Los Angeles the tavil search for weather in New York we can look at the responses this is for Los Angeles this is for New York um and then we can uh see the response here so this is an example of parallel tool calling that's enabled with open ai's most recent feature their their tool calling so this is the assistant it's using Tools in an arbitrary way to accomplish its task let's uh go create a new bot and now let's create one uh of type of rag so rag is really focused on retrieval over arbitrary files that you can upload so you can upload files and then the uh you can also give it custom instructions um and and then the bot will respond based on those files what is the difference between this and the assistant because in the assistant you could also upload files and you could choose retrieval as one of many to tools the main difference is that this is much more focused on answering questions specifically about files that you upload so this means that it will always look things up in a retriever it's actually hardcoded and we'll show this in in the um when we talk about the the back end but it's actually hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always doing a retrieval Step at the start and then it's actually always responding after that it's not doing potentially two retrieval steps it's not doing an iterative search this is a very simple rag architecture um which has its downsides it's not as flexible it can't handle multihop questions things like that but it's much more focused streamlined and that means it can work with simpler models as well so we have actually enabled mraw um through fireworks to work on uh this type of gbt a rag gbt so I have a uh I have a PDF here that I'm going to upload um this is Spade this is a paper Shrea recently wrote um at Berkeley and it goes over setting up kind of like a testing pipeline um Bas for for your prompts super interesting paper um I'd uh I'd recommend reading it regardless um let's upload the PDF um we can we can change the message slightly um let's still use 235 turbo research Spade let's save this now it's taking a little bit longer to save because what's going on under the hood is that it's injesting the file now it's in a method where it can be retrieves and and I'll talk about this when we talk about the back end as well um if let's take a look at this paper um and let's figure out something we can um ask what is a propped Delta so here it always uses retrieval the the it calls the retrieval function it gets back documents we format documents nicely um and you can see what these documents are and then it responds here so this is an example of a simple rag bot which always does retrieval hyperfocused on rag if you want to ground a bot in some external data source that you can upload this is probably the simplest and most reliable way to do that again it's a little bit less trustworthy than uh than than the chat bot we have it as a separate type of Bot because it is simpler so that means that it can work with other models like mraw which is an open source model so it just provides more flexibility and that flexibility is the same reason we have a third type of Bot this chat bot this is just solely parameterized by the instructions so you can write out long complicated instructions for how it should behave you can give it a character and it can act like that again because this is simpler it can work with simpler models let's create an example chatbot we'll create one that responds like a pirate so you are a helpful pirate always respond in a pirate tone pirate save this hi and we get back a response and pirate so a lot of gpts in the GPT store are really just complicated system prompts and so for those you can create them using using this chatbot type the other a lot of the other gbts that I've seen at least are the rag style chat Bots where they're parameterized by a system prompt and then also um and then also a bunch of uh files that you can upload to give it information besides what it knows about and it can search over those and so these you know these are much simpler architectures than the assistant but for a majority of use cases they're actually completely fine the nice thing about assistance is you can do more complicated with things things with it and you can also equip it with arbitrary tools and so here there's a bunch of tools that we've enabled in the back end by default but you can easily add your own and and explore with those um and so that's part of the power of this platform being open source as well you can Fork it you can make it your own you can deploy it either to end users internal company users anything like that other things that I want to highlight in the front end um you can make Bots public this means that you can share links um you can see old conversations and jump back in um you can create new chats um when you're in a conversation you can click in here and see the bot that it is using um when you create a new chat you can look at the saved Bots that you have if you have any public Bots they'll be down here as well and yeah that's basically it for an overview of the front end I'm now going to switch to talk about some of the architecture of the back end which will be really helpful if you want to Fork this and make it your own so this is the openg gpts repo it's under the linkchain org um there's some instructions here there's a good read me on everything that's in here there's some Docker composed files for deploying it um there's some other uh uh files for environment variables the API docs things like that for this I really want to focus on the back end so we can take a look at what's going going on in here um and most of the logic here there's there's some requirements files most of the logic here is going to be in app and so we can see there's a bunch of different files here so there's a few things that I want to draw attention to first let's uh let's maybe look at agent types so when we talk about the assistant in the in the uh in the front end this is where these agent types are defined and so different assistants have different architectures that are going on behind the hood let's take a look at the open AI agent for example so important to note this is built on top of L graph so if you aren't familiar with L graph you should definitely go check it out it's a really easy way to build these types of cyclical agentic Frameworks so we have this open AI agent executor which takes in a list of tools an llm a system message and then a checkpoint um and we'll see how we use this so first um we're going to create basically this uh quote unquote agent and this agent is responsible for taking in messages and deciding what to do next so there's first a step where we format um the messages um and so we add a system message um and that's defined up here and then we pass in the rest of the messages the llm also then has access to the tools so we bind it with tools and then we combine it using this pipe syntax to get this agent we next Define a tool executor this is a class that is just does some minor boiler plate for calling tools um and then we start to define the different nodes of the graph so first we Define the function that determines whether to continue or not this is should continue it looks at the messages if there's no tool calls then it finishes if there is tool calls then it will continue and we'll see how we'll use this later on We Now define the node that calls the tools um so here uh we uh take in the list of messages if we we get the last messages we get the last message we know that it involves a function call um because otherwise we would have ended um we get all the tool calls so when there's multiple tool calls we get them all we then pass them in uh into here into the tool executor in a batch so it runs them in parallel um and then we append them to the the messages and we return the messages from this node so importantly this will use a message graph and so this means that every node in the graph should return a message or a list of message so here we return a list of messages the other node that we add the agent this is just this is basically an llm call the LM call returns a message so both of the nodes return messages we set the entry point to the agent so when anything comes in we go to the agent we then add a conditional Edge so after the agent is called we then check this should continue function if uh it says continue then we call the action node otherwise we finish um we add an edge so after we call the tools in the action node we go back to the agent we then compile it compiling we're passing in check pointer equals checkpoint this is basically a way to persist um the state of the graph so we're persisting all the messages that that happen so this is nice for a few reasons um the main immediate way that we are using it right now is we are saving it to reddis and then we're showing that in the front end so in the front end when you see that we save the chat history um those are pulling from reddis the way that is getting saved to redus we don't have separate functions saving everything we just pass in this checkpoint and it all kind of gets written there there's similar things for the Google agent Google agent looks very similar there's some minor differences uh because it uh uh is a a Google agent so it's a little bit different it doesn't have tool calling it has function calling there's also an XML agent designed to work with anthropic models and so this is different as well so it uses some of the prompts um and things like that so those are the agent types um we also have a uh really simple executor for the chatbot chatbot just calls the message once with the system message so it has a really simple node the chatbot node just calls it and then ends dead simple um but we use the message graph again uh so that it it all of these Bots can speak kind of like on the same kind of like State um which will make it nice if we want to do any multi-agent or multibot things in the future um and then we also have this retrieval bot so this retrieval bot um basically it it's it had it's simpler than the agent node um so it doesn't have any Loops but it's more complex than the chatbot node so we have this prompt template um this is used for coming up with a search query to pass to the retrieval um and so we can see that we have the conversation here and then we generate the search query um and then the response pop template takes in instructions and then has context so what's going on here is that we have this get messages function um and basically what's going to happen is we're passing all the state around as messages um and so part of that um has the the search query involved and so we can see here if we scroll down I'm going to scroll down to this graph we F we have this invoke retrieval uh node we have this retrieve node and then we have this response node and then we always invoke retrieval at the start and then we go from invoke retrieval to retrieve and then we go from retrieve to response and then we end so remember how I said the difference one of the differences with the retrieval with the rag bot was that it always did retrieval this is this is what's happening so the invoke retrieval node it's always going to return an AI message that calls retrieval so we're not even actually calling the language model sometimes so if the length of the messages is one this means if it's the first message in the conversation we're just going to look up whatever that first thing was um so this is a little cheat that a lot of rag based systems or conversational rag Bas systems do is the first time someone types in something in we just look up that input the issue start starts to happen when you have a conversation so if I have a follow-up question or you know a series of follow-up questions I don't really just want to pass that follow-up question in because it could be referencing things previously and so what I do instead is I call um this other method um which is itself a call to a language model um and so this is using the search prompt to generate a search query um and then I specify that as the the retrieval thing then the retrieval thing is just calling the retrieval it's passing the results in this function message um and then the response is just a call to the language model with some formatting into this prompt so if we see this get messages thing what we're doing is we're getting the uh most recent message which is the result of calling this tool we know that it will always be that because we have this this determined graph we're getting the response from that and we're we're formatting that into the system message um and yeah so basically Al we're constructing the chat history the chat history is going to be AI messages that do not have function calls if they do have function calls then they are the result of retrieval steps and we don't want to include those in the messages that we pass to the final LM um and then uh the chat history also includes human messages um and then it also includes the system message system message is where we use the system message prompt as well as the context that we retrieve from the documents so that is the so those are the three types of um Bots that we have they're all put together in this agent file and this is where we also start to use um uh uh configuration basically so configurable fields and configurable alternatives are something that exists in linkchain and they're really handy when you want end users to be able to configure things or sometimes when you want to do the configuration on the Fly for example if you want to randomly select a model to use and you want to configure that on runtime and basically the way that that looks like and the way that we've implemented it here um is that we have this idea of like a configurable agent that wraps around a binding and there's these different parameters on here inside the initialization we take in the parameters and we construct the agent um and then we pass it in and then what we'll do down below and I'll return to this later on what we do about down below is we initialize this configurable agent and then we Mark certain Fields as configurable so agent field here is configurable with an agent type um the system message field is configurable with a system message um the assistant ID this is the assistant ID of the um of the bot that you've selected the tools are configurable the retrieval descriptions configurable and so a lot of the uh fields that we create the bot with are configurable and those are exactly what we expose in the front end of uh open gpts we've also exposed some configurable Alternatives and these are the different architectures so uh there's um there's a chatbot which follows the chatbot architecture and there's the chat retrieval which follows the rag bot architecture and if we scroll up we can see that chat retrieval uses this idea of configurable retrieval um so this is the same kind of like runable binding it's got these same parameters we Mark these fields as configurable um and then chatbots exactly the same and so basically the difference between the fields these are things that go in again we have like three seate types of high Lev Bots the configurable fields are things to configure this specific the the assistant type of Bot and then the alternatives are completely different kind of like Alternatives that you could even use so there's two different ways that you can configure things fields and Alternatives one more thing I want to highlight is just the ingestion bit um so there's the ingest pipeline um which is really quite simple this is something we're going to look to expand on in the future um and so if if you want to help make this uh retrieval more advanced would love that but basically we just split documents um and then we add them to a simple Vector store um and then the retrieval um is here part of tools I believe um and we can see here that it's just a uh retriever um really simple um really simple and then we just filter based on the name space so uh based on the assistant that you're using the assistant only has access to the files that were uploaded to it um so there's a lot that we can do to improve this we can add a reranking step um there's already some sort of query transformation going on based on the Bots um but we can add more things um and so improving the retrieval is one aspect that we want to lean into in the future um the last thing I want to point out is that this all integrates with Lang Smith so if you're a little bit lost about what the different types of agents are what exactly is happening uh uh what got configured um that's totally normal these applications start to get really really complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on different bots in the future um so there's a lot of things we want to do in the future one of those is having like multiple bots on the same thread or allowing you to switch Bots between threads um and so having this common state representation will make it easy to do that that's pretty much all I got hope you guys enjoyed this we're really excited about open gpts if you want to use this I mean one feel free to Fork it but also feel free to reach out to us we're more than happy to help thanks\", metadata={'source': 'HAn9vnJy6S4', 'title': 'OpenGPTs', 'description': 'Unknown', 'view_count': 8189, 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg', 'publish_date': '2024-01-31 00:00:00', 'length': 1530, 'author': 'LangChain'})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZfsKapYmMDP",
        "outputId": "6e50436a-9b6e-422b-e6fd-f0948beac01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'HAn9vnJy6S4',\n",
              " 'title': 'OpenGPTs',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 8189,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
              " 'publish_date': '2024-01-31 00:00:00',\n",
              " 'length': 1530,\n",
              " 'author': 'LangChain'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Add some additional metadata: what year the video was published\n",
        "for doc in docs:\n",
        "    doc.metadata[\"publish_year\"] = int(\n",
        "        datetime.datetime.strptime(\n",
        "            doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\"\n",
        "        ).strftime(\"%Y\")\n",
        "    )"
      ],
      "metadata": {
        "id": "1uwrDPdBmEXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[doc.metadata[\"title\"] for doc in docs]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuLDZ_54mEVT",
        "outputId": "cc709745-14e5-43a9-fdbb-36d880aeea0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['OpenGPTs',\n",
              " 'Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve',\n",
              " 'Streaming Events: Introducing a new `stream_events` method',\n",
              " 'LangGraph: Multi-Agent Workflows',\n",
              " 'Build and Deploy a RAG app with Pinecone Serverless',\n",
              " 'Auto-Prompt Builder (with Hosted LangServe)',\n",
              " 'Build a Full Stack RAG App With TypeScript',\n",
              " 'Getting Started with Multi-Modal LLMs',\n",
              " 'SQL Research Assistant',\n",
              " 'Skeleton-of-Thought: Building a New Template from Scratch',\n",
              " 'Benchmarking RAG over LangChain Docs',\n",
              " 'Building a Research Assistant from Scratch',\n",
              " 'LangServe and LangChain Templates Webinar']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6425-TxQmESf",
        "outputId": "f32e20e3-90fb-4420-9925-a11ced9ff1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': 'HAn9vnJy6S4',\n",
              " 'title': 'OpenGPTs',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 8189,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg',\n",
              " 'publish_date': '2024-01-31 00:00:00',\n",
              " 'length': 1530,\n",
              " 'author': 'LangChain',\n",
              " 'publish_year': 2024}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lqZnMuCQmEPn",
        "outputId": "fe0a4374-c2b3-4630-8ff1-f169d8574259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be us\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing documents\n"
      ],
      "metadata": {
        "id": "G6Rg3NBGyllH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "검색(retrieval)을 수행할 때는 쿼리할 수 있는 문서의 인덱스를 생성해야 합니다. 문서를 인덱스하기 위해 벡터 스토어를 사용할 것이며, 검색을 더 간결하고 정확하게 만들기 위해 먼저 문서를 청크로 나눌 것입니다:"
      ],
      "metadata": {
        "id": "l7CtaXKAfbA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "chunked_docs = text_splitter.split_documents(docs)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=OPENAI_KEY)\n",
        "vectorstore = Chroma.from_documents(\n",
        "    chunked_docs,\n",
        "    embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "Z1PP3iJhmpuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval without query analysis"
      ],
      "metadata": {
        "id": "lRr-zSiVy0MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자 질문에 직접 유사성 검색(similarity search)을 수행하여 질문과 관련된 청크를 찾을 수 있습니다:"
      ],
      "metadata": {
        "id": "xTgxN-TjgKp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = vectorstore.similarity_search(\"how do I build a RAG agent\")\n",
        "print(search_results[0].metadata[\"title\"])\n",
        "print(search_results[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnSd0uuWmpst",
        "outputId": "4f46d0fe-8d1e-41ac-9547-7760b7038e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenGPTs\n",
            "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특정 시간대의 결과를 검색하고 싶다면 어떻게 할까요?"
      ],
      "metadata": {
        "id": "zJIoy_f5g4hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_results = vectorstore.similarity_search(\"videos on RAG published in 2023\")\n",
        "print(search_results[0].metadata[\"title\"])\n",
        "print(search_results[0].metadata[\"publish_date\"])\n",
        "print(search_results[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "974-F7QimpqY",
        "outputId": "dbfe7b25-29d9-48cb-a1d6-d7587d2b076f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenGPTs\n",
            "2024-01-31 00:00:00\n",
            "hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가장 유사한 문서로 찾은 첫 번째 결과는 2024년 것입니다(우리가 2023년 비디오를 요청했음에도 불구하고)**, 우리가 **문서 내용에 대해서만 검색하기 때문에, 결과를 문서 속성에 따라 필터링하는 방법이 없습니다**.\n",
        "\n",
        "이것은 **발생할 수 있는 실패 모드 중 하나입니다. 이제 기본적인 형태의 쿼리 분석(Query Analysis)이 이 문제를 어떻게 해결할 수 있는지 살펴보겠습니다!**"
      ],
      "metadata": {
        "id": "bAclVJUMhUHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query analysis\n"
      ],
      "metadata": {
        "id": "sDAO9QYPzB7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "쿼리 분석(query analysis)을 사용하여 검색 결과를 개선할 수 있습니다. 이것은 **일부 날짜 필터를 포함하는 쿼리 스키마(query schema)를 정의하고 사용자 질문을 구조화된 쿼리로 변환하는 함수 호출 모델(function-calling model)을 사용**하는 것을 포함합니다."
      ],
      "metadata": {
        "id": "yi2a8YQ8jV34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query schema"
      ],
      "metadata": {
        "id": "qfYT17PszcQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 경우에는 **출판 날짜(publication date)에 대해 필터링**할 수 있도록 명시적인 최소 및 최대 속성을 가지게 됩니다."
      ],
      "metadata": {
        "id": "UY6LNUBelgR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "class Search(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    publish_year: Optional[int] = Field(None, description=\"Year video was published\")"
      ],
      "metadata": {
        "id": "FI7PiIHxmpn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query generation"
      ],
      "metadata": {
        "id": "5RcZxxfCziSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**사용자 질문을 구조화된 쿼리로 변환하기 위해 OpenAI의 도구 호출 API(OpenAI’s tool-calling API)를 사용할 것**입니다. 구체적으로는 **모델에 스키마를 전달하고 출력을 파싱하는 것을 처리하는 새로운 ChatModel.with_structured_output() 생성자를 사용**할 것입니다."
      ],
      "metadata": {
        "id": "-EWE-pQ8mNMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a list of database queries optimized to retrieve the most relevant results.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, openai_api_key=OPENAI_KEY)\n",
        "structured_llm = llm.with_structured_output(Search)\n",
        "query_analyzer = {\"question\": RunnablePassthrough()} | prompt | structured_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij8TPUWVziBN",
        "outputId": "a97858b2-a957-4e37-b8e6-332a7f96eb10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리가 이전에 검색했던 질문들에 대해 분석기가 어떤 쿼리를 생성하는지 살펴봅시다:"
      ],
      "metadata": {
        "id": "EcDpC0IKogng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"how do I build a RAG agent\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x2iBMNgmENB",
        "outputId": "7bc18f2f-1153-430c-97f9-144446abf0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='build RAG agent', publish_year=None)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_analyzer.invoke(\"videos on RAG published in 2023\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1_GmfsOmEKp",
        "outputId": "3d7148a6-78ab-4e3d-e2d2-50906396a521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Search(query='RAG', publish_year=2023)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval with query analysis"
      ],
      "metadata": {
        "id": "77UL-aC6z2bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "우리의 쿼리 분석 결과가 꽤 좋아 보입니다; 이제 생성된 쿼리를 사용하여 실제로 검색을 수행해 봅시다.\n",
        "\n",
        "참고: 우리의 예제에서는 **tool_choice=\"Search\"를 지정했습니다. 이는 LLM이 하나의 도구만 호출하도록 강제하는 것으로, 항상 최적화된 하나의 쿼리를 조회할 수 있다는 의미**입니다. 주의할 점은 이것이 항상 그런 것은 아니라는 것입니다 - 최적화된 쿼리가 반환되지 않거나 여러 개가 반환되는 상황을 다루는 방법에 대해서는 다른 가이드를 참조하세요."
      ],
      "metadata": {
        "id": "eq6LV6nkozuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "IPqkf9pGz01d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieval(search: Search) -> List[Document]:\n",
        "    if search.publish_year is not None:\n",
        "        # This is syntax specific to Chroma,\n",
        "        # the vector database we are using.\n",
        "        _filter = {\"publish_year\": {\"$eq\": search.publish_year}}\n",
        "    else:\n",
        "        _filter = None\n",
        "    return vectorstore.similarity_search(search.query, filter=_filter)"
      ],
      "metadata": {
        "id": "d3XKcIYtz0zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain = query_analyzer | retrieval"
      ],
      "metadata": {
        "id": "kcKFDu7Kz8tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYP9IOtwqQyr",
        "outputId": "88fe5dc7-2179-440b-94dc-d419a008f57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "  question: RunnablePassthrough()\n",
              "}\n",
              "| ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at converting user questions into database queries. You have access to a database of tutorial videos about a software library for building LLM-powered applications. Given a question, return a list of database queries optimized to retrieve the most relevant results.\\n\\nIf there are acronyms or words you are not familiar with, do not try to rephrase them.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
              "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7d68208669e0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7d68208649d0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'Search', 'description': 'Search over a database of tutorial videos about a software library.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'Similarity search query applied to video transcripts.', 'type': 'string'}, 'publish_year': {'description': 'Year video was published', 'type': 'integer'}}, 'required': ['query']}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'Search'}}})\n",
              "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.Search'>])\n",
              "| RunnableLambda(retrieval)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 **이전에 문제가 있었던 입력에 대해 이 체인을 실행할 수 있으며, 그 해의 결과만 나타나는 것을 볼 수 있습니다!**"
      ],
      "metadata": {
        "id": "RSdBf4Bfqluw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = retrieval_chain.invoke(\"RAG tutorial published in 2023\")"
      ],
      "metadata": {
        "id": "krk5XeWTz8qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[(doc.metadata[\"title\"], doc.metadata[\"publish_date\"]) for doc in results]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wzsXMXYz0wo",
        "outputId": "96ce5d2f-ffe9-4414-9d32-abd91da56da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'),\n",
              " ('LangServe and LangChain Templates Webinar', '2023-11-02 00:00:00'),\n",
              " ('Getting Started with Multi-Modal LLMs', '2023-12-20 00:00:00'),\n",
              " ('Building a Research Assistant from Scratch', '2023-11-16 00:00:00')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDZ7SiwY0BRe",
        "outputId": "e904d9b4-6a52-4843-f233-64c33c5ed791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"capacity and conventional rag approaches that just strip the text out really miss a lot of this so let's try kind of how could we build a rag system over the visual content in in a slide deck um so to start off what I did was I took a slide deck and this is um uh data dog's Q3 earnings report I randomly chose it you know it was just like an interesting demonstration of like kind of complex uh you know financial information and figures and slide deck and I created a set of 10 questions and answer pairs about these slides this is like my evalve set um and this is really easy to do I can just create a CSV that has like my question and my answer in this case like my input output pairs um and it's just a set of questions that I devised myself I looked at the slides I said okay here's some interesting question answer pairs I put them in a CSV and I load these into Langs Smith now Langs Smith is Lang chain platform that supports durability and evaluations um and I create a data set for myself in Lang Smith and there's some links down here that show exactly how to do that but that's my starting point so I say okay here's my evaluation set I have the slide deck I built 10 question answer pairs from the slides now let's compare some approaches there might be two different ways to think about multimodal rag um so one is this notion of multimodal embeddings so we take our slides we extract them as images in every image we use multimodal embeddings to map them into this kind of this embedding space that is common between kind of text and and images um for that I use open clip embeddings um and so I now have an index in this case I use chroma that contains a bunch of images uh that have been embedded using open clip um at retrieval time I ask a question I use I basically take the natural language question embed it indeed with multimodal embeddings same ones similarity search just like normal retrieve images that are similar to my question pass the image to in this case uh my\", metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1833, 'publish_date': '2023-12-20 00:00:00', 'publish_year': 2023, 'source': '28lC4fqukoc', 'thumbnail_url': 'https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ', 'title': 'Getting Started with Multi-Modal LLMs', 'view_count': 3720}),\n",
              " Document(page_content=\"uh context for it um instead of uh instead of asking a question so here we can just add our second route so now we're going to have um two different sets of endpoints um and I can actually just show that off in the uh fast API doc so if I refresh this we'll see now that we have all the invoke batch stream and stream log uh calls for rag conversation which was the first example that we went over or first template that we went over um and we also now have these extraction open AI functions ones um which just taken a single string instead of both like a chat history and a question and so if we go to our playground um so this is going to be our rag conversation playground um but we can go to extraction openingi functions um and we're adding a little index so it's easier to to get to these links um so in this case if we look at the readme for extraction open AI functions um what this is going to do is it's going to um extract the title and author of papers um which uh we'll look at in a sec and we'll we'll try and customize it to extract something else um but we can actually just use the same article over here um just because it also has paper ERS and authors um so if we just paste in some section of this um we can see that it's able not reminders um we can see that it's able to extract out um those authors and papers that are kind of covered in this Tas do composition section um and let's actually go into that template um to see why it's it's doing just papers um in instead um so here we can see that we have just a prompt going into a model uh which has kind of some open AI functions um set on it and then we'll talk a little bit how about how we can design those ourselves um and then in the end it's just going to Output that papers key um which is just going to be a list of papers according to our kind of pedantic model here um and then we can see that it's extracting title an author because we um Define those as as the fields to extract so let's say um I don't know\", metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 2441, 'publish_date': '2023-11-02 00:00:00', 'publish_year': 2023, 'source': 'o7C9ld6Ln-M', 'thumbnail_url': 'https://i.ytimg.com/vi/o7C9ld6Ln-M/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDf7gvV8D3I2UFy0UsA2Wh0qUhA-A', 'title': 'LangServe and LangChain Templates Webinar', 'view_count': 4906}),\n",
              " Document(page_content=\"and reason about what's going on so so that's maybe like a simple like mental model how to think about what's happening when you work with multimodal LMS um yeah let's talk a about use cases so Greg Cameron on Twitter kind of had this kind of nice visualization of a bunch of things that have been been shown with GPD 4V um a lot of people seen really cool demos with image captioning um extractions a really good one taking an image extracting elements text elements and so forth um recommendations so there's kind of like a lot of design applications um kind of suggestions about how to improve the visual Aesthetics of a scene of a of a of of like you know um of an object um and of course like interpretation this is like you know common in the rag context for example if you have like a you know collection of say we'll talk a little bit later to it a little bit later about slides um or about diagrams in documents you can of course use a vision model to reason about what's happening there in a question answer context um and this was like an intering demonstration of of extraction uh shown in the in the gbd uh 4V paper here uh actually this is a follow on to the GPD 4V model by Microsoft showing here are some interesting um explorations and they they talked about kind of extraction from complex documents um so let's actually walk through a demo to make this a little bit more concrete and I'll share kind of a bunch of code and and templates that can be easily reused later um so I think you know presentations like slide decks are a really good application for vision models because they're inherently kind of visual they have lots of kind of complex visual elements like like graphs uh tables figures and they're very common you know every nearly every organization uses slides in some capacity and conventional rag approaches that just strip the text out really miss a lot of this so let's try kind of how could we build a rag system over the visual content in in a slide deck um so\", metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 1833, 'publish_date': '2023-12-20 00:00:00', 'publish_year': 2023, 'source': '28lC4fqukoc', 'thumbnail_url': 'https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ', 'title': 'Getting Started with Multi-Modal LLMs', 'view_count': 3720}),\n",
              " Document(page_content=\"this is the main thing I want to serve the thing that I copy has a bunch of extraneous things we can easily remove that um we can now do add routes app chain this is all useless stuff from before let's call This research assistant um and then we can do if we do this install SEC Starlet that's an easy fix no need to run that twice query run main install unicorn burun may now we can go here and we can add in research assistant playground and so now we get this thing what is the difference between L chain let's change it up what's the difference between L chain and open AI so this is a nice little this is all autogenerated we know that the input's question because we we know the internals of the chain that we wrote you can see the intermediate steps streams things automatically um Lang chain and open a are two prominent entities in the sphere each offering unique Frameworks and models so not exactly right we don't offer any models okay here we go open the eye provider of Link language models um Lang chain is a framework for language model applications cool so it gets those right general purpose versus chat focused um okay so it talks about the two different classes in Lang chain talks about our Integrations um developer platform um and conclusion and so we get a bunch of sources as well so that's pretty much it for this video um I'll post the code for this um in a in a simple gist or something um I'll also post uh the code for a more complex uh research assistant um oh let's maybe do one last thing let's maybe change this so instead of scraping the web it's using a different retriever of our choice and this is really interesting because uh you can now change it to do to do research over any corporate of data that you want so we'll change it we'll do some research over uh let's do some research over um over archive data all right so I've done some basic setup I've imported the archive retriever from L chain and I've got uh I've created the retriever class here what\", metadata={'author': 'LangChain', 'description': 'Unknown', 'length': 2620, 'publish_date': '2023-11-16 00:00:00', 'publish_year': 2023, 'source': 'DjuXACWYkkU', 'thumbnail_url': 'https://i.ytimg.com/vi/DjuXACWYkkU/hq720.jpg', 'title': 'Building a Research Assistant from Scratch', 'view_count': 18580})]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOVlSrg70BPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehnzF6Zn0Auh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlWOLhKJ0AsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCdtsowr0ApY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}