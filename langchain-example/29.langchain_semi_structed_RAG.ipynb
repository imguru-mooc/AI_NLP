{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9606b2062ac142a489ec2b04a303e119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4b19bafa7124b5b992d4b05b13d5aec",
              "IPY_MODEL_8a2d2aa5da9a41e0aa46bd251ec2e824",
              "IPY_MODEL_15c8ef041c214a69b73627a882c4d778"
            ],
            "layout": "IPY_MODEL_abd29f1409ec491c9a10665cf8002105"
          }
        },
        "e4b19bafa7124b5b992d4b05b13d5aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2970027c94354189ab7de6b5a22500db",
            "placeholder": "​",
            "style": "IPY_MODEL_8b396514f7714d4f8b805c08f2cd7eb2",
            "value": "config.json: 100%"
          }
        },
        "8a2d2aa5da9a41e0aa46bd251ec2e824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4e136b9d61a4d4db133ea93967121cd",
            "max": 1469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bde184577cf4ce9911f680cb455769a",
            "value": 1469
          }
        },
        "15c8ef041c214a69b73627a882c4d778": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b22e04d6542548d792ff0ebf39c0cccc",
            "placeholder": "​",
            "style": "IPY_MODEL_57b27d4e421d49d0bb01c7c9d5689f97",
            "value": " 1.47k/1.47k [00:00&lt;00:00, 83.6kB/s]"
          }
        },
        "abd29f1409ec491c9a10665cf8002105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2970027c94354189ab7de6b5a22500db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b396514f7714d4f8b805c08f2cd7eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e136b9d61a4d4db133ea93967121cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bde184577cf4ce9911f680cb455769a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b22e04d6542548d792ff0ebf39c0cccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b27d4e421d49d0bb01c7c9d5689f97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "659de5be65054832a2dbf780a674317f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_603db6d78b4c4d35b17007c86edabd4a",
              "IPY_MODEL_1082b235ef4a4366835712a5f752a576",
              "IPY_MODEL_25ca5f8e228049518123b629acde9f9c"
            ],
            "layout": "IPY_MODEL_4815789dfea3435793441cfcc6171157"
          }
        },
        "603db6d78b4c4d35b17007c86edabd4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a230aa17cc3d4d778443ec55b8d18e34",
            "placeholder": "​",
            "style": "IPY_MODEL_8f5488ff5af34e6f9f9c17329a5523fa",
            "value": "model.safetensors: 100%"
          }
        },
        "1082b235ef4a4366835712a5f752a576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b15adb50e1e4bb88b0ee79a4ee5b9a1",
            "max": 115434268,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd3c2f197f4f4e7dbec5774f3a000af2",
            "value": 115434268
          }
        },
        "25ca5f8e228049518123b629acde9f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d58fe7703548969151f5e9cda05159",
            "placeholder": "​",
            "style": "IPY_MODEL_ee7b026ddf134cb99afd84c6a29bcf9b",
            "value": " 115M/115M [00:00&lt;00:00, 242MB/s]"
          }
        },
        "4815789dfea3435793441cfcc6171157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a230aa17cc3d4d778443ec55b8d18e34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5488ff5af34e6f9f9c17329a5523fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b15adb50e1e4bb88b0ee79a4ee5b9a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3c2f197f4f4e7dbec5774f3a000af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66d58fe7703548969151f5e9cda05159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee7b026ddf134cb99afd84c6a29bcf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5496e9b72b574313a8c076756176c86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfea25ce1fdc445ea6a333bf83cce607",
              "IPY_MODEL_6d742190ffd846f4ac3762d8df15de3c",
              "IPY_MODEL_71259ddb61084f988932a2030dfc7363"
            ],
            "layout": "IPY_MODEL_8cab9cc08ce6425babd9c7425c97a20b"
          }
        },
        "bfea25ce1fdc445ea6a333bf83cce607": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d6646f615384b1ca055386116b1a3ed",
            "placeholder": "​",
            "style": "IPY_MODEL_393fda27fc604ba1b55ba84c3479722c",
            "value": "model.safetensors: 100%"
          }
        },
        "6d742190ffd846f4ac3762d8df15de3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a38242828114fbfad1d217f55fb89ff",
            "max": 46807446,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7dfc5c3242ff4a26ba31a091478cb7c4",
            "value": 46807446
          }
        },
        "71259ddb61084f988932a2030dfc7363": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be40d098058b44a7979dc83286b91519",
            "placeholder": "​",
            "style": "IPY_MODEL_2c401367e4b047a597d4505fc90bc6fe",
            "value": " 46.8M/46.8M [00:01&lt;00:00, 31.5MB/s]"
          }
        },
        "8cab9cc08ce6425babd9c7425c97a20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6646f615384b1ca055386116b1a3ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "393fda27fc604ba1b55ba84c3479722c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a38242828114fbfad1d217f55fb89ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dfc5c3242ff4a26ba31a091478cb7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be40d098058b44a7979dc83286b91519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c401367e4b047a597d4505fc90bc6fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 랭체인(LangChain) Semi-structured RAG\n",
        "## 작성자 : AISchool ( http://aischool.ai/%ec%98%a8%eb%9d%bc%ec%9d%b8-%ea%b0%95%ec%9d%98-%ec%b9%b4%ed%85%8c%ea%b3%a0%eb%a6%ac/ )\n",
        "## Reference : https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_Structured_RAG.ipynb"
      ],
      "metadata": {
        "id": "pJ6i2qcdZ0Ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다양한 문서에는 **텍스트(text)와 표(tables)**를 포함한 혼합된 콘텐츠 유형이 포함되어 있습니다.\n",
        "\n",
        "반구조적 데이터(Semi-structured data)는 적어도 두 가지 이유로 인해 기존의 RAG에 도전 과제를 제기할 수 있습니다:\n",
        "\n",
        "*   텍스트 분할이 표를 분리하여 검색(retrieval) 시 데이터가 손상될 수 있습니다.\n",
        "*   표를 임베딩하는 것은 의미적 유사성 검색(semantic similarity search)에 어려움을 초래할 수 있습니다.\n",
        "\n",
        "이 예제는 반구조적 데이터(Semi-structured data)가 포함된 문서에서 RAG를 수행하는 방법을 보여줍니다:\n",
        "\n",
        "*   우리는 **Unstructured( https://unstructured.io/ )를 사용하여 문서(PDF)에서 텍스트와 표를 모두 파싱**할 것입니다.\n",
        "*   우리는 MultiVectorRetriever를 사용하여 **원본 표(raw tables), 텍스트(text)와 함께 요약된 표(table summaries)를 저장**하여 검색에 더 적합하게 할 것입니다.\n",
        "*   우리는 LCEL을 사용하여 사용될 체인을 구현할 것입니다.\n",
        "\n",
        "전체 흐름은 다음과 같습니다"
      ],
      "metadata": {
        "id": "ukVGziicaMtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![semi_structed.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABngAAAGCCAYAAADDr81aAAAMQGlDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkJAQIICAlNCbIFIDSAmhBZBeBBshCRBKjIGgYkcXFVy7iIANXRVR7IDYETuLYu+LBRVlXSzYlTcpoOu+8r35vrnz33/O/OfMuTP33gGAfpwnkeSimgDkiQukcaGBzNEpqUzSU0AEdEAFVkCLx8+XsGNiIgEsA+3fy7vrAJG3VxzlWv/s/69FSyDM5wOAxECcLsjn50G8HwC8mi+RFgBAlPMWkwskcgwr0JHCACFeIMeZSlwtx+lKvFthkxDHgbgVADUqjyfNBEDjEuSZhfxMqKHRC7GzWCASA0BnQuyXlzdRAHEaxLbQRgKxXJ+V/oNO5t800wc1ebzMQayci6KoBYnyJbm8qf9nOv53ycuVDfiwhpWaJQ2Lk88Z5u1mzsQIOaZC3CNOj4qGWBviDyKBwh5ilJIlC0tU2qNG/HwOzBnQg9hZwAuKgNgI4hBxblSkik/PEIVwIYYrBJ0iKuAmQKwP8QJhfnC8ymaDdGKcyhfakCHlsFX8WZ5U4Vfu674sJ5Gt0n+dJeSq9DGNoqyEZIgpEFsWipKiINaA2Ck/Jz5CZTOyKIsTNWAjlcXJ47eEOE4oDg1U6mOFGdKQOJV9aV7+wHyxDVkibpQK7y3ISghT5gdr5fMU8cO5YJeEYnbigI4wf3TkwFwEwqBg5dyxZ0JxYrxK54OkIDBOORanSHJjVPa4uTA3VM6bQ+yWXxivGosnFcAFqdTHMyQFMQnKOPGibF54jDIefCmIBBwQBJhABms6mAiygai9p7EH3il7QgAPSEEmEAJHFTMwIlnRI4bXeFAE/oRICPIHxwUqeoWgEPJfB1nl1RFkKHoLFSNywBOI80AEyIX3MsUo8aC3JPAYMqJ/eOfByofx5sIq7//3/AD7nWFDJlLFyAY8MukDlsRgYhAxjBhCtMMNcT/cB4+E1wBYXXAW7jUwj+/2hCeEDsJDwjVCJ+HWBFGx9KcoR4FOqB+iykX6j7nAraGmOx6I+0J1qIzr4YbAEXeDfti4P/TsDlmOKm55Vpg/af9tBj88DZUd2ZmMkoeQA8i2P4/UsNdwH1SR5/rH/ChjTR/MN2ew52f/nB+yL4BtxM+W2AJsH3YGO4Gdww5jjYCJHcOasDbsiBwPrq7HitU14C1OEU8O1BH9w9/Ak5VnMt+5zrnb+Yuyr0A4Rf6OBpyJkqlSUWZWAZMNvwhCJlfMdxrGdHF2cQVA/n1Rvr7exCq+G4he23du7h8A+B7r7+8/9J0LPwbAHk+4/Q9+52xZ8NOhDsDZg3yZtFDJ4fILAb4l6HCnGQATYAFs4XxcgAfwAQEgGISDaJAAUsB4GH0WXOdSMBlMB3NACSgDS8EqUAnWg01gG9gJ9oJGcBicAKfBBXAJXAN34OrpAi9AL3gHPiMIQkJoCAMxQEwRK8QBcUFYiB8SjEQicUgKkoZkImJEhkxH5iJlyHKkEtmI1CJ7kIPICeQc0oHcQh4g3chr5BOKoVRUBzVGrdHhKAtloxFoAjoOzUQnoUXoPHQxWoHWoDvQBvQEegG9hnaiL9A+DGDqmB5mhjliLIyDRWOpWAYmxWZipVg5VoPVY83wOV/BOrEe7CNOxBk4E3eEKzgMT8T5+CR8Jr4Ir8S34Q14K34Ff4D34t8INIIRwYHgTeASRhMyCZMJJYRywhbCAcIpuJe6CO+IRKIe0YboCfdiCjGbOI24iLiWuIt4nNhBfETsI5FIBiQHki8pmsQjFZBKSGtIO0jHSJdJXaQPaupqpmouaiFqqWpitWK1crXtakfVLqs9VftM1iRbkb3J0WQBeSp5CXkzuZl8kdxF/kzRothQfCkJlGzKHEoFpZ5yinKX8kZdXd1c3Us9Vl2kPlu9Qn23+ln1B+ofqdpUeyqHOpYqoy6mbqUep96ivqHRaNa0AFoqrYC2mFZLO0m7T/ugwdBw0uBqCDRmaVRpNGhc1nhJJ9Ot6Gz6eHoRvZy+j36R3qNJ1rTW5GjyNGdqVmke1Lyh2afF0BqhFa2Vp7VIa7vWOa1n2iRta+1gbYH2PO1N2ie1HzEwhgWDw+Az5jI2M04xunSIOjY6XJ1snTKdnTrtOr262rpuukm6U3SrdI/oduphetZ6XL1cvSV6e/Wu630aYjyEPUQ4ZOGQ+iGXh7zXH6ofoC/UL9XfpX9N/5MB0yDYIMdgmUGjwT1D3NDeMNZwsuE6w1OGPUN1hvoM5Q8tHbp36G0j1MjeKM5omtEmozajPmMT41BjifEa45PGPSZ6JgEm2SYrTY6adJsyTP1MRaYrTY+ZPmfqMtnMXGYFs5XZa2ZkFmYmM9to1m722dzGPNG82HyX+T0LigXLIsNipUWLRa+lqeUoy+mWdZa3rchWLKssq9VWZ6zeW9tYJ1vPt260fmajb8O1KbKps7lrS7P1t51kW2N71Y5ox7LLsVtrd8ketXe3z7Kvsr/ogDp4OIgc1jp0DCMM8xomHlYz7IYj1ZHtWOhY5/jASc8p0qnYqdHp5XDL4anDlw0/M/ybs7tzrvNm5zsjtEeEjyge0TzitYu9C9+lyuWqK801xHWWa5PrKzcHN6HbOreb7gz3Ue7z3Vvcv3p4ekg96j26PS090zyrPW+wdFgxrEWss14Er0CvWV6HvT56e3gXeO/1/svH0SfHZ7vPs5E2I4UjN4985Gvuy/Pd6Nvpx/RL89vg1+lv5s/zr/F/GGARIAjYEvCUbcfOZu9gvwx0DpQGHgh8z/HmzOAcD8KCQoNKg9qDtYMTgyuD74eYh2SG1IX0hrqHTgs9HkYIiwhbFnaDa8zlc2u5veGe4TPCWyOoEfERlREPI+0jpZHNo9BR4aNWjLobZRUljmqMBtHc6BXR92JsYibFHIolxsbEVsU+iRsRNz3uTDwjfkL89vh3CYEJSxLuJNomyhJbkuhJY5Nqk94nByUvT+4cPXz0jNEXUgxTRClNqaTUpNQtqX1jgsesGtM11n1sydjr42zGTRl3brzh+NzxRybQJ/Am7EsjpCWnbU/7wovm1fD60rnp1em9fA5/Nf+FIECwUtAt9BUuFz7N8M1YnvEs0zdzRWZ3ln9WeVaPiCOqFL3KDsten/0+Jzpna05/bnLurjy1vLS8g2JtcY64daLJxCkTOyQOkhJJ5yTvSasm9UojpFvykfxx+U0FOvBHvk1mK/tF9qDQr7Cq8MPkpMn7pmhNEU9pm2o/deHUp0UhRb9Nw6fxp7VMN5s+Z/qDGewZG2ciM9NntsyymDVvVtfs0Nnb5lDm5Mz5vdi5eHnx27nJc5vnGc+bPe/RL6G/1JVolEhLbsz3mb9+Ab5AtKB9oevCNQu/lQpKz5c5l5WXfVnEX3T+1xG/VvzavzhjcfsSjyXrlhKXipdeX+a/bNtyreVFyx+tGLWiYSVzZenKt6smrDpX7la+fjVltWx1Z0VkRdMayzVL13ypzKq8VhVYtavaqHph9fu1grWX1wWsq19vvL5s/acNog03N4ZubKixrinfRNxUuOnJ5qTNZ35j/Va7xXBL2ZavW8VbO7fFbWut9ayt3W60fUkdWier694xdselnUE7m+od6zfu0ttVthvslu1+vidtz/W9EXtb9rH21e+32l99gHGgtAFpmNrQ25jV2NmU0tRxMPxgS7NP84FDToe2HjY7XHVE98iSo5Sj8472Hys61ndccrznROaJRy0TWu6cHH3yamtsa/upiFNnT4ecPnmGfebYWd+zh895nzt4nnW+8YLHhYY297YDv7v/fqDdo73houfFpktel5o7RnYcvex/+cSVoCunr3KvXrgWda3jeuL1mzfG3ui8Kbj57FburVe3C29/vjP7LuFu6T3Ne+X3je7X/GH3x65Oj84jD4IetD2Mf3jnEf/Ri8f5j790zXtCe1L+1PRp7TOXZ4e7Q7ovPR/zvOuF5MXnnpI/tf6sfmn7cv9fAX+19Y7u7XolfdX/etEbgzdb37q9bemL6bv/Lu/d5/elHww+bPvI+njmU/Knp58nfyF9qfhq97X5W8S3u/15/f0SnpSn+BXAYEUzMgB4vRUAWgoADHg+o4xRnv8UBVGeWRUI/CesPCMqigcA9fD/PbYH/t3cAGD3Znj8gvr0sQDE0ABI8AKoq+tgHTirKc6V8kKE54ANUV/T89LBvynKM+cPcf/cArmqG/i5/RfCtnxfwW052wAAAERlWElmTU0AKgAAAAgAAgESAAMAAAABAAEAAIdpAAQAAAABAAAAJgAAAAAAAqACAAQAAAABAAAGeKADAAQAAAABAAABggAAAAAFNuecAAACBWlUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczp0aWZmPSJodHRwOi8vbnMuYWRvYmUuY29tL3RpZmYvMS4wLyIKICAgICAgICAgICAgeG1sbnM6ZXhpZj0iaHR0cDovL25zLmFkb2JlLmNvbS9leGlmLzEuMC8iPgogICAgICAgICA8dGlmZjpPcmllbnRhdGlvbj4xPC90aWZmOk9yaWVudGF0aW9uPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+MTY1NjwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4zODY8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KoM9XhQAAQABJREFUeAHsnQeYFMXWhs8SREBQBAQRBUUFA0pQQUTFhGJWvIJiwBx/9BquCcWsV8yK1xwQs2JOoCImjCAiCggCCogCAiIZdv9+C6vpmZ3dndnd2Z2Z/c7z9E7H6uq3emar6qtzKq8gMJOJgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAiIgAhkDYFqWZNTZVQEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAERMARkMCjF0EEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEsoyABJ4sKzBlVwREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAQk8OgdEAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREIEsIyCBJ8sKTNkVAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAQk8egdEQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREIMsISODJsgJTdkVABERABERABERABERABERABERABERABERABERABERABERAAo/eAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQARHIMgISeLKswJRdERABERABERABERABERABERABERABERABERABERABERABEZDAo3dABERABERABERABERABERABERABERABERABERABERABERABLKMgASeLCswZVcEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEJPDoHRABERABERABERABERABERABERABERABERABERABERABERCBLCMggSfLCkzZFQEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEJPHoHREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAERCDLCEjgybICU3ZFQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAKP3gEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAERyDICEniyrMCUXREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQARGQwKN3QAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQASyjIAEniwrMGVXBERABERABERABERABERABERABERABERABERABERABERABCTw6B0QAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQgSwjIIEnywpM2RUBERABERABERABERABERABERABERABERABERABERABERABCTx6B0RABERABERABERABERABERABERABERABERABERABERABEQgywhI4MmyAlN2RUAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREACj94BERABERABERABERABERABERABERABERABERABERABERABEcgyAhJ4sqzAlF0REAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAERkMCjd0AEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEsoyABJ4sKzBlVwREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAQk8OgdEAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREIEsIyCBJ8sKTNkVAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAQk8egdEQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREIMsISODJsgJTdkVABERABERABERABERABERABERABERABERABERABERABERAAo/eAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQARHIMgISeLKswJRdERABERABERABERABERABERABERABERABERABERABERABEZDAo3dABERABERABERABERABERABERABERABERABERABERABERABLKMQI0sy6+yKwIiIAIiIAIiIAIiIAIiIAIikKMEJk6caH/99Vf4dLVq1bIddtgh3PYrkydPtvnz5/tNq169unXo0CHcTnXl77//th9//NFd1qBBA9tyyy1LTCLVayZNmmQLFy506TZu3NhatmxZ5D3Gjh1rK1ascMe32GILa9iwYZHn5sqBJUuW2Pjx4xM+DuW73nrrGdwon8q2aFlut912VqdOncrOku4vAiIgAiIgAiJQRQnkFQRWRZ9djy0CIiACIiACIiACIiACIiACIpBBBPbee28bMWJEmKMaNWrYvHnzrH79+uE+Vlq3bm10snvjuBdP/L5UPkeNGmVdunRxlxx22GH2yiuvxFyO8PTAAw/Y7bffHu4v6ZrwxH9WBgwYYNdee63b2nHHHe3bb7+NP8Vt//bbb9a8eXPLz8932zznVlttlfDc8tw5bNgwmzBhgvXr1688k006rTFjxiQl0sHiwQcftG7duiWddlEnJirXos6N7t9///0NXhj5bteuXfSw1kVABERABERABESgwggoRFuFodaNREAEREAEREAEREAEREAEREAEUiGwatUqGzlyZMwlM2bMiBF3Yg6W88bKlSvtoosusrZt29q7775bptT79OkTXo+HDuJCInvppZdCcWeXXXZJu7gza9YsQ9RCtCgqT4nyWVn7fvrpJ+vevbs9/fTTpc5CeZZrqTOhC0VABERABERABESgHAgoRFs5QFQSIiACIiACIiACIiACIiACIiAC6SEwfPhwO+SQQ8LE33///XA93St//vmn3XbbbQlvs+mmm4YeOXgUlWRbb7217bzzzvbVV1+5U5999lnDqyfeXnzxxXDXcccdF66na+Xjjz+21157LV3JlyrdRo0axXhLrV692mbPnm333nuvzZw50xBobrnlFjv22GNLlX5x5ZpMgieeeKJ17drVnbrxxhsnc4nOEQEREAEREAEREIG0EJDAkxasSlQEREAEREAEREAEREAEREAERKAsBDbbbDP75Zdf7L333otJxm8zJ8s666xjdNbHG54/WF5enpufJ3rcH2MfIeCKMqKZR8/12z5Nwqhddtll7nL2JWN48XiB57nnnisk8CBiILhg5K1Xr14xySJ0MFcQ90NUKi7//kJCvk2dOtU22mgja9GihdWsWdMfMtLzoeDYyTrPXK1aNbeEJ/6zsnTpUhs3bpxLp0mTJvGH3TZp+DTJH/f47rvvDIGrbt26Ca+J38l5xx9/fPxuJ/Rtv/32bj9pwqtp06aFzps2bZrNnTvXtt1220Lz4/hy9Bf5bV+u7Pfl7jlwH0IAeiGPcuE6rKgyKCkPcMGi93U7/vnj88Bm/D2SeQ/Koxyi+dG6CIiACIiACIhAZhJQiLbMLBflSgREQAREQAREQAREQAREQASqNAE/xwqCBmHEvHkPnt122y1GrPDHESEQMVhatWrld4efCB0cq127drgv0QreIog43sgH1/n5VpiDx9+nZ8+e/rRiP3v37h0KTqSHSBG1oUOHhuLIfvvt50QZjuOxcskllxiiFuHiEDlYP+WUU+zvv/+OJhGuDx482IkfzZo1M1gxdw3PE/VIOvroo2O8YO6//373TOedd16YDisPP/ywS2P99de3Tp06uXTxXGF/vJ111lkhly+//NK22GILN7dOw4YN7YUXXog/PaXtLbfc0qXNRQgsiDhRe+qppxyzzTff3HlL1atXz/bcc0/7+eefw9NKKlcEMV+uZ5xxhl133XXGs7Zp08Y6duzo0jnwwAPDc+LnUkomD99//314vReNwgwGK1OmTAmPe0GL46m8B+ksh2hetS4CIiACIiACIlC5BCTwVC5/3V0EREAEREAEREAEREAEREAERCABgb322ivc6712EEXogMeix8MTM3wFr5d99903zCVePFGLCiDROXsQFAhJtmzZsvD05cuX26OPPuqEjOh+Trj44ouNMGK///57eD4rf/zxh5tTCNEiWbvrrrvstNNOs88++8wJDP46vFrYH+9l5I/ziaCFFxa2YsUK23XXXd16af/cc889YR7w8sEryNv1119vhLSbM2eO3+XEso8++siJcngepWq8d1dddVV4WYcOHcL1RCvJ5gGRzrNAzPnkk09ikkOc83bqqaf6VUv1PfAXlnc5+HT1KQIiIAIiIAIiUPkEJPBUfhkoByIgAiIgAiIgAiIgAiIgAiIgAnEEunTp4rwY2O0FHv/JPu/hw3o67OCDD7YnnngiTJo5d1599VVDZCiLRefViQo8iC+IERjixeGHH+7Wn3nmmfD5d9ppJxsxYoQLuYaAg02YMMHwvPGGV9Add9zhNjfccEO7+eabjXBhiAbVq1d3+2+88UYnhFx66aVODPLX9ujRwz3j6aef7nYxH9D555/v1jfYYAPnsfPDDz/YkCFDrHHjxm7/888/79L2aUQ/CQ1HqLWBAwfaOeecE+MRFT0vfh3xiPJnwWOoffv2zosG4cob8xcRog/jPogrGPl68skn3fPdeuutbt+iRYvsyiuvdOuplCvcWrZsaaSDmHXSSSe5NBL9SSUPXH/mmWeGyUQFHTyT/DbP17dvX3dequ9BmHiwUtpyiKahdREQAREQAREQgQwlEFQeZCIgAiIgAiIgAiIgAiIgAiIgAiJQ6QQCrxwmNnHLjBkzCoLQYm49CDPm8nbYYYe57SD0VkEQrqog8Ihx2/Xr1w/zvmTJErePdII5Z8L9fqVBgwbueDCvid9VEHinhNdwD2+B0BDuD+Zz8bvdZ1HXxJyUYCMQGwrq1KkTphvMyePO+t///hfuC0Sg8ErPgOd57bXXwv0LFiwoCAQAd00wX1G4v1+/fmE6gdAT7mfl6quvLgiEloIHHniggGfDnn322fD8s88+2+3zf4444ojwWCDS+N3uMwjPFh4LwrCFxwJxKNzfuXPncH9JK6NHjw6v8+9Aos9GjRoVDBo0KCa5K664Irz2ggsuiDkWCEThscBbxh0rrlyDcIDh+dz/7bffjkmPje7du4fnjBkzxh1PNQ+8p/5dDELfFQShBV06gYAXph14R7l9/En1PShtOYQ31IoIiIAIiIAIiEBWEKgRVFhkIiACIiACIiACIiACIiACIiACIpBxBPDS+fTTT90cPITY+vDDD10eu3btWmji+UzJ/H/+8x+7++67C2UnEDAsEInc3Dl45zz99NPunEBgMTxzigrPNnHixDAtvHMmTZoUbjM3zPTp0+3XX391IdDw+Iie78OA+QvweknW8vPzbeTIkeHpgfAVrrNyyCGHWLVq1VwYNOa4+euvvywQ2gqdE7MjyQ08mAhHhidPIICFoengBqdAnItJKfrM8+fPj5lniLS8EQ6NOYGStVq1alkg5iR1eqp5YA6oE044wQiBt3DhQuc5Rbi7qNeY96QiA9H0k3kPopmmrGQiIAIiIAIiIAK5SSC2VpSbz6inEgEREAEREAEREAEREAEREAERyEICCDw33HCDyzmhxugIx5Kdf2f16tXu/Ogf5oJJp61atcqYHyfegiGg4S7m1/ECDyHOEIW8mMI8Pfvtt587N/DysLlz54bX9e/fP1yPrpB24HniwokFnk/hocAzJFxPdWXevHn2559/ussIe7bVVlvFJLHRRhtZq1at7KeffnL7EZqYWyZqm2yySXQz6fXAS8fgghESjnmLmHvplVdesUMPPdRefvllQ3zxxr29PfbYY3610OfMmTML7StuBwIaIlYyVpo8IOAg8GCElUOIISwetuWWW4bveWneA5fIP39KWw7RNLQuAiIgAiIgAiKQmQQk8GRmuShXIiACIiACIiACIiACIiACIlDlCTAHC14piDLMQeIN4ScZC8K4xZyGV8qyZcti9pX3xrrrrlvIk4V7+PlvWMcrBNFkzpw5zvuGuWW8GNW7d+/w3CCUm5uPZ/HixU5owLuD9BMZc+RgzZs3t/Hjx7v133//3dq0aePW+YNogxiEgFKSkT+EAUQR8onn0NZbbx1ehuAyefJkt52Xl5fQMyYqwoQXpriC1xNeToh6lF8QMs3OO++8mHmHEJu8BaHSrF27dn4z5nPHHXeM2S5pI5X8lyYPPNvuu+9uH3/8sQ0fPtyJfn///bfL1qmnnmpwxUrzHrgL//mTynNEr9O6CIiACIiACIhA5hNIbihK5j+HcigCIiACIiACIiACIiACIiACIpBjBOjY3mWXXdxTeQ8YwoB16NChyCcl9FUwR487jgeKF07Y8f3338dsF5nIPwei3hvRdIq77sYbb3SeRngbRZeo0EKIMcJxeRs8eLBfNbx7ouZFFcQNeBx11FFuIVwZIgvPGsx1Y17gad26dXg5YkjUzjzzTCcsNW3a1IUE41hxzxgN8fbqq69Gk7I33njDiUXsJI/RUGj+xPISFvbYYw8L5tbxyVowh5ANGzYs3PaM2EGoOM+ITzyqeEa8i3x4tuKeOUw0WEkl/6nmwd/njDPOcKuImJdeeqlbr1mzpvXt29ef4j59+sm+B9GLU3mO6HVaFwEREAEREAERyHwCEngyv4yUQxEQAREQAREQAREQAREQARGosgTivXXweIh6wyQCs80227jdePBceeWVrtP/yy+/tOOOOy7R6UXui3rL4MkyYsQIJ2wUeUEKB+KFHC6lE3/nnXeOScULAOy89tprnacHIbtY79evnx1wwAHu01902mmnhXPUEP6L+YDGjBljAwcOtKFDh7rTEL7wjsKiz/jNN9/YJ598Yp999pk7Rjg0b9dff70TVpgLCU+iyy67zB8Kw+iFO/5Z8R4o8ftLs33dddeZL1euhwscMJ7Z3+upp54ywrRxDBEKzj179nScvHdM9JmLK1efprtJCX9SzYNPDhGqYcOGbhMPKwzuhOqLWqrvQfTaVJ4jep3WRUAEREAEREAEMp+ABJ7MLyPlUAREQAREQAREQAREQAREQASqLIF4gSeZ+XeiwsRNN91kzEXTqVMnF2osfi6Z4sDiHdOyZUt3CuLA3nvvbSeddFJxlyR9DK8b5rCJWiLRB08O/8xjx4514d3wYkLwwDbddFN75JFHwmS23357u+iii9w24egIZ4bHE/P84P2BMZ8RIdgwPFu8APD111+7kGF33HGHO3b88cc7YYgNPGPwANphhx2cd4kXI0455RQnoLgL0vgHUebxxx8Pxb1p06bZ1Vdf7e6I8OO9XxCvTj75ZBcmjzlteGY8ppjzyM9JlI5yTTUPHhXeNSeeeKLfdJ/MzRNvqb4H8ddrWwREQAREQAREIDcJSODJzXLVU4mACIiACIiACIiACIiACIhAThDw8/D4h4kXfPz+6CfzsCBs+LBhdPDj+TNq1CjbfPPNo6eWuH777bfbeuutF56HGBI/t094MMWVeEEnfpvkEAAIR3bhhReGoowPF4dI9PLLL1uDBg1i7oyoNWTIEGvWrFnMfsSghx56KCbcGQIW4ogXebgg+nyIRQgrXbt2jQlZttlmmxneMg8//HDMPdK5Qbg+5ivyhhD13XffuU1C4xHqzpevZ9SiRQvncRQNN8cF6SjXVPPgnyMq6JD//fbbzx8KP0vzHoQXa0UEREAEREAERCBnCeQFcYwLcvbp9GAiIAIiIAIiIAIiIAIiIAIiIAJVlgDzr4wfP955ykRFmlSBLF++3H788Udr1KiRNW/ePNXLy/X8WbNmGWHFEFjiw3glutEff/xheLtssskmTvCJCjnR8xcsWGBTpkxxrPx8PtHjrMPhhx9+cPf2YcXiz8mEbeY+mjRpkhPE4BSdcyeav3SWa7J5iOYnlfVU34NU0ta5IiACIiACIiAC2UNAAk/2lJVyKgIiIAIiIAIiIAIiIAIiIAIiIAIiIAIiIAIiIAIiIAIiIAKOgEK06UUQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQAREQgSwjIIEnywpM2RUBERABERABERABERABERABERABERABERABERABERABERABCTx6B0RABERABERABERABERABERABERABERABERABERABERABEQgywhI4MmyAlN2RUAEREAEREAEREAEREAEREAEREAEREAEREAEREAEREAERKCGEIiACIiACIiACIiACIiACIiACIiACOQ+gYKCAsvPzy+3ZfXq1VatWrWkl+rVq5d4bl5eXu4XhJ5QBERABERABERABMqJgASecgKpZERABERABERABERABERABERABEQgnQQQZ5YsWWKLFy+OWeL3xW9zPvsQZDLdEIHq1KljdevWjVni98Vvcz77EJxkIiACIiACIiACIlBVCOQFI3gKqsrD6jlFQAREQAREQAREQAREQAREQAREIJMILFq0yBYsWFDkEhVrli5dmlLWa9SoEYokiB+1atUq0YMmFY8czi1PjyDSWr58eYyItWrVqpSeuXbt2jHPvMEGG1hRS7169VJKWyeLgAiIgAiIgAiIQKYRkMCTaSWi/IiACIiACFQYgaAPwb75JrVOg44dawQdI8lncdy4VbZs2ZrzO3SoYcGg1BJt0qTVtnDhmvEX223HKFaFKikRmk4QAREQAREQgQwigCiRSLRZuHBhof0ledV4zxTv0eI9V/xnUcfZv84662QQldJlZcWKFaG3UlTsSrSeaF9xd8VbKF78WX/99Qvt4xzEMpkIiIAIiIAIiIAIZBoBCTyZViLKjwiIgAiIQIURWLy4wNZbL7WRsIsX105JcGndeolNmrTmkebPrx10GJQs1uy//xIbNmzNNWPGrGPt2qlDocJeCt1IBERABERABFIg8Pfff9usWbNs5syZMcvcuXOLTaVmzZoJRQSEhAYNGsQcKzYhHSyRQFRomz9/fiGBzR9fuXJlsWk1atTINtlkk5ilWbNmQV1yvWKv00EREAEREAEREAERSCcB9Rilk67SFgEREAEREAEREAEREAEREAERyHoCiADxIg7beOTEGx4g22yzTSGhJuopIlEgnlr6tj33ku6AWOfFnkSflPfYsWPdEk2L8o4XftjmvjIREAEREAEREAERSDcBCTzpJqz0RUAEREAEMpZArVp5Nnhw7L/C559fZW+8sSbLBx5o1rt37HGukYmACIiACIiACOQmATr2p02bVkjMIfRXvOHRscMOOxTq3Jd4E08qO7YpN5bmzZsXmWFEoHihDw+uH374wS3RCwmhFy/8tGzZUsJPFJLWRUAEREAEREAEykwgtteqzMkpAREQAREQARHIHgKEUj/++NjY9D/8kB8IPMHkPIFtt121Qsf90zF/z/Tpq+233wrcnDxNm+ZZy5ZJTLATJDB7dr7NmJEfpF/datcunWA0bdpqmzu3wLbdtuQ5eubMybdffsk3Io80aZJnLVpUT2keIf/M+hQBERABERCBXCKwePFi+/nnn8NlypQp9ueffxZ6xKZNmzqPnPjO+lq1ahU6VztymwACUOvWrd0SfdLly5cXEn4Qgn766Se3RM/dcMMNrVWrVrbFFluEC/MlyURABERABERABESgNAQk8JSGmq4RAREQARGo0gSeemqF9e+/KhjhG4th663Nnnuu6DlzpkxZbWedtcK++mrNdcG8vnbSSXl2113rJj2vD/f+979X2Zw5a9KoVs2sa1ezxx6rFXQSxApMo0atsgsuWGGffx6bzxYtzP773xrWq1esuBV7lrZEQAREQAREIHcIML8KAk5U0Jk9e3bMA9arVy+Y966d63yPijnV+GcrE4FiCCD2ecEmelp+MCIo6vHDO8jyVVAZZPGGiOiv5xMBiHmaZCIgAiIgAiIgAiJQEgEJPCUR0nEREAEREAERiBB4882VgVfPKisoWLMzL3DA8euTJpntsceKIERHtSC8R+HOoD33XGHBYGHz16xebfbwwwVBQ3+pffBBnchdEq9ef/0yu/LKNd5F/gw8iT76yIIOqeX26afrWNu2a/61T5262nr0WBHMDbDmzCA8vOGxNG8enkcIS6uCa6oHI1BjRSGfrj5FQAREQAREIJsJIOREBZ1ff/015nHoPG/Tpo3rSKcznaVx48Yx52hDBMpKAHFw0003dUs0rTnBSB0v9vhPBMfPPvssPI3rvOjjPX7Cg1oRAREQAREQAREQgX8ISODRqyACIiACIiACKRC4666VoaDz1FM17MgjawajgfMDwWS5ffml2aJFZh9+uMqOO66wd0wwMDgI/1bTdtutpo0cudKOOWZlEGbNbMQIs5dfXmFHHFH4Gp81BJvrr18j7tD/dPvtNeyAA2rYE0+ssIsuynf3vfLKlfbKK2v+tT///MpQ3HnhhRrWs+eatIcOXWFHHbXKyMsTT6y0G2+UwOMZ61MEREAERCB7CRBa7bvvvrNvvvnGLfFP0jKY+8QLOXzSeS4TgcoigJjI0rlz5zALiJBe7OFzWuAqzr6RI0eG53Ts2NFYmPuJUG8yERABERABERABEZDAo3dABERABERABFIgMGxYnaDBvdrNodO165rQGcyDc+CB1QKBZ40Aw9w4iQwxpVu3Ndfsu29Nu+KK/CDcWuDGE9gLL6wOBJ5EV63Z98gjKy0I7+7s+OOrhQLShReuG1y7xL74wuzVVwsCsWm1C9UWnQv6vvtWOa+hbt3WCD1z59awhg0LexgVfXcdEQEREAEREIHMI0An+Pjx42306NE2CTfaf6xhw4Yx3jl4QVQnLqpMBDKYgPf06datm8vl6sDV23ui8a5PmDAhRsDcOogN3KFDh2BOx+2ceJnBj6asiYAIiIAIiIAIpJGABJ40wlXSIiACIiACuUmgZcvqts46eUF4teX28cf5QTiNAps8ee2zrliRWODZc8/Yf7t77UVn0xqBZ8qUxNf4VCdOXHt8/vwCu+22Zf6QReflnTIl3wk8Bx5Yw264YYURBg4PoREjEHlWBaM+zQ4+uJqdeGJN4zlkIiACIiACIpAtBBYHcU5/+OEHt4wbN85mzZrlsk4YLObOadu2rfNsYP4cmQhkOwFEya222sot/lmYzwdPNd5/Pr2w2axZM/f+b7vttsZSN1o59BfrUwREQAREQAREICcJxPY05eQj6qFEQAREQAREoPwIzJmTb716LXOiiU+1dm2zzTc3mzp1zZ6gnymhbbBB7IFGjdZu//FHwkvCndOnrxV4HnuM9bXb4UnBysyZa7yIOnWqYYMH59sFF6yy339fcwZzBX39NUu+3XTTcrvzzup25pm1opdrXQREQAREQAQyisCyZctszJgx9u2337rPv//+2+WvRYsWgffsga4zmw7tddddN6PyrcyIQDoIIF6y9OjRw/hueMETT7Z3333XLeutt561b9/eiZ586ruRjpJQmiIgAiIgAiKQOQQk8GROWSgnIiACIiACWUCgR49lQXiMNRk988w8O/bYGtapU03nzXPOOWu8capXz0v4JLNn5wfx0td6zcydu0aM4eTNNkt4Sbhzo43C1SC0GyOV14pDa4+Y7bjj2vSPPXadYL6ddeytt1bY8OGr7YMPCoLwHmvOJtxbv36rgzmE8m2jjRKnFU1X6yIgAiIgAiJQUQRWrVoVI+osWLDA3RoPHTx1EHQQeGQiUJUJINwQoo0Fmz59uhN8EEM//vhjt2ywwQYxYk+NGuoCqsrvjJ5dBERABEQgNwnov3tulqueSgREQAREIA0EZszID8WdoI/J/ve/wHXnH5s1a61HTX7+2nV/nM+hQ1cGnVJrBZh3310VHm7VKrEo5E/Yeus8e/PNNen+9VeBE278sWefXRGEjLMgBnt1F56N/V98sSoQc1bbTz8V2MUX17LDDw9OCOynn1YH68uDDgCzlSstCO2xWgKPI6M/IiACIiAClU0g6qkzd+5cl52WLVvaPvvs4yaWZ10mAiKQmACiJwvePdOmTQvn6xkRxOpladSoUYzYkzgV7RUBERABERABEcg2AhJ4sq3ElF8REAEREIFKI7B06Vrh5uefLZj4drUTVD76aKXdc89ab5yFCxNn8Zpr8OBZHsyBU9M+/3xVMEfO2mtOOaX4f8mnnbZOEFJtuRFm7amnCoIG+vIgVNw6gVfOSuvTZ5XlB0m1bLkqCGFT29ZfP8+uuWaFvf32mnz89tsy++9/awUN+2ruvEWL1uavTZu1gtPavVoTAREQAREQgYohgJDz2Wef2aeffmq//vqru2nDhg2te/fuTtTBa0cmAiKQGgHEUJaePXu6+Xq+CdzPWYYPH+6WTTfd1HbbbTfr0qWLE35SS11ni4AIiIAIiIAIZBKBvILAMilDyosIiIAIiIAIVCaByy5bZjffvEZ4ufjianbLLWtj+vMfs337JTZ27JocMtcOodNmzzZjHp6lS9fsP/XUPHvooTXePa1bLwm8ZCxoPFsgvJhNmVL46Xr1yrNnn13rDbT//kts2LA1540Zs04QjmaN+HP55cuCuXPWikLB3Lu2ek1UOCPixkcfrWO77rrm3HHjVtmee66w+fPX5jWI0mF//rn2/hdeWM1uvXXt8609ojUREAEREAERSC+BiRMnOlEHcWfJkiXuZp06dbJddtnFCTvr4JoqEwERKDcCK1ascCLPl19+GXh6f+HSrVOnjhN5EHtat25dbvdSQiIgAiIgAiIgAhVHoPjhwhWXj2LvtHjxYhdLNv6kakHPGhWSZs2aBZ1mQa+ZTAREQAREQATSSCAvz+zFF2vZCScst1GjzHnD/PGH2aGH5tm999YKhJhlTkB5/fUCJ7wgwHhDXPnss3WDOXuWBWEy1ggztWoxD041u/HG5EQWzttmmxU2YMAqmzp1rbjDNAQ33lgjFHe4Z9u2NYLY63l25ZUr7NVXC1xevbjTpInZ5ZdXt3POCTIgEwEREAEREIEKJICnDqIO4diwpk2bOm+dXXfd1fAqkImACKSHAKIp3zMWvOVGBZXZzz//3N577z23tG/fPhR70pMDpSoCIiACIiACIpAOAlnhwcMIE0ZzFWe4Ft97770upmxx5+lY2QgMC4aUTwhm6O7Xr1/ZEtLVIiACIpDlBGbOzA88d/KNEGd16wbKTwrGHDpTp64O5uOpYTVrpnBh5NSFCwvc/DmNG+fZZptVN7yJirLFiwvsl1/ybdGiguDcatakSTVDrJKJgAiIQLoJ4Jkxfvz4pG/DZOHVo+p40lcWfeKfgbo95R/3ySaBwr3ZZpsVfXIajiwM4nZOwpUzMObA2HzzzdNwl8xP8rvvvrNXXnnFtSXILWXduXNnt2ji98wvP+UwNwmsWrXKiTwIPaNHj3YP2aZNm2C+xsNthx12yM2H1lOJgAiIgAiIQI4RyBmBh3KhMXj11Vdb//79c6yYKv9xZs2aZWeddZa99tprdvbZZ9ugQYMqP1PKgQiIgAiIgAiIgAiIQEYT+Prrr23nnXdOOo/z5s0L5irbMOnzkznxmWeeCbwnj3WnnnfeecF8Zncmc1m5nfN2MCHagQce6NI74YQT7Iknnii3tLMhIaIxvPzyy/bWW2+57DIwDx5bbLFFNmRfeRSBKkPg52CCSb6neNhhfE+POOKIYCBT3SrDQA8qAiIgAiIgAtlIICtCtEXBMupu4MCBwSTTBbZ8+fJg9PRse/zxx4OJrn8OwuGsDsLWDLCDDjpInjxRaOWw/vHHHztxpxySUhIiIAIiIAIiIAIiIAIiIAJVgAAhoPDaIRxUq1at7OCDDy4xMkMVwKJHFIGMJIDoeu655zpR/o033nBiz7hx45w3D2HdZCIgAiIgAiIgAplJIOsEnnr16tnxxx8fQ/Oyyy6zXr162dChQ4M5BvJdpYTYzkUZIRoYSYbrcTKTd/7111/2448/upErhFSIH8GC2IS4hDEvEEvUcHvG8oJ4OD7kRPQa9nGMSQ+pQLVs2dIaNmwYTcJmzpwZzOvwp2233XaF0o+eSD7IK+kxSWKicAcwYsH8cba///5792xU7LjeG2n689nHOs8U/6wIbjTefvvtN9sgmGyC8BeaG8lT1KcIiIAIiIAIiIAIVG0CjRs3tttuu61YCOutt16xx3UwOwjQVnjkkUds5MiRRpnSVkPc8W2h7HgK5VIEqiYBwuPvtNNOhsjz5ptv2j333GOEWDzllFPC/oOqSUZPLQIiIAIiIAKZSSBWicjMPJaYK0SKu+66KxRrcClGaIgaokOfPn2sefPmtuWWW9qOO+7oGhs0NOLP9dcx90+7du2cSEF86LZt2zrh5cwzz7S///7bn2bvvPNOMIdCTbecfPLJ4X5WqAj5YwcccEB4jNAQfv+HH35oF110kRNFqEjR+N1nn32CuRIW2Q8//ODuS76JgYvww1xD8bZy5Uq75JJL3DORz+23396tUwmL5pXrrr/++vDeX3zxhV1zzTXWoEEDxwQ2TG46fPjw8BZHH310GNaCnffff7+7nhAXGMIUIhsxxbfaaivbY489XF7ZxqWb55CJgAiIgAiIgAiIgAhUbQJ+oBaDtYpaooOvEAlY/EAjBkgxkGnq1KmFQDLPDeHgli1bVuhYoh3UnZlv4pdffkl0OGYfg50YCMVcQuSnJJs/f77RjmCQWFU0BtLdcccdTtwhHBshtA877DCJO1XxZdAzZy0BxFi+t3x/+R4j1vK95vstEwEREAEREAERyCwCOSHwgBQBBNHGm59Ile3ff//d9t57b3v66aedJ4w/h4YdI1IQQ5hUMGrMNUNFZuzYsdHdLizcAw884NyUaWSWh51++uluNKNvkJLuBx98YN26dTNcoWlQeluwYIEhrBDLO2rEx73llltiGrV41Dz66KPOxdqnHb2G9f/7v/9zlbaoCIO30CGHHOLC38Wfn2gbcermm292QhJePQhEfNIAJiTD+eefn+gy7RMBERABERABERABERCBhATGjBkTDkhicBL1eAZBbbvttm7uFjzOGQiFsEMnJPP2MNcPXuR9+/YNRaH4xKlnU2fGy7xjx47WokUL5z3PgKt4S2UAFdfOmDHDtTnICyPgGUCF50q0nh1/j1zbJuIAXlqUH4PbCPfUrFmzXHtMPY8IVBkCfH/5HvN95nvN95vvuUwEREAEREAERCBzCOSMwAPSTTbZJCQ7efJkt04jbr/99rMJEya4bRqAxIL+6quvnEcPOxldh+cNggi2dOlS69evnwu7RqgyKjQ0IPFqaRmET8Pef/9957LsNsr4h7zefvvtroH67rvvhiHYGFWIdxLz3yC69OzZ092JUYxDhgwJ78rEse+9957bxgNoxIgRbmTjiSee6Pbx7HjdJDI40GieM2eOE5J8AwwWr776qrvk0ksvtYsvvji8vEePHu4YwhQjGhGRMBqyNLIZCUmlD4EMLyVGU0YFtzAhrYiACIiACIiACIiACFQZAtSxqUMXtSTyzAHO4MGDXb092qnIuXi842XPwCzv5UMd9oknnrD+/fsn5ErYMLzeyYu36dOnu7QYYBW1VAZQUQf29XCfBu2Q559/3rUz/L5c/iRiwq233uraXQhbJ5xwQi4/rp5NBKoUAb7PfK/pW+B7zvddJgIiIAIiIAIikBkEckrgiYZ0QBDBmNOGBWNE3wsvvOAagjTAEEnwNsEI90ADDCPkGg097NBDD3UxZ7fZZhvbd999XWWGUYEIMszHUx62//7727///W8jbEX37t1dCDmf7o033mhdu3Z1I9+injB4JXkbNGiQX7WrrrrKef4gREXD1uFOncgOOuggu+KKK1zoN+b3Oeuss8LT/D0YDckIR288N1wIBYeXDqMbMcJWwIWRPTwLYhXh4fCCYlJVmQiIgAiIgAiIgAiIQNUlQIcg9emiFoSZRMZAoeOOO84NeELY8YO6Zs+e7TobH3roITe46LrrrgsvLyotwgsRbhmxaO7cueYHRCEQXXjhhaFQlOoAqptuuslFDSADRBWgXrxkyRI3D40fRBZmLkdXHn74YZs2bZprTzCoTiYCIpBbBPhe01/A9xyxXCYCIiACIiACIpAZBHJK4Jk1a1ZI1Xui4M3ijVF4eJREDaHC27fffutWJ06c6He5EGnhRrCCF81jjz3mBBlCu5WHdejQISaZ+vXrh9u77LJLuE64B2/Me+Mtml/m/MFtmoVG1sYbb+xOY56h6DX+WkY9Rq1JkybhZjLhJGrVquVEKS5CzBkwYIDxPITQOOOMM2JGVIYJa0UEREAEREAEREAEREAEkiRAqLMHH3zQ1WsZxMQAJW+9e/e2U0891YVDu/zyy8M5Of/44w9/Sswnod2YMJw0mduS0Mu+/ktbwEcBSHUAlR8oxs1oKxBKbt1113XeO8ccc0xMHnJx46WXXnID5v71r3/Z7rvvnouPqGcSAREICPD95ntOhBO+9zIREAEREAEREIHKJ1Cj8rNQfjkg7rW3rbbayq1OmjTJ77LddtstXPcrhBHzDThGomDRdIjPnYoRsixqiUSV6HHW69atG7OLCQ294QnjLbrf72NkIKMPvRUVjoIQEQhgPsScPz+aPvsQbFI1PKH69Oljb731lnEfjFGRxEpnad++vQshFxWoUr2HzhcBERABERABERABEchuAk2bNjW8bYqyrbfeOuGhNm3aWO3atcNjRQ2GYg5I6u6EHsbLHK8c9kWN9gAhkL1R92XOS+aNxPAWIh/xA6iibQoGUOHt7wdQcQ9CFGPcn7pv1Lp162ZPPvlkdFdOrfuOXjyXjjjiiJx6Nj2MCIhAYQJ8z/lNROBBzCbaiUwEREAEREAERKDyCKxt3VReHsrlzoy2o5HlrXXr1m412sD65JNP7Oyzz/anuE/mt/G25ZZbutXmzZv7XWGoBb+DEAuEg9hss82M+XnizYcr8/sJA1GSJRJu/DXFHeOcOnXqOIGI+9C4JBwFowUTGSHq4i3awI0/luw2Ddk33njDfvrpJ+exQ1x1uOLRgxGy7dprr7U777wz2SR1ngiIgAiIgAiIgAiIQI4RoN568MEHp/xUyQ6GIuGS6s6J6sONGjUK84TnT6oDqKh7+0FeeAbFG57tuWx08iLAHX300bn8mHo2ERCBCAG+717kKWqQaeR0rYqACIiACIiACKSRQE4IPIgbzE/jG1ZMAOhDkzEizxseJggwPkwb3iavv/66PxyOtvPiEAfefvttF3bMn0TYs3PPPdfNMfN///d/dsMNN4RhHTgnPhwE88+k2xhliIjCKEUazkceeaS7JSMX//e//7lRiMyvk6hBm2zeoqMfPWeu5Xm/+uorN8oRYYzY5Sx4Lt1999128cUXu1t8//33yd5K54mACIiACIiACIiACIhASKA40aa4Y2ECkRUGasVb1BueQVypDqDCo4i5QKn/khZtjOhAsPj2Qfz9s3mbAXbMZYpnVHnNT5rNPJR3EagqBPi+E5r9008/dQNt/dzGVeX59ZwiIAIiIAIikEkEsk7godGEmIPhTTNv3jz78MMPXSgG9iFiDBw4kFVnuAzjmYOHz8KFC13YgEsvvdSN7rvrrrvcZK2c2K5dOzvqqKPcNT169HChzAjZ9sUXX9g555zj4mcTfoAJVDHmp/Hz4yAI0YijMffRRx/Z0KFD3bw0eLJcccUV7vx0/mGuGyaLxfCUIewajaybb77Z/GSzuFGTr9Ja1Cvom2++MbyhEH3wAPIjMRldyaS3iGo0cJcuXRreTm7bIQqtiIAIiIAIiIAIiIAIVBKB4cOHu3q8D1OMtw71Wm+tWrVyq6kOoGJunwkTJjgPdtKLzkMTHVDm75Mrnz40nTp3c6VE9RwikDwB/73nd8CvJ3+1zhQBERABERABESgvAlkn8CxYsMAQZhIZc7wQ13ujjTYKDyO8IADts88+zsvkzTffNJaoMcEqk6H6cGWIGUy+iiiCF8x9993nlug1hx9+uB1yyCFuF8IG6b/33nvOi6hnz57hqYgdeLiQTrqsb9++9txzz9mIESMMj6Hu3bs7Act72lDZeuSRR8p0+7Zt24Yi1tdff+0arQhiL7zwgpvYFs8mPKm6du3qRDbCs/lnhu9FF11UpvvrYhEQAREQAREQAREQgewmwMAsP1CrqCc57rjjbKeddirqcJn305Y46KCD7LbbbnPz5TDwy3vw7Lvvvi4MMzdJdQDVqaeeGtZ3jz32WHv00UedFz1tjFdffbXM+c7UBHyIbAQumQiIQNUi4L/3/negaj29nlYEREAEREAEModA1gk8UXR4kDRr1sw1xBAWaKAlinuNVwmeNYgMI0eODCdBRcihgTdo0KCYMGvcA6+UUaNGucYd4c/wzsEQcwjNduWVV8ZM2krsaQSPDz74wIk8nIfQc//995cpNJq7aQl/mBx22LBh7vkHDx7svJm8uNO5c2e79957E3IpIdmYwy1btnTp4xXkWfj5hh588EFj8lvm2JkxY4bRcMYQ1xDB8Hpq0aJFTHraEAEREAEREAEREAERqFoE8KYvaqCWJ4G4k06Bp1OnTm6uSO+J7+9LuyAaBSDVAVS0DxhQRbgy6sMMuPLGPYkKkIvmO3a951MuPqOeSQREIDEB/733vwOJz9JeERABERABERCBdBPICzrr1ygX6b5TBqWPCzEeJoRWSyZuN+dOnDjRCTXE5fZz+CR6JM6dMmWKMeeN9whKdF46982aNcuFniOvTZo0KddbId7wfFTm4uf0YQ4g4prTqCUWOaIOE67KREAEREAEREAEREAEqiYBPL933nnnpB/+ySefNLx4GGDF/A7Y/vvvb++8806YBoO6/vvf/7ptPGVOOumk8BjzcPp5dhjwxICwZ555xvCqwYYMGWLLli2zyy67LAzxTEhn9rdv3z5MhxW80bmXH0DlD/oBVB07dvS73Cf15FNOOcVeeeUVNzcmotHxxx9v11xzjRuUxknMFfrEE0/EXJfNG9dff70RxvrWW28NnzGbn0d5FwERSJ4A/Q4MouU3tH///slfqDNFQAREQAREQATKlUCVFHjKlaASEwEREAEREAEREAEREAERyCoCjHFjzhwGLCEKlWSpDKBiwBfzfzIHJZ72uWxe4Dn77LNdqOZcflY9mwiIQCwB5hsjnL0Enlgu2hIBERABERCBiiaQ1SHaKhqW7icCIiACIiACIiACIiACIpD9BAgljACTrBEWmiUZW2+99axdu3bJnJoz5/z8888SeHKmNPUgIpAcAb73svIlMHr0aBfyPz5VvFHr1KmT9KCE+OvLe/u3335zkVtINx2RY8o7v0rPbNy4cc6DGRZ4SCcTzWjSpElGeF2MKEW8g5VteGH7cL8MLqlXr15lZ0n3F4GMIFAtI3KhTIiACIiACIiACIiACIiACIiACGQlAeYewnNJJgIiUDUI8H3ney8rXwJ77bWXMUdc/MLcdHhKMdBgo402Cju4y3p3QpkyJ/Xw4cNTSurxxx8P80gYVFn6CJS2jOJzxJzh/r1atGhR/OGE28wv6K9B7MkEY05DH6qXATUlGfOV33333SWdVu7Hy6vcyj1jSjBnCciDJ2eLVg8mAiJQFgK///67rVixoixJ6FoREAERyCgCjNRr2rSpm5MkozKmzIiACGQ9genTp9sLL7wQMx9S1j+UHkAERKBIAnzf+d7LKp7AnDlz7Pzzzzc63BFnSmujRo2y008/3b7//nt7/vnnS5uMrksjAZVRYbgff/yx29m1a1fDG7soI7TuWWedZa+99prh6VORpnKrSNq6lycggceT0KcIiIAI/EOAuPz8U547d66YiIAIiEDOEGDCd0bv8SkTAREQgfIksMcee7gR4FtvvbXttttu5Zm00hIBEcgwAp9++qn7vvO9/+ijjzIsd7mTndtvv90aNWrkHig/P98WL15seM589dVXbt/9999v1113nW244YaleujBgwc7cac0F1P21157rbt01113LU0SuiYJAmUpoySSz8pT/G8O72BxhhCEuFMZpnKrDOq6pwQevQMiIAIiEEeAkSCHH3543F5tioAIiIAIiIAIiIAIJCLQs2dPF66JUeCbbrqpm5Mh0XnaJwIikN0EfvnlF+ft0bhxY+N77ztbs/upMjP3RxxxhLVs2TImc2eeeaYL00a0CUQfQqv16tUr5hw25s2bZz/99JNtueWWoUgUPWnVqlXGoEZvhJNiH97etIXZ5jjr7CMk3+TJk61t27ZuG1GnU6dO7vKi5nJZsmSJjR8/3oWUa9Gihb9V+Mn9vNWoUbhr0ueBc3y+/Pl8Tps2zQ3IJHRdKnPD8Fykjfl0J0yYYA0aNLAmTZq4/f7PggULjHeez/XXX9/xrFu3rj/sGMWn5Q9SPixY/PNFny3+mL++pDLy53EPvOmYF4m5mvDWj39v/Lnxn7Nnz3ZzKTG/Tu3ateMPJ7WdSjngfQbPlStXOta8F+Q5WYMJA3Gx4gQe+Hr2nMs613Kv+PuV9J5Gy8p/H0jTW/Q95n2Kns85bHOOf9f8dfoUgfImkPw3qbzvrPREQAREQAREQAREQAREQAREQASyngCdvX369LH58+fbTTfdZN99913WP5MeQAREIJYA32u+33zP+b7zvZdVLAE6p1u3bh3elA7zqH3zzTe2ww47OFEHEYYyatWqVaE5dhAyHnjggfDSY445xmrWrGlDhw51+xBN2G7Tpo29//77Lr327ds7sYYO/f/+97/uOOf4Ce99Yn/88YcdeuihVr9+fTd/C2ID4skdd9wRikp08Ddv3tylsc466xQK94cIQ75JH8/zmTNn+uTtqaeecvnYfPPNbeedd7Z69erZnnvuaT///HN4TnErzz77bJj3J554wnr06GHbbLONbbzxxvbvf//bXYpA9q9//cs994477ujSb9eunTVs2NAuuOCCUDz45JNPwrTOO++8mNvus88+4bEXX3wxPIagwNwxPBvCSlFWUhlxHSzgtMUWWzjvWcocLrwj3377bVFJ25QpU1zZ8Mye4WmnnWbkLVlLpRwQZcgb80cxnxTr5JnlueeeS/aW7pkQG+HXoUOHIq87+uij7dhjjw2P4+0G72gZJfOekgBlx7UstWrVstGjR4fpXnPNNeEx3iG87JIptzABrYhAORIoLJOXY+JKSgREQAREILsIMLqEigqjUxgJtMkmm7jRJrj/xxsVb+JvM3qGkUJRYwQRI7Y4RqW9JGOS1oEDB7r7MSqKituRRx5ZqOLmR/xQka0IYwLKG264wRjdRCOEBhP5KmqkVbrzBHMaVa+++qprXC9btsxodNAoo8yKMhpFM2bMCEfaJTqPtChzRlKVNYTX2LFj7fXXX3cjlpjLitHcp5xyin322WeugckIJkZSMeJw++23d9lh1Dd5OP7444t9lkR5T7SPyjcV8oULF7ryokJOHGYaPRj5Ouecc1xjqG/fvm5fZf6h0+See+5x5cnoTN49JgT99ddf3UhNGgsnnXRSSiMUK/N5dG8REIGqR4CJmC+66CK799577eabbzZ+y4obYVv1COmJRSB7CeCpQycpHat8z6kTyyqeAIICk8x7i3Zyf/DBB3bAAQc47wh/nE+Ej/33398eeeSRlOdJQ+ggvO/y5ctdkggcCDZFGYITbRPaTlHDAwZhhPruY4895jrFTz75ZCcY0r558sknrX///uElH374YSj68Ey0w7Drr7/errzyyvA8VmhT8H4iwBA+EC+jZI0wc1OnTnWnkw/EBz4POuigGM60s9gPB4QqjBB6hCRFROO53377bbefP0uXLnXtHr9jxIgRjiPbCGa0eTDalaW1N99807WbyBfm88g68zPx//eHH34I2bHfG4IYYoS/Bi+Thx9+2Ak/vEclWSrlAF9ENNpkGJ5QtKV5t2h70r6h7KLCZVH39/PvIBCVpT2e7HtKPmivUraIgXA69dRT7csvv3TvMhwwxJ+nn37a/T66HfojApVAICcEHiaFe++99yLPx1MAAEAASURBVCoBn25ZWQT4p8A/ApkIiED5EqDjvVu3bq7iTmc4HeD+Nxahhsr1mDFjXKxnOuY5Rrx9Km5UEv/8809XmURQoAJPBX/cuHFOsCFtKndU5DhGvGgaCHT+M3qHeNJU4KlQ33nnndavXz+jMsx+RtjQMKHTiHz83//9n3PPRgzafffdyyxIFEWREWGITXSyMwqLkUqMaEO8YkQbo9F4PkY/UUFG2MK9nRFuNL4YnYaQ0KxZMyfI7LXXXm7kEZxofPDsqRiNBUZOIlJcccUVjjkVTMQbQgvQwKGCjAjECD8+GZUFN54BwYAGF6O2qBgTaoG8U8GmocEIPNKnTHme0hgVXxr+J5xwghOeyBNC4MSJE+0///mPazzAjkYRDTlEGEaL0QCB78EHH1zqWOLR/DJCjAk1H3roIdfw4t1DNKExxIhCGmQ8O8/NCDcaR5QHIx8pd66HH+VP+cGV95Yy5jvCOeVpCGswoLFLg4V3C8GVUbKMIKTRRQMjlRAU5Zm/dKRFmfC+MJqO731Rdumll7rRcnx/LrvssqJOM0KZ8E5j/LYgMnqjIbf33nv7Tccy2pEQHtCKCIhAmQjQ6XvhhRfafffd5zqD+Q077LDDypSmLhYBEahcAgwsYpQ9HdnUrZLpiK3cHOfG3en8Z9CVFxYQSWhHUbfGDjzwQOvSpYtbp85I+4i2B8YANTqiR44c6fYT0u3yyy+33r17u7YKdW7q/e+88447nzo6dWPaDVHjN5w8UGciHwz+K844z4s7DEC7+uqrXTsDcefzzz93neS08Wgr4THCYADSjRd4Hg/mGvJ2+umnu1Xam75DnXeRtgRtRzreaXtQz0f8eeWVV/ylJX6SZteuXV1dG/GAMO2IRLSvMEQS5lPZYIMN3HfgjDPOcPuZ24X7097CWwnxjDooYfG22morIy3agN6ioslbb73ldxcr8JRURpQf7DDO5X1BzKOfjPzDA6HsuOOOC+/nV2jHvPHGG67MeUcoK+Yfpu398ssvuzq1Pzf+M9VyYBCfF3doExLaEcNbDPGQvFCGN954Y/ytCm37kJAlDR6h7cCAUAaQYrQ5GHTiB/ql8p5yPYPwKFP40hfC94s2LN87jAGy/rtTUrm5C/RHBNJAICcEHrj4H+A0MFKSGUigpIpFBmZZWRKBrCBApz5CBhU3DFGGijMVRjpgO3fu7Dq+b7nlFteBQ4c3nd1ff/216whntBAd+VSU+SQ9RjkhZtBxzqgoBAQ6yx999FG3Hg8G0YaOXCrKCBo0SKhYM8konbRU6GfNmuW8hqhM4ZlRUiUv/h6pbFNxp5Ofjn0q9XTCU5ljFOP555/vGk00Kmj8kEcaIVSUyfO+++7rJj+lAYP3CuIQndl04ieKRV1SvuCBi3v37t2d0MVoIUZg0elNZZmKOkIP5UIHN+cj/lBhpyxoWFEuCEw0Lii7Bx980DVEKFue0zcMS8pLUcdpTHBPGpy8PzQ8aCyRN4Qc3NcRMHgnEJ4YCYcHFzHCKXtGP5177rlFJZ/0fsRIGlaUH89OhZ7yYoT5kCFDnIBCYgiIiDzkkXuTNyrsV111lf3vf/9zI1QRHHkfEYsQNuFa3gIPefSLf0j/7hFKgu8fgxtyyRicw3fJN/SKejZ+OziXUaPFGY1qxFcM8ZnvLCIeRoOVfd74jZKJgAikhwCdv4S5IfwPncJ8L/kNY9CHTAREIHsI4JHNwBj+f1JvpR7cMqhfyiqGAB3JiYzBZHTi33rrreFhOvJpo2B49SDmYAwSoh5FSDWEF+qUXEv7DuHOG200hIpERtslUTSH+HOp01NfxqhPU89nQB82YMAA18lOu4B6NR361M0ZwIfIRBuSdopvj7z00kvuOvp9/CAg0vaeRHj8e+GCQQUIBwyu45nohGfwYDJGfR6hgfYlwhNGHZH2DO89g78Y6IUR8ot70V6ibeWNAUb+ufH0QODBSydqtM/gT9QLvlPYZpttFooC0XP9ekllNGzYMDcYjXaXr9fS1kT48/2j0Xz6dPlETKEdiNFeZeCgD08HS56pKEu1HKJh3xj8QXuHe1P/J38l1e99Pnh38GDDSmr7I7bwHnjjXfPvd6rvKWnwniDcMLCUtsvVgXDpjWe55JJL/GZK363wIq2IQDkQyAmBh04cfihkuU+Af4q40apimftlrSfMHAJ0KiMk9A3CWNEZzogqRuHQKY7XDpUtvAqosOL1gDcEAg1CAeILxxmlxX4qeIwEo3JGhz6V0HjjfEZAkRYVaCrIVPqpaNO4JFQVvwU0DNiHWJFuo1LHyDbyhqhEflhnPxVTBBwaVowYw1MGLyMWGliwozKIiIG3AqOJLr74Yvd/i+enkpuskRYNEPhzf66lHGg0kC7CGY04uCBkcJzGEvmkfBBcaFQwQo+RTDQGEDVo6PkGDI2YVPIUn3fuxXNShnih0BCigwBxhfL34gijEGn00QBkNCKNFJ6NRt+JJ55Y7gIK+SRsG+IajS/KEAEIMQePHt4r8sA6jU0alIh2iGnPPPOMe5d532kw4+HEOekyGg4slDHvERwR7YgnzTsgK54A7znvIL8dvvHvPb39seJT0FEREIGyEqCujlDO/0W86egoYwAJXpr83spEQAQylwCDqxiY4zupDznkEDcQprSTsGfuk2Z2zuBOvZk2lO+op/2EJ0F8eGwGcHkjjN5tt93mN13by294T2e/ncyn7xgv6VzSpv6F8TtPKDZvtAtpX1C3jeYB0dB7EdG2Q+BB3KHOjVFvp+6GRZ+R9mT0GRkk5o1IBckKPNyP9ke8kV7Hjh2dNw99jbSnGKxHeWBR7xwEEto/tL9oYyAURb87tL3wumKwEQPcaCtixYko7oQk/vC/lrYCnv68F+Qxyjeaz2hyDBCMGm13b/ArzlItBwQnPF54N2DAwrsAX+oEtPt4jpKM9i7fA/oXaOeW1krznnIv3hXqNSzeaKPhfcagPJkIVDaBnBB4cJeUVQ0C/h89/8RkIiAC6SNARzwVWyrjiCxUXv766y9XaWVkLvuppDHiBrGBSisL1yD2cJzvKenQyKDSg2CDGIMQQaWcbSpqhOuiMxthgQYlHbE0ZPAsYRuvHe5JpRkPGSrFVLJp4JAHBIJ0Gc9Bo4JOf+5Phz8Vd0JlMSKZvCE6ITixTf6ooJJvKp+ILzwnHPgkDUQYBBYaAqka1+Byzsgp7sUCZxoLjNRiBBfhcSgrGMMUQYBPRgFSMUW46NSpk2vMIAYhvjGqj4VGAPMcUd6lFXnghYDFCDq4+ZFONA7JAyPGGHVFGRNmASaMsCLMAY0mXPURABFQylpZ5v2gIenfVxoScIIj6whPeAshniB0wYeygiGju3iXuR4vEPKJKMZoO8q2vA32NDBpVBGS0DdmqeMwwo1RmvDj3ZEVT4B3Gq9Cfku8wOMb2v5Y8SnoqAiIQHkQoDOYsC/8piL08D3ku8lobDp1ZCIgAplHAGGHDmnq90wAj4dzcRPBZ94T5E6OCK1Mu4I6PdEUPgwGk9E5Td363XffjRExGCzljTBWPpSV3+c/CaOdqiUbPSWaB+5DhINEFs0D/wtopxCdAY9Pwo4h9GC0A5jD01s0/ah45I/7z2j6fl9Rn0U9G20t8s+ANG+0YWlr0UaItlFoF9BGIBQZZcT9vfcVg7PwKOd/H8IG7SxvlGlZjHYobVLS9cb/XbxVfDSOaD79OXzG96HSpvRGO6Q4S7UcaHcS2YIwfbSRMdpXMGEhugcD/AihVpwhYGGIO7TJS2vR/Cf7nvp79e3b1w32pI2JEZmC91cmAplAIH29YpnwdMqDCIiACIhAqQjQAY7nBR3biC+4HdNZT8c9YgYjyuj85hzMd3pzDiGs8MzAdR4jHjSjzHBXpwMfgYYKLe7fiBMIPFQ+6eynwsw+vCQQj+hs53pEHO8JQ8UaIYkKImJAtELqbliOf6i80TnFpxerqIBSKWUOG0QQQrUhPjMKCzb77befO5/np8OeSivPgpcPz4nLOJXvVL13eCzuxygxRqUhBMALrjRAfLg8xDAfRgy2lCXiEt5ECBgIYzSiaDAiQDFii5HV5BHWnFtaccejp/LLveBBXhFr8ASjXBnBhVjBft4HhEHmRkHQgCuj8ShT1stqPAfhDfzzEG8ZIQ6BCY8lGmNezEJ8pJGJpxWiJGVEoxIvYcqQ9x0xD37RCW3Lmkd/Pe83fChTjJGPlAXfPxqUfD/8c/hr9JmYQLcgVAINRu+1My2YSwkREZ587zgmEwERqDgCdA6z4MmD0IOgjpBPJyWepLkWfrLiyOpOIlA+BBBz+E7SgUoHtBdnqe/LKp8AdVXaTQzoom6NyEO7g0FLftARdRxviHJ9+vTxmzGfPnRtzM4SNvw9SjjN1bP8ObRPippTJTo4j3Xq24SAQ8giDBahrjG86Kl7e4s+IyHFGDSXyFIJB5ro2RA5mb8IQyzBYwMPVNoO5AFhxQ/E8venPBB4aFtxPu0bjOtoe1D3RIhhcCNGOj6smttRij+0YxishyGOEEIOMQVvHubRxeLz6XYGf3iPfPg89nkPMdZp1xdnpSkH8ka7nRDhw4cPN+Yk8ixo++D1RP9ANO34PHjRsqTwbPHXxW9H75Hse0oatE1pz3txh314TDFIsSgxk3NkIlBRBCTwVBRp3UcEREAEsogAFbto5S46uonwVMWZr4jTKY1FQ7HgScEoF1y7GdHrRSAakn5iwmjaCDnxlV/fMEkU3i16bXmsMzoo3gUcYaco888ePR4fIoCKZFkMkQiRgSVqCD/e8FLw5ivvvgzpSEMAippnT8OlPAyxgkYoS9RooOKBhLDDiDjEJ0QwX9HmePS9i15bmnXKD6HJG/ei899b9JjfF31f/TuMyIMlKl9/XVk/KRdfDonS8owSHdO+WAIIm4jKjMqj8UinFUY4Colksay0JQIVSYDOYv7300lDpwgdeXSk8X8esac8f/8r8rl0LxHIVgKIOYg6/J9E5KEuQqcxHajprPNkK6/KzDeDn5gL0otuDOwidD0Dp7Bo+4xBYHSme6OMGeyC0I7Hgbeod0e009of95+JRBB/LPpJvZk0ETfwIOE98vVXvCaIgkAe4tshiCk8B9cxmNCLI8xrGjWe0c9fQ7SC6DMytxCDBEk/vu0VTSN+PdGzMRDB28CBA8O5IhFvfGhwn0d/HmHIuD8e+Y8//rjbTVuVdiuD/26++WYnzPl5YQ477LAYLyCfTvxnUWXEgEcv7tDeYt5Qbwzk8xafT7+feYeibWk8wrz59o/fjv9MtRyYG4n6OHNlUr4IkBjbrDN3FH0EhM/270v8Pdn2HjzJCjxFsSvtezpo0CDnOUdeaFd7TzHERso4KiwWdW+ulYlAuggoUGC6yCpdERABERCBQgTo+KeijKcCSzq9bwrdPEd2UFH3HdeMdCuN+ZBljN5iRDWCS0UYo/ToeGc0HCIUIpU63CuCfNW7hxfy8OLx4dmi8cWrHhE9sQhkBgE6jRncQWcXn3iP0mGH+M88B+PGjcuMjCoXIpDDBPie8X3je8f3j+9h9HspcSczCx9Pc+bN9HbLLbc4D362Eeb8QC3K9+qrr3Yhf+lYJxrBCSec4AY94U3iLRrmiroSXhXR+Vv8ecnW1RmoRUgyDA90vEjovCfk11lnneUWOueZsyRqiPvkH5s3b577pL3gxSy3I/iD94TPCwMECNNGhAUGCuCxREhjPPcJL56s+fSi50fbRR8GIdd4FoSds88+O/TeIHpDVBQjigXeOpgXVfzgO8IrM5gxeoy8JmNFlVE0j4hGXjhiAMU999wTJo1wm8iYU5bfAERePI+8UMi50bB4ia5NtRy4F9EduAdh2ry3EJwIge2NAVpFGflkoS3ZpUuXok6L2R9lhxiGkM3gktK8p0R4YD5eDK+ol19+2b2PbCPq8f4hAHqL3ru475Y/X58iUB4EapRHIuWdBjE3o7Euyzt9pZf5BPjRpRIiEwEREAERiCVAZZg4xoSKw92f30vMV5DxisHVnconDRxGY9J4QQxCYCP8GCIbjUTC3DEaidFujJjimEwEcoEAAg+jOQkDwZxdGPsYRSgrGwE6a2QVRyA6wrbi7pr+O3lPATr1CDNEpwudLyyI/4yEZYn3Vk1/znQHEchNAnTcM/cgi+/Ex0udztJ4b/XcJJAbT3XHHXe4ug0eHAgPeLnw+0kdn4793r17u45mOtUJ7xsVIQiZttNOO4Ug8Kj0RlgvlnvvvbdMv7uEWiM/dMa/+OKLbvFePdyL9gfhuOLtjDPOCL1zOIaQFQ3lxj68jxAlCZ2M4HLyySfHhMzifEKA8v+lLIb4QjoYPJgbiP5JBsgh1CCuEK6LgXI+QgLnEqYNLyVvXvDBSwgv1WHDhrlDzH9DeOpkrKgyQmzifyTfZ8QmIkTQliNPXkwifeb/iTcGWMIIAc6HcvPnMKdPSfN9ploOzPNKXZy26KOPPmqPBx5OMPDeUNybsO/FDfz03jsMEEy2vYpnE21gyoo2M2WA1xe/eam8p5Q7c3p6UY1wbERdICw65c13EeGUd5P2OVZUualO4/DoT5oIZKTAw7OqAZ6mEs+SZPmxlImACIiACBQm4L1gGCnmGxVfffWVq1gyeotKOZVgvGSY4PPEE0908x5RqaZBT7xqRr2RTseOHd2ooyFDhriGCw1B5pqRiUC2E/AePMT6pgOEkaDRsCTZ/nwVkX+EHEJ4eJOw40lU3qcXe3iXWfx25eWofO5M5zIL/8NGjx7t/ne99NJLxiKxp3wYK5WqSSCRqEP9kLkumE8wlVBWVZNg5j01HfOEavMeL/SbMR8oHf6E/fr000+d6EEIN+o/GIO/6Mj3c6f6p6KzGyHDCw+0DXwntj8n1U9+s+lMJ+wanguIDwxOY+AZwgkDyxIZIc7wQGKuVjrl/Rw48eciUvH/b8CAAS4MnBew8DrjWHGhtOPTKmqb7wfeUVcHXlB4CDHnDuIJc4iy7jnixXHuueeGyRx66KFuLhz/vL4uygl483jOiFwIcslYUWUEIwQ0BkWPGjXKMcZTijwgStHeQ0AhUgOMonPxIK4gwjEvDvMCcRwRCuGtqHmT4vOaSjkgtNA2vfLKK+3VV191efXiDqGxieoRLzTF38/Pv4NIk6y1DOabRXTBY9jP7YpYg6XynvKuUTfBePcQTzHa4g888IAddNBBbvvuu+9264RrK6rc3In6IwJpIpAXvOhln8W4nDPnv+zlnKySywICjECgwsLoh3/961+FckxYIuJ2EmYF11CZCIiACFRFAoQkoGLNXEaMimP0Ho0y3MeJZUy4DRpQjMQj7AGhC/jtxKOB0AiMLsKLh1jRVHpZGPFHZ7gP71AVueqZK5YAjVs6H+iYYORmUUZDiVBrvKPPPPNMUae5mO6MoOO9J7wE8335kYuMSOT97xuEiMBTnMloo/HKi0y0ih3wok4iMaes84dVMZRpeVzi1cebD/PiP+OPp3ubgQG8L360c3ndj/9vDFJgocMPk9hTXnSVTi4TSCTqULdj5DtLeQ+kpJMYsbl///65jDXrng1vftoFhIpC1Es014x/KPpYEC4IkRUNLeWPl/YToYP3EZEEMZEO8fI0wo8xbwtzbNLeic57Uh73QewifQSyyhZDiysjjuG5Q/kxf20qxlxGzJfEdzhZ0Sk+/VTKAcEP7y4iT1BmCDyIVSUZdQEGK/KM1O9TMbyvmJeKuXcQt+Itne9pceUWnw9ti0BZCWSkB4+fELqsD6frs4+Ad7fUCPLsKzvlWAREoOIIMNKKhYouFUcmLqVzj8lWaZjhLs7vKQ0e4gITw5pRZ4QDYMQf4o93WeeT82gUlXXUXsUR0J1yiYCPyx7/TAgyPpY7x9gmTEa8cX2iEaGMnPSCkObfiacWu00HPR313hBzWJhIV8KOp5I5n36SaUR5vFyiVllCTzQP5bFOJzQLwu73339v3377bYxnD155dPSwEKamrCF5yiPPSkMEKoMAnasMkmSuFRY6ejEmd8crgpH88ZPaV0Y+dc+KJYCgQ9knY0QE8FEBkjk/2XNoW1CPSJfxu0+orHQZoc74/5IJVlwZFXespLwjupX1GVMpBwQovGBSNcTp0hqiDlErirJ0vqdlKZui8qv9IlAUgYwUeIrKrPaLgAiIgAiIgAiYmwyXSURfe+01J+jQwU2HH2INnbGEumHUHh1fiDZ0ADJKig4AYh8j+IwcOdIJPozYwosCLwlGOOHYm8xIKpWDCJQXgaKcyRPtT7SPkXeJLCrwsC4rTCDeY4fOQIk6hTll2h4fDoRPvHoYYRwVenJF5PHc6ZxmIQY+/8cQfJhEnMmvWTDeW+aTo0NToqRDoj85TIDvPaInXqt8/71R1+N3ge9LWTttfZr6FAEREAEREAERyHwCGRmiLfOxKYfpIsA8Ekwc2KdPnzCWZfRejFRXiLYoEa2LgAhURQJehKFjGzGGBU8d9hN2LWrso7OP2MaIPog5CELs92EYuJ5tTOJOlJ7WRSB3CSAIeFGADvHzzz8/dx+2CjxZ1KuHx+V3vyKEnnSFaEumyAgrg+CD2MP/N2+E02GCY0YJE5KFuRlkIpDNBPDUJsQQ7zlzqxDeyBsDdbygQyiuijaFaKto4rqfCIiACIiACBQmIA+ewky0RwREQAREQAQymoAXYaKxpuOFHf8AnHvbbbe5DgE8exjh7IUdfw6fPs3oPq2LgAjkJgHfKY+ww2hveTxkfzl7rx6eJBq6rSJEnsqiR2c2C3PP0eFNjH68GliYaJwFY446hB4W5lHgk7j/MhHIRAK///67E3N+/vln94mww+Acb9ThOnfu7LzVCFuEoCkTAREQAREQARGo2gTSJvBE43hXbcR6+igBRtKxMNJIJgKZTIAJB6ONqUzOq/ImAiURIPbwHnvs4U7D0+fPP/8s6RIdz0ECCILEyZaYl4OFm+QjRUOyyWsnSWhZdhpCD+HK7rzzztBDK5dFHl88dHLzf87/r+Ndp4PcLxMnTjQWb8xTFy/6aB4fT0efFUWA+XMQcKJizt9//x1z+4022sgJk16cLM38FTEJakMEREAEREAERCDnCKRF4CHcA5VqmQjEE+C9oJEpgSeejLYziQChqiZPnhwT/iCT8qe8iIAIiEBpCODltdNOO1nNmjVLc7muyQECfgAWc+1EPT5y4NH0CBECiHeDBg2qciJPBIFra0TbG3j4eLHHd6YT3o3FW6NGjULRB/GnWbNmThT3x/UpAmUhgJgza9as0CsHYWfu3LkxSfrJwBFz/CIPnRhE2hABERABERABEUhAIC0Cj78PsbwV8sHT0CeTQTKaUCYCmU6A0e0dOnTI9GwqfyIgAiIgAiKQNAGJO0mjypkTaYtVNU+eogqPTnImnY9OPI83qxd9vBfFF198YSze8PTZZJNNYhaEH8QgmQgkIoBog5DD3LHRJd4zp27duta2bdswdCCCzoYbbpgoSe0TAREQAREQAREQgWIJpFXgKfbOOigCIiACIiACIiACIpAWAsTwX716tRuBnpYbpDlR8r506VLDo7JOnTpWvXr1NN8xt5P33vXy3Mntck70dIg855xzjgvXpjDJsYToTGfBs9Ebv52IPlOnTnWd83TUx4d349xatWrFiD5eBGratKlPSp85TmD27NkxAo4Xc5YvX17oyZnziYGvvCfMG4WYo3mgCmHSDhEQAREQAREQgVISkMBTSnC6TAREQAREQAQY5cukznRGMxKT+W3mz59vDRo0cA353Xff3U3uXJGk/vjjD/v444/d6NEDDjjAPvjgAzcZb3TUckXmJ/5eixcvNkKFLVmyxHXcV2S4MMrpt99+c3x69+4dzkXz448/2vjx48P5aWbMmOE6+Bil3aZNG+vatWupy5HnnDBhgsNAyB/meCAfv/76q3tXuEdJnTyIHMwLRpjTXXfd1RgFPH36dFu5cqWba6N27doxmLnns88+azvvvHNWCTx0pH780Uf29jvv2Ouvv24Lgu8StmHDhnbqqafa2WefbQ2Ddd7vzz77zIW5ad68uWPSvXt3gwOs4IIgJFFozWuBuMNC56LCsq1hUtX+ek8evLj69++vUMnFvAD8HrPwW+uNORl95330ExEIMShqzHXmxR4+CbnlF+oGrHOOLLMJ5Ofn24IFC9z/aT79Ei1/zokaEQD4nx4tf79eo4a6XaKstC4CIiACIiACIlC+BFTTKF+eSk0EREAERKAKEaDB3759+7Cznk4hRnRuv/329uGHH7oOdsK7VKQRDpNQNAhODz30kG222WbOEwJvCDrQGTVKJ0RlGcIE96eDq2XLlhWaDTrpEFbeffdd69Wrl8sHI22ffPJJu+SSS+zpp582JjM+4ogj7Nhjj7Wjjz7adcaVhRedQQhI3IdwQPvuu6+LuY/whgBz11132Q033ODyMm/ePCdOMAKc6+gIpBwRLXivHnjgASfWffPNNy490qRzkY5IrsXwduE+CGeMROfasuS/IgoIQeriiy+2ZwcPto4tt7AdN21pQ04507Zq0tSC3NuonybZwy+/YoPuvde92/XzqlmnVltZyw0b2cxRX9p/nhhsp+evtqsHDLDbbr/dpv481eqvX98Q8Y466ihDaEVUrKqGuIPRyS+rmgQQ9/Deeuutt2zo0KESeFJ8Deicb9GihVviL00Uiovfb/7XFGX169cPRZ948cdv84mXkKx8CfB/04s1/pOBOX7dfzKooijj/yt1Ky/e+E/EHZkIiIAIiIAIiIAIVAYBCTyVQV33FAEREAERyAkCu+22m+uAnzRpku2///6uI5/OfDw08NSojBGbHTt2dJ4LL774ohMD6HxiYl/mYTjmmGMqvbMfAWXy5Mm25ZZbVnjnFZ1leDIRkseLHnTmIIogiuENghBA2SFAIc6xryy26aabOsHlo8AzZY899nBJIWhwX7yDvvrqKyfqcN91113XKDfyOGfOHOvWrZs7n7zQuYiHCiOGie+PmMj19913n+G94r1V6Hgi/999950r97LkPV3X8vzvv/++jRw50vEdMmSItau3vk2+9V6rHjxrXlAeQYFYXtCpmh9wOrz++taycWPreedAu+nQnnbYLp0tr+Y/gk0g2hUEHjs3vPaSnXvuuXbjUcfaYaf/n/3+10J7+MP37cTnj7YFQYfec889Z3i0eU7perZMS9eLO3Tuy6o2Aby3GACAJyDLtttuW7WBlNPT06nPgmAfNX7DEea9YBD/yfFffvklekmhdX7zEXrw/OH/FIK//yxuvSoIQwg1eASz8D+lpHXPn8EuxZlnjmgTFdv8OgMwGgf/j2QiIAIiIAIiIAIikEkEJPBkUmkoLyIgAiIgAllFAAFg2rRpLuwXozkRd+g0Y76D6667zh2jE74iDZGATjw8Oy6//HIbNGiQnXzyyU44qGiPmfjnRvhC3EGk8POreKEl/tx0b3vPFvJCRxGGOMJSnobIh+hGKB/C+eFBhfn7E06MdYwOuxNOOMGFI6PsEJ0SGef762HKexjvKUaZZ6KNHj3a9txzT2tcu45t13xTG7N8mX0feOjcdemANeJOo4aWt24dKwjCHSLc5AXCV/6SxXbZc0/ZGxddZtu0bmPVGwSTUAfPHahwblm9cIFdfuiR9uaY0VYzCM3WNOgQbRZMgD4oOLcg8Ox56sMRdvIxx9oW221rb775ZpWaxNoLPArNlonfhorPE+8BYr+8eNLPHhGgJCEg3puEwRheiIh+Elo0FUPILk4AYkABAwfKc+F/aXkuhLwtTrThf18qhkDDwAgv1PhPBkT4dT6rgjiWCjedKwIiIAIiIAIikB0EJPBkRzkplyIgAnEECLVUGd4RcdnQZhUnwKjRU045xd5++21Hgm2ECyZkxuOC+UIq2qZMmWKPPvqoCwX2+eefO2+igQMH2u1B6KozzzzTunXrVmleDOSN0bGwWbZsWUWjcaIIYVfoOGL0NL8jfJIXRDHCtBDubNGiRY4R53ohpbSZpSwIx4OQw2/We++950RAOpLw/OrcuXMYPoy5dZ566im74IILnECHaIjIQx7o+OOTPNJpyLvGKPwuXbqUNmsVfh1eRZ122cVu6X2cnbzf/sH98ywvKIux06bbJc88aa9ffJlVW/S35QUeSmtsTSjBaUEZnbbXvrZNEL4Nr57Vv0wP855Xr75VC0a3FyxZalcd8S8b/MlIO6LjLtZwg8ALKxCCEIhO6HWsHbfHXnb8oDusSeBBNiUQP2Gb6+bFHXnv5HpJJ/98hGpjkRdP8szSeSZiAqIDS0mWSOyI915JdA4en/yvy3bj/yeiFYMZ4BX1ZGJ/vKAVv53tz6/8iwC/294YUIb5T79fnyIgApVHYJtttom5ud+Wx3QMFm2kkYAEnjTCVdIiIALpI0AnOo09OotlIlBZBOg0+c9//hN20CNcMDKW/cz9UdLo3XTkm3sT4ov5VxARECzwkmE+HrwnGGFbGWGqECfwLqJjnU4tRI/K8N6BT48ePZxg4ucq6tu3rxN6qIATUo0OsZNOOsnlt6wCD+8BYXgQePDeGTdunBP+8GJhrpx+/fqFHBjNfeSRR7r3hpA/vpzIA8cOP/xwJz61a9fOze+A6OPDvqXjXSrPNBltTUjDR0872w7rsrtVwzupZi3L/2uBbRcwx/78a5E1DubXwZ9p9er8QNBiAus826zhhtYy8MgpWBF4WgWC0MpVq+2nWb/ZRsHI6wbB+1xt2VKrtv4GtsVGTdz7vTxgHVxsq4PwhAFcq964kVULeD7570vs3iCUW6ugHGYFI+Ir4vvJ+1ZZgxG8wCPvHd4umScgLx5PIrs+vYhRmlxHvWEQgfhdKk9vG9IqT28g0vKCjn9u6lYyEahqBBB18LiMijtVjYGeVwSyhUBx39OePXu6x/Cf2fJMymd2EZDAk13lpdyKgAhECHgPAIk8ESharVACiBTMveOtQ4cOfrXSPlu3bm0siWzvvfdOtLtC9iHmENPeixaVIe5wT+bUKWleHUYHH3bYYeXChfl+WLzhQYUxB1G8RT2+Nt544/AwnV0IY1GvE8LKZJO98MILVjPQaw5u18EKAoE+P/CaCgLNWfWNAyEr8Jg6omMne/3bb+yErnvapJmzbPj4sTbs+++sTiAG9tqli7VvubltwSj3wKmnZiDuN2mwvg146Xm7/OAjgnBsDS1/7hwXmm3C7Jm2Ouhs/CvoxKxbq7blVSuw/DlznWhUI3j/zg3m8JkXeAkxXwbhAtMtviDssVT0/ynfyJT3TjZ9Syomr3jwyKoWAcQRFubykYmACGQ2gUSijv/d9p9bb711+BB+X7hDKyIgApVCgGgQ3ojS4O2tt94yP+iKTy/y+E9/nj5FoKwEJPCUlaCuFwERqFQCEnlKxs/ISkZvYjTw6SxOZJxHRyTeAiycx3wkRZ2fKI3S7PMjP+OvZcQ/YaiKmock/vzK2I7mnXUEBBYfG571dPNL9NzRMmSkLnmojHzE582LO/H7K3obPpRN1Hz5eXb+WGWVob9/aT9//fVXdykeSZlghAe84YheNmPOPKsWsF8/mDC8/np1Lf+3Wa4sDtqxvZ3++AM2J/BKemvsaHvs1HPsXzuv8T4ryC+wzyZPsgkzZ9iBwXxGNZo1t4Z/1rI+gSfQm2PHWK/Ou9p6tde1erXWtf6H9LR+Tz1mVx16lK27Tk1bN/gNq1Gtuq0fhPCpN2OGVQtEogE9e9mnkyZY3cAD9Px//9sGDBhgdYLj6bLK+D+lsC3pKs3cSJcOQToRWRQ6JDfKVE8hAiKQ3QTihR1+p1kQcyTiZHfZKvdVg0D0expdx3Ma8QfRh08v9ngqEno8CX2WlYAEnrIS1PUiIAKVTqAyOs8q/aGTzACd1h999JExQS8T9uJt4id5j09iRtD5OWrUKBfPGTEAD4J9993XNt988/hTy22bsFOffPKJ7bPPPoXSnDZtmp177rnh/DaFTqjEHQhhPwfzeEyfPt3lnblavvjiCxf2pHv37vbaa6+5+PCE0IJ51IMj3dn+448/XBiw2bNnu/lZxowZY9tvv71rIKb73smkzzwzhGojzCKfiIgVZYhdCB80ovFs8CIPFW6+HxjvPnmkAo44wvdgu+22K7WnB79PhGjD8AzC8wYBkBB6hMphRHWjIPxYcYboxFxBfEfxEuM6wsjxyfeZkHecg/FMvI+8g+3bt3fPUFza6T7Gd5zvMXn6zwtDgvyZ7dyyld15bF9bGXBouH59l4UN69ezu/ucYpe/9LS9cO5FtlHDDcwCYcfbUY13Db5fq5m1J9gV7F9nXds4YHfw7TfZ0G++cGlOnjPbPpn4ozH19qF33+IuzQ+41Aves5fOucjWD0SgTfAaCubheebc8+3AgTda9+q1bcMgna+++cbatm3rb1funxX9f8oLPArPVu5FmRMJ+jBtvCcSeHKiSPUQIiACWUogkbDDb3S0gzhLH03ZFgER+IcA3+fod/rNN9+MEXok8uhVKQ8CEnjKg6LSEAERqHQCFd15VukPnGQG6ER/8cUX7eabbw47gMeOHWvjx493nf+IKHQM77XXXk6EoEFBZzed34SLmjp1qo0cOdIOOOAA1ylOh/Ebb7xhuwQTpSNoEAoMoYMO6nnz5lnXrl1t4sSJTuigc5xzSJvOcubfYFJcvHK+/fZb1+FLB//DDz9szCnC/ZhA/v3333cd4cxnkyneB/G46aBHQHnnnXesWxBya1EQYoqO9UGDBjlWhH5q06aNvfvuu8ZzdOnSJT6JtG3DGmEJlo899pibhweRArGPTnbm5alMmzlzpmPFewebihR4YMC8N4ycYh4eygzvtmeffdZ69+7tyhMRhvf8uuuus/79+zvxpCy8+I4hsnIvKva8L+Rh+PDhxuSbhC67/PLL3S34DiHWMIk068w9wHcEVnxvHnjgAbd89913Lg0EK9JHwEMwwriWcuYdRRDiWu5dGYawRRkzf87c+x6xGtWru2wwv06bS8639y+6yjasvyZ/5PDmt1628/btYY0bBWGEgt+KguA5gsw7UYdwbjVqVLO8usH+YE/BwgX2ayBy7bfdDnZXn5Oc+MM51Zy3Tm1bN+BIec9duMhWrV5l+9xyrT128tlByLZq1ixIc8NAaKserHfo0NF+e/Rp+3/2zgLAqqprw2uSHBqkGUBCGklFaUQRg1BERQzET0BFAfnsACywQfwNEAsQBJP6DERRQkIFA2kUpEF68l/Pvp7hMs7AMDP3Tq2ll3vnnH322fvdJ9e737WqtmgpK39eLdHR0QGDKpj3KRxG/i+SAeuUVZwjEfCODY8IzJGdsEYbAoaAIZDDEeB51JvNz3XZiJ0cPqDWfEMgjQhwrnvmXQcgeYzo8VCx7/QgkHKcnvTUZNsYAoaAIZDFCOA8g9AwO44AzmFyTVxzzTXy3HPPOWXHVk08jmoGpzPqDhzEGI5hCJjly5c7xyBEEM7mpUuXOkcxyhCUQMw4wSlP6DRImffee09+/fVXV8/s2bMdgUA5nNCQN56CyO1E/4HcQSkEeYSDGsUL+8YRPX78+CQH9apVqzItD4q378z6Blec6uTgwSAFcMaX10TuKEDI6wExhloEJ34wrYmGsLrkkkukQYMGjiAjLB/YEoYKZx6/s9LKqHoCggNVE8dXMA0sGA9ILo/0gAwjdBw5cXDuM6a8ZHMu8DehMTKSp4U6GjZs6EhAL/cPCiEwgEj68ssvk0IogscLL7zgSNOvv/5aPEKAY6ps2bLu2IK44RxGCcT2nMscfxCKfCBUUBzRx6y8HkIuRWtbPrrrv/LDY89IuF4vQitVVsIm1OG95NEn5Ne/fOHZOAaOxcbJLiV0qpY5Q0L0eqCFZMEvP8vytetl267djsChXKISXXF/bJG/du+VIVPe0hw83aVMiWJSvkxJqVimtFSoXlWiVNUUoZiEh4dJuZLFpULpUvLF8IfkhgkvyZGYYxKr152QuASpXKKUHNPrWoRexz59aKQ7bwJ9fth9ilE0MwQMAUPAEDAE8jYCnlMXFAYPHuw+Hvmet5Gx3hsCeQMBSB4+Xr5M/2tC3kDAepnZCBjBk9mIWn2GgCGQpQgEwnmGQ5XwTcygD7TzL7PBIzxS7969ZcaMGY5MQX0D6XDXXXe58GI4glEueIZDGOczzm9UKaVLl3b9xjFO/2fOnOlCjoEHZcCjZ8+ejuyBVECxAvEBYQMBhHKkadOmzpkNKYLhkOYFBtIJwqGtKhpwbONoX6ZhkmgfbZg3b55TCqGwgAjKbuYRBLQLHAjvBGkBxvT9iiuucKGpbrjhhqA2HZwhDyZPniw33nij2/fQoUPdw2OLFi2SiI2gNuqfnXEuQeyhrCDMnT+GWdEe9kk7OE8wiEuOda9dfEOuZMToMyGQODZQV3nXEDDAWO8t47xhzEaPHi2VK1dOVW3FtmxD+2g/5w/HHB+IKeq57rrrpG/fvkl9yUgfTndb8CR03G0XdJE2tevAgEqitjNBz21tuCQqpsULFpKODeqJsjCiMiWJjAh3nz0HD8D2SGhUEWlZoxbF5YKnR8nwyW/J0t/XyU/rN8r9702Rpo/8V4Z3uVTOqlRR+6ikUcVKEqaEUqLu+5iSS9vW/C5/7NgtMXrtIOdPuZIlXF6e9Tt3KKHmu56UKVJUIsM1RODRI9JQ21k6PlHmz59/ut097fKBuE/5NwL1DmaOIn9U7LchYAgYAoaAIZA9EBg5cqRT7nCfhtyx+3X2GBdrhSGQFQhA8njXASN5smIEcs8+M+a1yD04WE8MAUMgFyGQ2c4zQl0988wzLqQSzticZIRbw2GJGgdFB45mVASocVDfQAaAF4bDmJBK1atXT8qPQqix9u3bOycxIclQOqAW4JucIDisL730UucEZzvUESh3IIZQGAwZMsQ5z3FIe2oCluOIZr+0D2KH9fwNIQS5g6MdEgmlAvVB+mQnAytyoBAWC1xQPaFYwKldoUIFp3QCG/pA2LpgGpg++OCDLjcQ6imIOh4cX375Zfniiy+ylCzj+IKUQBXDt0dsBAsf9kfoM8aOY4pwcYwhGIEby1HAQJCxjGM8o22E8AN3jNBphGbj/EHxxTECMQiphHG8MGaXX365axttwGgD5yzrOd4gi8i/g8qNnDzZzWapyi+fhkp7tEcvSdS+hWr7Y48claPHYjSImnI6ep6H6nUoomJlCStXXlmuWMmv5c6tXltmLl8qxw5oaDZV9OQvW06anFlNFj/8uF4jEiVBr797jxySnk1byIoRT0n35i0kgvoJQbhju8Qqebhl8x/S9+UX5ctfVklUofyKte9Rl3Bs59aoKb+pupDcP9jzfW6QcF2eCGGm/7U8s6ZTVLmVAf4ns+9TAW6uVZ8LEcCh6JGBubB71iVDwBAwBLIlApA7XhhVz6mbLRtqjTIEDIGgIeCRvah5IHm4TpgZAqeLgOXgOV3ErLwhYAicEoHsoLbwSIsCBQqcsr2nKkDIJnLMkAydUE0oWQi/hfqFWfbYpk2bnMOWcuSdITwS+WnIR4PTuGXLli7vDWGgmGHvGU5lHvLPP/98R7h4yzPrG8IBBQKECQobyBPCp0Vr2Cjazux/L8wY+6Tdt912myN+Wrdu7ZzcnTt3dn1GDYCjGcc0uNIPCBlIok6dOrnv22+/3TnHwYYxoCwKHEKX8Ru77LLLXDvY18CBA10YMdoI4XTHHXe4b/6GjKpatarbd7BDebmGnuQfD4fu3bs7Zz2kDmQBaiXCZnkkFefC1VdffZKaMn8V5AFj7Rljw/hDCpAvKasMzPhA7nD8QOpB8gTbIFNQj9EWjjOOS45tyBNIUM4NSEdUbpzPGTXOI0gZQqxBHhHOkHrbtm3rSBpe7j0c+OZaQe4pyE3G0jPajRqMNnOMQVBxLQpmfievLSf7hgTvo+qhK5u1dP0KPaOsJPyxRfb9fUB+3/6XtFISOF7xCC1VUhI1F0+CEugqY3JVXn3ueXLpc0/IVS1aSXXNtROqBE/I0cKSH1VhaIiU1WtG5TPKJOHlNtJxTFCs4hPiZd3Wv6TPq+Pks//eL1EFjl9nlSJTvihe1itJVkav56iFMA/3xL37XK4fcgX9oARgsCwz71P+bba8Kv5o2G9DwBAwBAwBQyB7IOCROzhxmXxlZggYAoaAPwLedWHWrFmO5CEfrJkhkFYEjOBJK1JWzhAwBP6FAM5rT5WRHUid5A0MlPMMxzSEDk5YDCfr22+/7ZQxhEXC8UrS9gceeECGDRvmnO3MskcFdM4558gtt9yS1FQIno2aAwfHMgRHZhvkDXV7hpOfD+YRLt46nJ0QNnwwtiVfCkbIJz7+5h9OwNsG5Q4fzCOyvPBR3rY4uj0jZ4hnkBD+dUIAYdmN3KFNONkhdfhgkAUoo1gOjm3atHHLs+IflFR8UjLa6Dm1U1ofyGXsF4LDIy3AKthGG1CgQVp65h27EBPe+HEuokzLDKN+9olRv3f8QwBzLfHwYD3qNj6Yd2zxm3aznbcty7zzIxA4cl1P6zWd89YLv0i7UDxyTezftpN2mCWq2dHzI3++SLn1rddkxqChUq2sXle275CExAQ5fPSYHD6ieXGUoFmzbav0O7+jvDb/cxl55VUSoiqlUL0OyaGDMqBDZ7n2lbHycLcrpGnV6hKn5b/69Wen4mEvO/bul4FvvS7zht+XRO4wpuT1idfQazsP7JebJoyXOUPvl/0HDsmBQ4elIKHtlEg6pkT3kvW/y8bduySiqo+0p87UDPUUY5cZFqj7VGa0LaN1aIo3DbUpOpkgUUOFpkzmIm7TNG96n0qUiy5KuczptAOu0I8XPZ1NrawhYAgYAoaAIRAwBJiVz6Q+I3cCBrFVbAjkCgT8SR6uGz169MgV/bJOBB4BI3gCj7HtwRDINQh4Dr/Tcf5ldecD4TxjNr7nCMbRhyIGbMgtww0YxyxOzokTJzpCB+f/okWLpK3O2IcIIjeLR7Kg3IEsokygjJBUhMZCVUOYJ5yeqHfAhhBQOI9xFnuKAdQ2jDHhqVhO3yiHmsbsOAI4eMESBzcYMobeOHrHHcdJVhBU3rkKecBY0kavncd7EPxf/mRG8Pd+fI/ggxrP32gbyxlDvr0xpRzYZcRSImGoPyN4pFSn10b6AHnMsRetiqTTNfrPJy2WvA9cHwl7VrJQYQnJpwpKVfBhUQULSISGS+v24mipXqqMdKjbwOXHWbx+rew9rARO+84SpWrAT374Xn7bvk3u79ZDCu3fJ4m6TCkuqV6urLw38E75cfMmmfXDcqlUspRc2LCRqzsuLkEOHjsq+zR8W1R+VJu+6+mWXXvkhz82SsH8kZIvLEIm3DxAFm1YI68qgVQ2qqg82etaOaDnR35dX7t8Bcmv43zntMnyP51Vi/ItNQObzCJ42Id3vcgMxWlqbT7d5atXk1/NFzowtW07dYqTF15I/d6lKdXkySfzqUI1Tgken0oreV2ffx4i48ejqotRgif1upJv5/+38onyyCMhMn9+mKoUw/QenCAtWsTLqFHxqpbLOGnkvy/7bQgYAoaAIWAInC4CXl4NI3dOFzkrbwjkTQQgeQjlzbUDM5Inbx4Hp9vrE70bp7u1lTcEDIE8gQDOrJxE6iQflIw6zzzHPQQIRM0y9VrhiIOYIeQVTn7CMC1ZssStxxlMMntmsRNmjNwskECEVYL4oY6LLrrIOZAhX0hITm4O9oPTNzONNpADhBcKQojN0+nUfCZMmOAIKbD57rvvXKi0jz/+2OX0eOqpp5yq6FPNo4HigjBk1IGzGPLHzKfaAreNqr4ixw7h+7788kt3nhCSbfr06U4BxTHTrVu3E5QXgcYPMo9jlO8LLrjA5V9q2LChy48U6H2npX5CoaHughxFRebln0nLthktQ7hAiA9yUjFO/ucbywltBUFLG1eo/ACFGQqcRo0a/YsQSmtb2Cdh2SCKuB5gLFuqsgXy/xCS7VTnFWQC5y9t7NChgwshx7HFdYMcPl7IO+qmT1xnOB69kHMsD5btV+wcriHqrIec+UfpMvazOTLumpulvJ4r63dul027dkq4kmqDO10sNcuXlcJ6TeVviJaeY592BEqsKnviVGkDmRUZHiblSpaQsiWKu2vlvoOHZJ8qcXbu+1s27twhe5QkitV71W+KEyRPqOIQlT+fXHp2U197NMRdCKrAuFjpe347Wafh4vq9Nl7uvbib1FVlW5mixaR5nfoy6/YKcvHN/WXkC89nmoorLdhn9D6Vln2cThm9HSnJd/x+tHdviBvKwoUT9V7gqykhgfXpI2VOpy0nK8vhdcEF4TorOkzPMdHrnCrB1oTJtGmhGi41THNexaiq9Hg/TlaXrTMEDAFDwBAwBDIbAY/coV5vZn5m78PqMwQMgdyHAGG8n3vuOSN5ct/QBqxHRvAEDFqr2BDI+QjkdGLHfwQy4jyDsCFcE/lqIGHIIYNBzEB64HxFkXOW5pagDI5cfhO2COULTlbqIEwTIc/8ncooZMiX4oVb8m9zZvwmBxCh5AibFhUVpbOoe7scQTiYly9f7nLe8OJBTpBrr71Wnn/+eddH2gNpwbY4j+vVq+eIKcgeM9+Me8gzQnlB5lx//fVO4TRixAjp06ePc9yzDpIAcsU/tFag8YMIIJwe+0RFBnkAUcdxiuM/EKEAT6dPkBqQopwnXhi/09k+I2U591CckEOLc8E7FzlviXUMaTtgwAB3nkJ0Qt6Bl1cuPfuGnOBY+fHHH5MIHq5HEDScZ/4KLxR2XDcIj8hvxhGMvHZPmzbNETzURZ0snz17tiOgqA9jW8gz1m/YsMGpCDPS/tPt81G9LmLxCer4x/se6VM/jfn0Y5k79D4lafS6UqaUC88WEhImIeEa1rCg5ufS9kq8Kqd0s8olSsncH3+QD1cslR2aDwkKoZRevy5r3Ew61q0vW3btlqdmfyBrd2yX82rUkrPKV5SSuv6erkpGq7pn3qof3HXsHiVvapcrL6WKFRU4iURVJYYqYReuCsYzI8Jk9t33y6VPPy5jevVRQkgJpAL5pFiVqvLaTf+RbhpKs3379v8KTUnfAmUZuU9ldpvq1QvRmYM+9RV116oVoarOUFXsxMjll5+4N8ReOslQrzmJet9ImUw5dgxVGWUkzYQLdUZGJqriNESP9RP36f21eHGiI3f0Fqv3tqMSHR2ixGecNGkSqSR3qMyZE6LXZI0IuN23hZ7OjgjSS5Dmx/MtI4KiCrl0O9H7narNokQnbxDqLdHVRykO63XrEpX0DVHC0bcd/SZlU2hoousT+9i9O1Gx4jrjO/zpA9Fc/VLv+TbWfynLNmeddWL/9u1L1P2FaLhG3343byYEKMRwiGsfUQsxr82055/IqL4V9q8hYAgYAoZAtkHAm4GPs9bMEDAEDIHTQQBS2CN5TMVzOsjlzbJG8OTNcbdeGwInRSA3ETv+HU2v8wwHbVrCkyXPUcO+cbb6m5fvw1vmnxPHW5aZ36iHvLbjOEY10KtXL0dEQT6hngAX2v71119rKJ3zXQ4QCCkIApQHOLnpB8eFmQ8ByC/yBT3xxBMyZMgQRwiAFSQeznSIwA8//NDF2obgCaahHqMNX2hyC4g5Tx12xx13uLFv1aqVWx/MNvnvi3OAGOTg509u+JcJ1G+Oa8gujnd/0gOSDrUThBj5izjnaRttpbx/2dNtG0QWx0XykF6cTyhy2IdHutEuEvBCGK5cudKFCfMIHq4VtANyDJKM8zM6Olod7i84lRjtxCjDvjaquoyywTb6kKDkDp/EI6q++SenEEqP+PgEJVJ8bQzXEGl6ARK9GEmiXoPi9XvXvv3yzJxPJEQLnVG8qIy5+jrJr2Vilfj5Y88eeffbr+Weae9Il8ZN5N5uPaXmmbUkn3rjQyJ4nFUnuZ53iVp2qKp+/vpzk0ycP1/GfT5Hnrm6r1QuXUrHNUxJmuVMAABAAElEQVTi/9om4RUqSWjxElIwbpc81buPvL9kseYM6ijHDh+RAkVipE69BtK5Zh2B5Hv00UeDCmF671NBbaTfzsaNC9FjNsIREihozj03Tu65J15VrMcLbdkSKs2bR2p+Oh8z0rdvjIwZk+hIkOOljv/S01Fz2EXoce4rX6dOvLz+epwSJ8fLeL90yJMMjhDjtjtlSowqZyFoWBKiJGg+R9LMn39UJ12EKJmbKE2bEv5PtNxReeedELn77nyu/YcOhajKTxkatQsvjNOwGPEyaFCk6yPh3x58MFZDrYqsXUt/8+n5m6hkYJyqhnxkZnR0gjz7bIzcdlukKnhD9XqTqOd0rB5Lvsay75tuCtdJFr7XsHLlElQpG6dh5hIdeTR4cJh89FGEntcQz+F6T05UkipOnn46Utscpypc33n98MMh8uqrkdKxY5y8917wz3UHkP1jCBgChoAhkCoCHrlDJAX/PKOpbmArDAFDwBDwQ4DrBtcPJiJyPTGSxw8c+/kvBHxvTv9abAsMAUMgryJAKDYc/7nVmY/zjD7mFUOtANmA4UxG3XHmmWc6BzaOY0LMETqOdTiEa9as6ZQDKD1QfuAsZnscxl49eQW7k/UTLFBjDR8+XB2AFzqSrHnz5s6RTzg7CLG+ffu69SijgmmMGcqNzz//3Kmy2PcDDzzg2nnOOedkiKzIaD9QyhBPGAKFczE7HFMoZfYoeUDYNhQ8nhLG6yt4ZrZBrEK0Qbbed999SdVzTvbv31/Gjh3riNmTKfvADgIHLPlAJPHhnIfIIgQdarJAtD+pwSn84LyIT0yQPYcOyJEDByVR8QtR0ufZa26Q+9+fItt275O9fx+QnX9ulQSVPyTq/SZe8+ds37NX7nx3klzQqKFMGXqvtGnbQcrXbyAlataWkkrGJGidyzesl+l3DJOxtw+R+g0bqypHr0t7dkmC5v1JUOImbtd2SdC6QzXfT4U6DeTe7lfI7Z0vlkFvTZAde/e71obotSxBj8NQVENqjVWx84EqhRIUz5i4eG2Pyjh0HPq0bS9vvvlmlpBkOeU+pZcYvbZEOlXJiBEx0rlznCxYEK7khY/ocADrPxs3hqryJUG6diW3lcikSZEyY4a39sTvBQsSNWRoPlUbhmoOoFgNYRjnFDoQIinxlS1ahLiwbDqkqm7Lr9uGyvffiyp4RPcXoiT36Z2/334b7tJG9eihx4i+Jc2ZE+7InSuvjFWSKk7voaGqdD1xftzu3RDq4S6PEGQM/e3WLb8SSQmOpIEweumlSKfYobf9+vnInS5d4uThh4854mjcuEglmU7EYuZMcAzRe3GivtD77uOQQjt3+n7PnetrBwSUmSFgCBgChkD2Q8AjeCw0W/YbG2uRIZBTEOD6AdHD9YRJkmaGQGoIGMGTGjK23BDIgwhAfHizh3Nz93OK8ywzxiBapy+TiwUjXNt+DXdEfhbCtTVRDxikDkQEeYFwvhO2jfwjlKEsicZxGrOMmflmPgQgBchlBC6Xa7wi1BY8cKHUQAECriyDaOnatWtQYSN818MPP+zC681QLyptJNTUnDlzZObMmW6cg9qgf3YGIcGxCLkByQgJEWyjDX8pGQCJA9EEwbl69WoXxowxJbQZ4QtRyIAbhGhy5c3ptpnzCnUXebhQU02dOlUdtDtdjhzGqpafLGGXxowil9eVV17p2sA5iNEG1Hish4QipCJ1cZ6i2MpOhroNnKctXSx7lGyJ/2urhGpcrMvPaSXnKIHcb+LLqtL5VI4pLh6BvFOVO5A/AztdKJ1bnCvh2r8ElWPEawi2eM2V8+umjXLP1Hfl3dvulCZKSCcoeXRgyxbZu3OXxBw5qqHd4lUBFCO7duyR9RvXS4xinbBtqyRq3p42mvfovBq1ZfPuXf+QNUpaa74e6kdVBOGwT3/TZhcfTpVVKIEaVq8pR5R04vjICssJ9ylNV6Wqt2N6TMcosZIozzzjU3lu3Rqq59lx1Bo1itcy8UqYxcuNN/rCvk2f7lPIHC/l+/XWW2GIuuTWW2PklVcSNP9bvF5XExzJoxzsv0z5TL22xWkY1Vi33eTJEarGyydt24YraesjQv610UkWNG4cryRVnCpjEpSg8REnAwbEKqmTKO++6yOoyPdDaDXPOIa+/z5GXnwxUUl1bbxas2Zx8vbbKI8SHAGlp7CGhmTChKiyJ97hNn58vNx+u+g9xLfNsmUnklEoczZtOqrkUZwSZCJ168brtUA0r5DoNUVcCLp8+RI1p4PXEvs2BAwBQ8AQyC4IeOQOs+/NDAFDwBDICAIeScy7vZkhkBoCJ05BS62ULTcEDIFcjUBuDcl2skHziCzyy+Rmw5FOHhicwm3atHEOTpyqJLfv2LGj+xvVADmGMJQAEDnMwiekG+twelMHigAzHwIQZNFKnqGwGDVqlCNGceB37tzZqaDI7wKBgcLC34EfDPwYu4EDBzrnOWHGyAGFqgNVEcQGY5xVRju80Gw41IOtLqHfEBDDhg1LCkUIRoQ7I0wa5wgh0yBlRo8e7cY4o22kz3Xr1nWKHPZVRwkHxgjSDQJn6NChScPB9ahdu3aOwEFV5F2faAPtu/fee90xR04tyCfIKM7x7GScG7T1g+VLNOxZByVddosGR5NwxfXua6+XwUpS4caOUOIYgmXt9m3y+Y+rJDYhXmqUKStxBw9pchElbfCEO0uUB6ZPkRevvVFKq5InXkmXL376SV5b8LmUiSoi/dt0lEpKCL0y/3Mpkr+AdKxX39WLJzzkGGRCiNzSvqP0ful5mdR/kJTR0G8hSo4liip1dN0SjbNVq2w5CQsJlciwCF1Oq5Qzijkq1cqc4Ug0zqGssOx+n4J0UI5SCeUwJbNDVf17nKA4cgQUfX9HRXljKRpiLFFJD9FwbcfL+mO7bJmP+Jk5M1yJDN92Bw/6yipfLmef7V/a91sPOXnjjQQlPI7qd6h+IrRdYUp8hGro0WN67qW8r3/XJC6cGqQRdsYZvv3rJcNZiRIhLn8PBNT27YlKVh+v14tEqJcSZ942/FGmjI+g2r5d1WyaZIo+PP10iDz+OERRqKqKffUcPcq3P1bxJ+yjR484JRzDBOXOzp0+8gnVlM29cJDbP4aAIWAIZCsEIHh45vUcs9mqcdYYQ8AQyFEIcC3hw4RSPrxPmhkCyREwgic5Iva3IZDHEIDcISRbXrTs7jzLjDFBLUEYKB4EvBwdXr2e85i/cXr7G/k+PDum2aUJM4bj1syHAARZy5YtT4ADksCzrHIIs38ICj4pGcRAVhkkRXR0dNLuM0qcJFV0Gj/YJ8c66rXkxvnhnSOcG8nHN3n5tP4NoVa5cuWk4vXrQ0CQeL2S+ySt0B8cV3wwLy8Pv712+5+ntfGuZ0OD0IIc69+vn8udI5qcfp8qbYrFHJOQfAUkMkIfPdWHjSqpz/+9KNv37ZULG54tpZSsufjZx2VQ+wulR4uWUrhAfnquod6U8FHVwpl6TIeoZz1etyuUP5881+cGWf/Xdhky5U3p2ayl7ite+ilpFhaZT3PyRLr8Px48xaOUtC5aRNY72QV5TkKlhC4LUVJnxIfT5IZW7dRxHyb5IiMkRK+ZibrPg/v/lqO6P66hWWncpyCLUVJmN5s8mXBq+fVYTdQwZjF6nIuGZ8t30mZ63UiNZ9buOitfPsGFJuMPwrthZcpAfhwnVVj22WeiSjzKJErr1iGaHydR86LFqIImQlVEoTJ5cqgSo8dJE7bJSiOVXbduEbJuXahcckmc3HlnjHz6aZgqlU49vt27J8qIEaLqvzAleH298IVnOxGTrOyf7dsQMAQMAUNAXCglcMAha2YIGAKGQGYgAFn83HPPyS+//GIET2YAmgvrOPXbRC7stHXJEDAEfAjkZXInLx0DOAYzQjj4kz15Cbfc0lfUKDi0cbyb5R0ECD8HUZUa2RdIJEgAetNNN8lAzX3zSt/+clC99jHq2S5aOEbyK4mCk773uGelcXQ1mXTLIAlXAkM0x84WVffc8Mo42aF5cAZdcKEUUMUTSh7lYXS1KnJUhRah259bRonKsHA5Q/9+6LKecsX4Z+X7B5+QUO1vSFHN17Njm8TExslSVeeULVpMzixfTl696Va5/pWxUqN0ebm0cRMpqurEv5XI2bxnt7SqWcsRPnFKEkWoKurQ1m2yYfsO+U3VRa3PPz+QUJ2ybq7f2ZHcoeFz5/quKX37xsodd4jmskqZSCFHjQ6fXodEw4D6FIRVqqRctm7dBEfMlCuXKK+95iN2yDmjERRVCfdvIkPnLsjDD+eTihU1R9PyWKewgSMtXZpwjKLqSh/EZcsmqJI11BEjCLI2bfp3XakNhgvfl9rKVJYnCdCSrddD0pE7HPKEaNPDUNvt62eyov/6s3LlEBf6bckSXw4flFEXXJD2fvyrQltgCBgChoAhEBAEcMBipt4JCLxWqSGQJxHwCGPv+pInQbBOnxQBI3hOCo+tNARyNwJ5VbnjjSohyPxVLN5y+zYE0oIAyiZyoJQvX96Fsvviiy+cI5ZY24RlQIlB2DZC2/krL9JSd0bKkGNm/vz5LmcLuVy+14zj9erVc6HjMlJvZm3LdYfQfwcPHnTnXzAVEoTMW7Nmjebm+MEp2/xVRISvI28S4dPIkUPOKhQ9FStWlMaa9yW9TnaIdPL9QLJ54foIrUb+HfL+ILH3VzalhDMOZvICUc/5SjgQNnGj5nkiD0/Tpk3/lc9o714NY6bHI3VnBcFDOMAXXnhB84vcLh+tXOYIFQgecu2EqorqcOwxDc22Xd4acIeEKwETUqCQKmfCpWJEPpl553Dp9MSjUr9SZencsJEU1/hT+cIj5BtNOnLeWWdJaOFCGvpQHeLKGIQr81NDz79QJYzAN1TVOwk7tytxc0TGfz5PzlRVXas6Z0mIbpNf65j83wdlt+63sHrXURLtOLBPbmzVVqIKqrJIiaO3Fnwl59c8Szbu2in3Tn9XrunTRyr5qa9SGptALuOYy87KyQ4dEuSjj0RzzYSrEjhOc8wcf63Yt+848UDOmk6dEjXkGfljfGV69yZfz/EyHo7Dh8erQiVc84VFyE8/xSupkyDffhuu45uo19tYR4h4Zfnu1i3R5er54w89FmpEqvIuXhU9IXquhCmpjXPNRyR16RKvhEqoDB4cqeXJswPRGHxjMnelSkpmbgmVfv3CNFRkokyf7mvL3r3/xiN5C1HsQPBgXbvGCRyomSFgCBgChkD2QoDICZZ7J3uNibXGEMgNCFiYttwwioHrQ9YF4g9cn6xmQ8AQSAMCRu4YuZOGw8SKpIIARMHs2bNdeEMS3hNuihBtnrMdkoWwSqgoli5dmkotgVn8k+YnQXWFY3/s2LGOCNi9e3dgdpaOWv/44w+dwb5OZ9f/5Utsn4460rsJaiYIlwULFpxQBQTKG2+8oeGePlOHaT5HxDC+hLSDrMiIcWyA/7fffptUDeQgRBNEj79C7vDhwy5vEwUJzUVbMdrHNtOmTXN/Q1BBDkH0zJs3z4VgpL18Fi1a5IhH1rOP9KgP3E4y+M+tt97qclLd9/678ua3C5LakaB9uXXS6zKyR2/NeaOOah2TRFXcxKtaJkRDr0Xlyy/DulwiM5ctVhVOrFIAITL66utk0jdfyr1T35W358yRD76aL7t37HQtjFAvfrh+IJASlTyMUZwmzv9CmlWvLj1bnith5SqIhGuYNbBU/3mp6GjJp+csVkAJJXIBRaH8UZKpUdVoGfHx+/qZLpt375LLLrvMlcuKf7I7uQMmV16ZKNdcE6vHaoiqbSL1vElwShrWrVrFvz676CIdm8QQmTMHNZJoDqwYJV5SJjOUr9Tj/JiSNfGydm2YEkgRSoIkyIsv/pvcofZKlUKUEIrVsSKcHqqicEfu1KwZLxMnHpUWLXxtuPHGeHW2xcnevaHy3XcRLqScb01w/4V0Gj06VknjeIfHZ5+FyxVXaFIftVWrTv1advnliY64onyPHmlT/lDWzBAwBAwBQyA4CDDJy8wQMAQMgUAgYKrAQKCae+r0TQHLPf2xnhgChkAaEDhy5EiS4zANxXNdEVPu5LohDXqHICkWL17slBdn6LR0SJ4VK1Y4x/odGqsIZchXX32lCbFXyyOPPBLU9nXq1MntD4UIJAoKGYiEcePGORKqe/furn1BbZTfzlDI/KpqDGYgQaYE0zj3q6vjn2/PIEDmzp2roY4ukKlTpzq1E+MHbqh3opUQyIhRT4UKFdR56+fx1grpO0Q7KhyPREJ58+abb8oNN9wg81WF1aZNG0fUoU7hOIOgglxkXMHxzDPPdOPKw75/sk1IpU2bNmXpdZ6QgJ988omqFPrJU0qezf5xhXRpcLY0VGXOJlXIlC1S1MGaePSIMljKvcRq0njNe4OVjSom2/btk0NK1pAXp1yx4vLazQNksyqrXpn/udx7aXcN3+ZTPRzWMoc1v09YmI8w2KRlpixZKIMv7iqhZc+Q+G1/SoiOcWxcvCOZIjTXjuQvoPHCwqSIEjtf/LJKbmjbXkKLREmDOvVlQnR1SYyPk3umviMTJkxQlURX16Zg/pMdyZ3ffvOREP445MsXosRLgowZc0wJyBA9jll7IunQr58mYfrHdu2KVbJGlPw+Tu6MGpUoo0YdL0PRdu1Er6/kB4QYkn/q9Wr59zepxSZOZL8xLpQbIdr4+CuEqlcPUaVRvOzfH6/KwUQ9v0P02uztN0SPU9GP97dvH5Mn6zEpfI7b9u1eGV8f9uzx/vaVGTiQvEQnLpsxg7YdX6aXGr3exCnxG6vqTtRnIi+9dHz9G2+cWP743uFDlfLUXZcqRc4h/zX22xAwBAwBQyA7IWCO2Ow0GtaWnIAA78xMYiNawZ133plqk6dPny7vvvuui5Dx+uuvp1rumWeeScqHRaEPP/zQvbN7G/Tu3Vs2b97s/uR975133vFWZfvvGTNmnPDul+0bbA0MCgKnnioWlGbYTgwBQyBYCEDuMDM8r5qRO3l15DO33zjmcaDXr19fLrroIkecoFioWbOmbNiwQQprWClCM5CLZMiQIZm78zTUtlYTPUBWDBo0yJV+9dVXHWGR1eQOBAVh6yA9ICqySl3iDyGkCmO5S/O/EAaN70AbhA3HStu2bWX48OFJu4MIglAYPXq0I4VQhaVkEDhgx4ffyY36yYEDueIfhi55uUD/DckDSUK4uOpNz5b/+3a+3DjpFQ2Ntl8KKsFFzhsUOkeVgPxaw5nsPXBQ/lT8n579sWzbv1d+3LhJjmkunXjtI2X+VoUT/S1cIL+EKUHD77807FuYhmqLUDVQQkK8/P7XNulzbpt/uqa1K0FAPp23F34tI2ZOl480ZOFRJdVCdNviRQpLn3NayyMzp8k+JW1DVHVHnp+QYsWklip/IHGDfYxmR3LnVMcJRE9ahG6lSoWcQO6cqt6oqFOTO8nrUN7zH3In+Rrf3xBMkDvZwUqW9JE7aW3L88+L9OwZ7sRoV13lyzeU1m2tnCFgCBgChkBwEDAFT3Bwtr3kPgQIlc0ENyZInsx4b6PcwoULT1bMTYgjX4338Y/gQDhuJvh563h3zgnm5eHJCW21NgYfAVPwBB9z26MhkGUIGLljYdmy7ODLZTuGwIHcIUdL1apVhdBanuIDNQ+kASGy+K5bt25Qe8+DKoQO7UNFBKHbqFEjl/Nl1qxZLmwWTuSssD///NOpUCprXpOUiIlgtImwZvv373efPXv2yEbNZYP6hXjpLPeuk3yTJwgHf0ZIEvrJSwRKHZRUzB4755xzXJg61ET+OVYI5cbMNWZ9cuzQnmJKNtAGtodY5IPiB7KMPEENGzYMBmzp3gfYtVNJBh/6Qdg52n9Ij8u9Bw9JqaJFVI0TKfuPHZF7pr0rf+7dLb1atpKGFarIkCmTpPGqalKtdBnZrlj8+McmeVrDtflMMdHzbumGtVJM5RoQRkgbjiouRzTHD+HdIlQpFKLyoGj1+l+P51+ty5hRSgaFyQUNGmm4sDDp0qSJlFWM73vnTbmuVWtpqqqo0JIlfMSTtjGj4+9ra9r+zYnkTtp6ZqUyA4FJkyL0ehUqzZvHyX33KXNpZggYAoaAIZAtETAnbLYcFmtUHkWACWdMLIQUYrIjxjsy5q1zf+SQf7w8PDmkudbMICKQNR6eIHbQdmUIGAKGAAiYcseOg8xEgNBdKHeWL18uU6ZMcQ+NECsQPkV1ijjkCjl46tWr50KRZea+T1UX+0XajrMYJUfLli0dGXDttde6nCyn2j5Q63GUk0emVq1aGiKpgCN4MkKcpKedkC0QJahkmKmFHB+SpXz58m4MUdDQNkKg3XLLLW4Z2/Dwn16D0ICMIT8T5FK1atVcbh++t2zZIo8++mhS1ZCEnTt3lpIlSwrkkxdKDuwgewglsHXrVjemhOCDhOrWrVvS9tn9B+PN8Uluo+WbNkopDYu29+8QKVa4kHRt3EQ/Z7vQaYlhSstoyLYpA+6UJevXyjpVVrWqWUuGdOkqhXR84jTc2oEjh1W9s08e/+QDuaxxUylYQAke9XnXPKO8PPThNOnV/FwpFZ8g+ZU8Yr9gGKsvd2/2v00a3j9MFj0wUiqVKS2RUYWluYYLbKbEDhZSoKAkxsTJ6j82u3OXcygYZuROMFDO2fuYPDnWhXPTw9XMEDAEDAFDwBAwBAwBQyANCDRo0MCFUvdIHTbxfnvr0lCNFTEEsj0CRvBk+yGyBhoCmYOANys9c2rLWbUYuZOzxisntBZnLKoYPp4VL17c+5kleTu8nZOXhU9KllrIr5TKZvYynOyQO55lhDTx6jjdb5z1EDp8PENxhbXVcGmeQUBEK/mTGQZp06pVq6SqmqhiBOOFgo+/oebxFD1eu1hPu5mt5T8j9LzzzvPfNEf9Hjx4sAwbOkxql7tbqpQqLYePHpMihQtKAcUqXPOzhBYpSZIiKVkmTjpXqqipVWIk/tBBJcriZJ+qqg4ePiKHdNkDM6ZKuGLzwOU9neJG/5Fq5c6QSsVLyuwfVspFDTk/fTlLIHg8K6+KnelLF8vNbdtLVKHCEqZET4K2wWWvDwuV9armmrJoodw25C5vk4B+G7kTUHhzTeV+l89c0yfriCFgCBgCuQkB1OCY//Nabuqf9cUQyIkIcD4SRYKJdevXr3eT7bxwbbyjkUc3JxrXG/8crDmxD9bmzEXACJ7MxdNqMwSyLQLMIs+LZuROXhx167MhYAhkBAFUTGm19Chc7rrrLlm2bJl0ff5JuefibtK1YWPRrEKyXw653Yb8pXmQlKxBknOclvG1KEbvZUs2rJNRn7zviKHXb7xViaEIOaCkD2HZjqhKbPx1/aTfxJfl9+1bpVuTFi4EW5gSQSrjkSNxsVK3XEV5VFU+DStVkQZKGhUsWEBCC0dJ7IG/ZauG07vj7YlSsFhRGTZsmG+nyf4tpCHh0mpMrjjZ/dfInbQiaeUMAUPAEDAEDAFDwBAwBAyB00cAImfmzJlJyh3IHib0+U8+PP1as2YLCKvff/89a3Zue83WCBjBk62HxxpnCGQOAqdyMGXOXsSFhMJZRQieQFta+pRecoeZ3jgfyXVhZggYAoZAbkGAa3OLFi1OeY3mOh5oe+edd9yss2effVYe+2SG1K1QSS5t1FTKFy0uRQoVlIIRvtBqsfFxsl9JEvLtLN7wu3y04ntJ0Gt029p15ckrrpH82qc9Bw4mNZd1O5Wo6dGkpTylJNCSX36Q9fv+1j7Tp0SpWLCgXFu7poTVqC49xj0tt3W4UFpUqyER2ucNO3fIE7M+kHAlfD766CMXbjGpYr8fp4MPZVMjeFjnKbb8qrefhoAhYAgEFAF/RWNAd2SV53gEgh1KN8cDZh0wBAyBbIkA4csheOZrHh7vugbp4/3Olo22RhkCp4lA4N/gT7NBVtwQMAQyHwGSrAfScFCdjsMrkG3x6k4vucP2XigpkvGZGQKGgCGQWxDg2padrtX33XefJou/TyZMmCDvv/++fLD+V5djiFxF/g7IfBqujVl2BzU8WwnNqVOpcGFZsWGtDH/vbWkSXV0VPPnkaGyM/PTHJvnyl1VSWRU2jcqUkpldL/QNHTIg7TsWql9g0KJsWalXorhM+f47efnLeRKvuZYi8+d3OZDeeOONVMkdV0km/GPkTiaAaFUYAoZAuhGI0TCXZobAyRAglG52emY4WVttXfoRQAmwZs2a9FdgW2YJAhdffHGW7Den7tQLbf31118ndcFblrTAfhgCORwBI3hy+ABa8w2BUyGQ2szhU22XlvUZIVHSUn96y2RGu2xWdXrRt+0MAUMgOyDAtR+SJBiKyoz298YbbxQ+zrTNHhnj1euRPf/3f/8ngwYMkKdbnydnlSohhzS82n1ffyuXVKsilfMXlDZVK8jwGlUcSR+pyh7InFD9JzIsQoqoKqdAvvwu6FusYvPH7t1yYbWq0lXzMR2OOSbblFT6SRVEIx97zNttwL6N3AkYtFaxIWAIGAKGgCFgCJwGAs8999xplLai2QWBmjVrWq6n0xgM8tOeccYZsn37dpk9e7bbElXP0qVLT6MWK2oIZG8EjODJ3uNjrTMEMoxArOYkCIRlR9UO/cwMcicQeFmduRMBQgXy8eTdnG84o/m7aNGimic+X5Z1/JjmIiE/yo4dO1xbCqvqwUyEWctgAx7euIHLvn37hNwqrENBwjrGsqCG9EpeNrvjSLu/+eYbKasqldq1a2f35p7Yvn+UNv4LvXG6/vrr5fHHH5dBXy6Qt7pcIGV1bOqVLikVdSwrRRWRMsWKyb5DhGsL0TBuCarK0Rw+iaFyJCFGDu5VJavWHa7rIjRnT3SZMm6Mt+/dK/tV5Tr8m0Xy7v0P++82IL+N3AkIrFapIWAIGAKGgCFgCKQTAXJ6mCIkneAFebNPP/3U8q+kE3MUO0QMYBJcGX0P4Lg3giedYNpm2RIBI3iy5bBYowyBzEMgEOHZjNzJvPGxmnI2AsTxXbJkiUDs8KBIcvqtW7dKtWrV3MNj7969HekY7F4maLgpnOFjx46VTz75RJo3by6NGzcOdjNS3N+2bdukmDri9+zZI8WLF3cESooFA7AQEmf69Ony448/SsOGDZ1qBPKAmMyQwyTcZBxxwn/wwQfSrl07qVChghCjOb1KmL1KIHCcYIxBdHS0I5lWrFghf/31lzCjrG7dum59av8QLnL16tWyYMECGTRokNuOvyHvLrroIilSpIgw5hj9AeONGzdKZGRkalXmyOWM0VdffSXNmjWTS2Z+LB0qVZSemkun77wv5Pk250tdJbbCNaQM5E4+VfAUK1RYc/SES1hIKJyPs6NxsXLoyFHZuH2HqnmU8NO/7134nVTRsTnnnHMCiouROwGF1yo3BAwBQ8AQMAQMgXQigLPbzBDIDgjMmDHDvZslbwtq/9GjRyct5j2oZMmSSX97P1A3fffdd96fSd8ewcMCC8+WBIv9yEUI6BuvmSFgCBgCaUfAyJ20Y2Ulcz8CxCcfOnSodOvWzak/Klas6BzsderUcYqQQIZIPBm6kydPdrO7aB/kE6oUSII///zzZJsFZd3+/fvlp59+ciqZYBMQYNCgQQOX9+X55593JBwKrO+//17at2/vlDpg1rFjR0eiQCRAyrAsvQbRgiLo8OHDsmrVKlcNLyQQPG3atJExY8Y4pRArNm/eLLs1dJj3+++//3a/2T9EEKQO6pwffvjBEU4cZ4w1eH744YfuQ2xp4qlDVkFkUT43GQQZJOqVV14pKw4dllu/+MoRNcO+XSyXfPSJbPx7v1PyVNQXvoKar4fcOkc0P89RVW6BRVT+AlJWicUqZ5SWmIR4uW2+4hUTKx9//LE7dwOFlZE7gULW6jUEDAFDwBAwBAwBQ8AQyE0I8Mye/ONNZvPvZ/Iy/J1SObbxJ3X8f/vXZ78NgZyMgCl4cvLoWdsNgVMggOMyM43Z0zipsptZWLbsNiJ5pz0XXHCBI05++eUXR/Ig88YJD7FTokSJDBED6UURxz6huQgPB6mArVu3TubMmSOXX365lC9fPqCO7FO1m5lWqFr4Dvb1hH0yLosXL5ZrrrnGkSQQKoRhQ6FDWD3axDUFJQwEFMsyYhBEKLtmzZrllCfUBekG6UPoNwggwsYxXsSGfumll6RRo0YCuQPp5Bnh9jBeWlhHODn6s379ern11ludIskrSxmOQQit3GiM1dSpU0Xf/GSIEqxTJk6U5SNHy+XPPinXq5oHUueiypWkX/06Uogx1Nw7WJwuB5v9x47K/D+2yTMrVkoJxZAxQAUVKDNyJ1DIWr2GgCGQ1xHgXvfjjz/IVVf1dpMk8joe6e3/1VdfLRMmTEjv5radIWAIGAIZRoCICmmxIUOGCJ9T2VNPPSV8PKuueTeJIOFvXPv4mBkCuQGB7OepzQ2oWh8MgWyCQGarB3BSZjczcie7jUjeaw8PozicIQ6wevXqyc033+weKFHMoLwIpq1Zs8Y5/QlHtnLlSufQJl5z/fr1BcWHl88kmG3y9nXw4EHXNsgJyCcwC2Z7mNW1du1a2bBhg9xxxx1uZhjECaoi1mG0LTMNtQ11Qt7wm5cLjH5DwEAieeQNJE/r1q1dfGhCr6XUFrZD0UN7IYZSui5TX6dOnTKzG9mzLsVi1KhR8uqrr8pd70ySuXffL3HxCXLdK2Pl0xXLZM4ff/owVrwK6AfsDim5FqskD6TeaFVP3XnnnQE9Brk2pDRG2RNQa5UhYAgYAjkHASa0kDeE5x4zQ8AQMAQMAUPAEDAE8jICRvDk5dG3vud6BDKT4IFIyW5m5E52G5G8157t27fLs88+K19++aXrPE4GQm9NmTJFatWq5ZQ0wUalRYsWjjBAIUIoMD4QO5wvxC0mpBxEQrANQuKPP/5wqpNy5co5FUuw20BumhEjRjhyCUVTr169XPg6HP+zZ892xA/LUDxR9ueff5ZKlSplSImFAwoCCcKtatWqcv/998vAgQMdufX555/LhRde6AhCsNi0aZMjwG655RYXdm3Xrl0uJxBEEOHdUJosW7ZMwI/2sQwVWV42jmvGsmO79jKu700uB8/E/gPljAE3uXEmzxF5l2ZoUtUEPQavuuoqN+7kWmLcA23BVqkFuj9WvyFgCBgCWY0A98prr73W3Q9RZVauXFmmTNXnrpq1srppOW7/b7zxRppmwue4jlmDDQFDwBAwBAyBPIaAETx5bMCtu4ZAehHIbjOQaY85ztI7mrZdZiGAk5jE916OlgEDBjjnPY7jYDiPU+oHYb9QgaBSwfFBO/imjeTiyQpyx2snYbAIH+cpVrzlwfqGGJmo4bzYP6SJN26ErsNhBFmCoobQZ0uWLHEqGa9MetvYtWtXgahhDEqXLi1dunRxVV1xxRWO+EGp41mVKlWED+Z985s2NG3a1JFQ/I1BLvorx3xL8+a/TZo0kWNxsTJh/hfSr11HyRcWLrXLV5DffvvNKdfuuusu4WNmCBgChoAhkHMRIL/c4MGDZf78+e4eTt7D8ePHS9u2bZMmSuTc3mVNy7PymTBremx7NQQMAUPAEDAEcicCvoDuubNv1itDwBDIJASyo3rHyJ1MGlyrJkMIQJ74EwD8hjzIKnLH6wzt8NpGezhf+Dsrz2X2T/6frCJ3wMbDhG8PE37ziY6OduQO5bxx9B9blqfXSpUq5ZQ47Mczwq9lJB8SId+8sIBenf7fkHl88oLhoLrnnntkyqKFSd0d3uUyWbRoUdLf9sMQMAQMAUMgZyKAOhp1LRMdUL5C7Lz11lvy66+/unCkTHYwMwQMAUPAEDAEDAFDIC8jYAqevDz61vdcjUBmhmfL1UDloc4x4x8HM6qTQBghqFCPnIp8O3LkiHO8kpuG8FepGYqHxYsXS1RUlMtrQ6ilH3/80SWex3Ht5Seh3E8//SQHDhyQ8847L7XqArKcZPe0D+M3jmYcDeTewSBUAoW328FJ/gFnCAovLFrRokVPUjrvrOLYIf9PsWLF3Png9RyVDWN59OhRdyxxrnBscUwnL+ttk12/aTdOMMLWkBMqL9j1118vH775ZlJXayiZWKt796S/7YchYAhkTwRGjhyZPRuWi1tFyFQv71x6ukkIWpSvgTbCpRJadtq0aULY2WidiEHetBtuuCFpQkag22D1GwKGgCFgCBgChoAhkBMQMIInJ4yStdEQyGIEbGZcFg9AJuyecFS8JBPGgpBRmW3Uj0O5Xbt2J1UVsF8SzZM/pFmzZidtBg52SCmc8TipCZ01adIkF3KJvCZsTxk+mzdvDqoyBDJpxowZLgTUI4884vKmTJgwwZEpfBPTvEKFCi5827Bhw4LuZMfBT+LhqVOnyqeffirnnHOOm/l6UsCDtJKxQ3kCoYKyxSPqgrH7PXv2uGOIPDZnn322cxRx/DATGAKM8C+MG8Tc22+/7TBkpnCbNm3S7UzavXu3zJo1yznTGIcaNWo4Zc13333niMCzzjrLteVk/YewJ7fT3LlzXQ4fSDv+5jzq0aOHI6AYcwyFFBizX4ipvGKooaqULJ3U3XU7tkuL4sWT/rYfhoAhkP0QIM8ZHzNDwB8B7mH/93//58KvQewwGejuu++W2267TQoWLOhf1H4bAoaAIWAIGAKGgCFgCCgCRvDYYWAI5FIEYmNjc2nPrFuniwCzNHHyk1cElY1n5GjBoY1CAYd3v3795MsvvxRCYeAY5qWaPCB16tRx5E3fvn2dIgTSgDwqkC84p0lw3qdPH7fu4MGD8uKLL7o8Izilmzdv7uKkk8+EcEmDBg1y+4CcIY8STn7ahnMdpQsv7rzYUz/7Xrt2rXO4sw9mjOLEXb16tUTrLE4UM2/qjP369evLt99+6+Kyf/TRR07hA8ly++23e13N9G9IiQsvvNDluQFf2v7www/LjTfe6BQ8/F29enXnuAKTYBrteffdd4X9Qs5CYDAW/A3BgaojKw3CbuvWrY7kCXbIOLBo3769oPaA7MRZFBMT48K8PProozJ9+nR3PnTu3Flef/11R+wwlqdSpZ0MT8gYxgESlPMNgmfHjh3u2OCc6t+/vyOTPKIS1RXH/8aNG13OHvIWsX+Oc45rxhfFWnElLzjGIKIgVpnpjEGeURc5hRhzSCX+zu3GtWPKZ59JfkkUBVsuu7yzhCnZZWYIGALZHwFykzEpwSz7IzBw4EDXSJ4neaZA9Ypxn+I6zD0LNWx67js7d+50efIee+wxVy95626++WY3sYFnVTNDwBAwBAwBQ8AQMAQMgZQRMIInZVxsqSFgCPghkBHnpl819jOLEICw+eqrrxzZQtguZvrz4o2TfenSpUKotF9++UXmzZvnCCBe2nFC42SGOGndurVAGPISjzoA5QAqB8gZnNYrV66UDh06OKUNzufatWs7pQEhNChPWDVIG/YJqUTicxLMs46/P/jgA+dwX79+vSOJ9u/f7xLc33LLLfLFF184BzjtXrhwoYu1DlmEwxxC5bnnnnP7RcEDiYFTHFKIcG6BNNrur2zDmQEOEFWoKmjv119/7TAKdh4U+o8yhvMWLDEIvG+++cYRb4xRehwvmYUnGKEugaDIrBw3aW0b+2T/CxYskP/85z9uDDmOcUqBiRdi0LvmMc4ZdSo1bNjQ1fHxxx9L48aNXVM5VjlGCOnHeQjJxG/OOUhL8uuwb/5Obpy/EDccczjSOLcaNGjgPl5ZyrA952dessIljyt2iuSljltfDQFDwBAIIgI8hz300EOp7pF711VXXSU9e/aUmjVrnpLwQRWNQhuVDvcv7m9XX321PPXUUxm+B6faSFthCBgChoAhYAgYAoZALkLApjbmosG0rhgChkBgENi7d6+b2Q8ZgmM2JxkOfogGlDOe0xhnMs5sXqIJP0Wf2qqagZmYzJZEhcI6iB62R3GB+ocXblQ3kDIoc6pVq+Yc45dddplT0+BQhliADMJBXq5cObcf8tGgKoAgon7UQZBKtAEnQN26dd06nNQ4vSmDegjcccZ7CeoJT8V2VatWdY5r9oHznbai9sFJj5Pgk08+kUsvvTSow4STHoUFiibIFNoC6YVCAxIqmEZbUI0wfpBr4LN8+XKnuGLMspLc4XiChOMY4LiCbAqmsT/CAXGsc5zzN8cRx5rXFo7zzDRIUkg+lECMB6QLY+ARMBA7HtFFGEIUc1xrUIlxviU3tqXNtJd6U2ovdZOPitByWTneydtufxsChoAhYAjkfAQ81Q73Lia7cO/im3sT9xwm2fDsw32IZ08m/HDf9e6zHgI8L6JM5blv6NCh7r543XXXOQXqSy+9ZOSOB5R9GwKGgCFgCBgChoAhcAoEjOA5BUC22hAwBAwBwkgR7gjywHPE5gRUIG4+05BFvEBDijDbH3LGc2bzok2IJ8JF8UJO33BA85KOYxlHPIaKhtAplEE5EK3h0VhPuDTUDyznZZ+QaSNGjHCkDeVQ1WC8uOPkxtmMUx91kGe0B0KCMszyJMwUJA/kEO2h7dSF09urk3GAcEIF8v777zunAX3zHOZeiDdvH4H4xklPmKzvv//eqY1QQBEWDWzpJ/lVCPc1ZcoUGTVqVCCakGqdLVu2dCoqsGZ8UBQxZiirHnzwQTdWqW4cwBUcbxBPOHs4HlHTBNvY/wMPPCDjxo0TiEmOH5RrOKX4jboMnH799VenjEGVlVEVDEQbhA1jAdE5YMAAtz+OYchIElWzfwz1FYQo4QU53gmDiNEGSDrWcy6hoMOB9uGHH1pYI4eQ/WMIGAKGgCEQLAS4B5FPkfsQ9ynvw/MGz2Aos1HwQPxw7+dZiPCwl1xySVKoYJ6ZeI70Qr5RnudGcu8wScbMEDAEDAFDwBAwBAwBQyDtCFiItrRjZSUNAUMgjyLgzVD0ZiaiQpg4caLLe4HqhJdX8svgsCVPzHvvveeUJ506dXLJ1Zm12Lt3byE/DM53XowxnLaEn2BbZjeiOoGkyCyjvSRg94yk8nw8I/QaxgzL5AbB0rFjx+SLXb/GjBnzr+XJF/hv26JFC+GDw5ocP4Ts8Ax1EQQI+8OSkyGQJJ7NnDnT/URthBHyyjP6BfFzveZWwcHg1eetz+xvjonzzz/ffVDI8GGcOQYgyMjfAvGVmeOZ1j6gCCHvCuQZ7aIdHKO0xQsFlta6MrMc7QAflCxZgQt9QbHGMcX+Of8YR88gVmgbDikURhAqWEbbSm4Jb+YyBM8FF1zg6uU84Jhln56hUOODde3a1Vvs2nnuuefK//73v6RlqLE4x2mrmSFgCBgChoAhECwEmBzE825KhqqUHIV8Xn75ZTeJh/vu448/7iYdMZkHdTYTLjAmOTz2+GNSpXKVDN9vU2qPLTMEDAFDwBAwBAwBQyAvIGAET14YZeujIWAIZDoCKFhwDuO0RrUxa9YsN+Nw+PDhLqQEJA/ObBKgk9SdWY6QGeT+eOaZZ9y2OJhx9kISMfsRZ3xGnckZ7ShtoD+03TOW4YjmhRzjN6omiIy0GqGkUEqwHQoODNIB/FDvpGYoeFAHpRSGKvk2qCQgMAj5BskRaPPGyvv2z5fiTxwEuh0p1e/t3/v2ygQDF29fKX37Y5TS+kAv43jzMPFUM94+IX8888bU+zuj3xA7yY0ZyhmZpUzowpMZ1xfO3dM5T09Wn60zBAwBQ8AQMAROBwGe+ZiMMGzYMPesd+eddzq1N4pvVLyoWBs1apR0Xz6duq2sIWAIGAKGgCFgCBgChsBxBDJvqvjxOu2XIWAIGAK5DgEcpRikBLPocRTzWb16tQubVKlSJRdagtAUEBaEDIPcYfY+jlbUAChPeKkltBeG45XwZCtWrAi44sTt8CT/QIwQboPwT4TH8DeIExRLGCqcRx55xOXI8S9zst/0H+UOBBHKHpRMGPgNHjw4KTxV8jrAev78+ScodSjj5ZdJXh58CY/GWJgdR4Axg2DLjsYYmwUGgc8//1x+//33wFRutRoChoAhYAgYAmlAgDC/PJvdeuut7lmkX79+Lmchk4n69+8vGzZuSEMtVsQQMAQMAUPAEDAEDAFD4GQIGMFzMnRsnSFgCBgCigBOaGb7L1myxKlvyG2CggfSgpw15F8hzFmHDh1c7hXUOvzu1auXKwfBgeqFsFkXXXSRI4JYxsstOVwgV8j9wbJAGOQU+T/IbUPb2R9Of/pFu1iGYoHZlHx7ahmWU45l5PNBSQMOixcvdgQW7aWMZ9RHnzDKs559sZw8P6hHqIuY6xAOrGcdxjfkl7+h3qHtEEGUJcwV282ePdvlSKHsoUOHksgL6mZcgmHg9vzzzzsCD2IMg9y74447HCY33XSToOYizBbLs8I4/gg7hmIMEjE7GeMNece5E0zjGCPEIESjF/KP/c+ZM8eNJ1hBZhIqkHOYMZw8eXLSMZaetnIOQArj5PKMc3Hq1KkuZxXXlVMZ5wFYeeQr5wLnAW31kl371wEJCnlM/h8zQ8AQMAQMAUMgKxDgXs9EJ+5zfPO8NHbsWDeZqFu3bm6S1BU9r3AhZbOifbZPQ8AQMAQMAUPAEDAEcgsCFqItt4yk9cMQMAQChgDExKBBg05ZP6HBTmVXXnllUhHCNqGGCaTt3LnTKWYgZQjpBPlEXh6c3Pfff7/MnTvXKWuIp05iefJ/tGvXziWFJ4E7pAk5gyBy7r77bpc/ByUOpAVh6VAJEFudvEQ4sVkOmQQpQ7nvvvtO7rvvPkf0QPqQ4N4LXcc+x48f7xzfOPtJuIvTm5AeEF+QXrSPWZ+sYz9Dhw51y3Ga4yhgf4R8Y3zYZ7AMoqlevXpCXhQIgLPOOsu1mbw3tL9p06YSHR3t2rRmzZqgqopwqDAuqMsKFSokXngwSAHCk3mh9oKFVfL90D6OC8YwtRj+ybfJrL8hQ8gHRf4b4v6T8JnjkuNv5MiRLhF04cKF5eqrr3a5sW6++WaX4yZ5OLfTaQ/HJeQpxzThGjHIS3Bo3ry5C0/j1QchA/lDXh7OI5SAHmlJHcuWLXPbURft5Bh85513XG6v9evXu2oYb+rn3OC8hVgO5rnh9cW+DQFDwBAwBAwBD4ERI0YI+XcwFOwTJkxwkyd4liRnJfdh/1Cp3nb2bQgYArkXASagvf/++66DvG8Supz3w8ww3jl51vY3omwwGS+58dxcv379pAmE/ut5V+GdKiPvAv712W9DwBAwBAKFgBE8gULW6jUEDAFDIIsR4KGW8GqQD6iNIJc2b97syAceYnlYhYTAyeypcTZu3OhetF966SXneIYggFgZMGCA4ECGIMIpzgM5DnqP+CLsHI7oyy67TF544QW3LS/wLCNkGqGiqlSp4rahDIoJ9guJRDtQHNxwww2OHMHBvWjRIuncubOLz75t2zZHCrGcvqDCoN2vvvqqc17zUB5sBzZkGW2BkKpatapTyuCM5+XEUy5BmKGggfAJpm3atMnhgaPfe7FBacYY1KlTR84+++yg40X/GSewQf3E8dKnT5+gx92HOOFDPqxrrrnG7X/37t1OtQaZWaxYMed48nIV8TJ3qlw3pxpbzgP/nFaUp14cWRzTkJicU56h9OEFFBKxdevW7hzy2gZBh5oHDMnfQ06jadOmCYoxf4KZcWcffAf73PD6Yd+GgCFgCBgChkBqCPB8yDMUEy64DzKhAmVtep+ZXvpknxw44gunnNo+c/vy4Vf8O99fbu+z9S9nI4A6nc/tt9/u3gV5N0BFz2TDjBpqfey1115z72dE0Ugt9yXPyzx3p2SpLU+prC0zBAwBQyArETCCJyvRt30bAoaAIRBABHCmQ8qgWsChTpg0ZvXzoArJAkHCgy4JcH/++WfnyMbZDeGCA5zZ/8yi4sGb3zipUQ6gmCEUGy/nOMsxHNXbt293YaGuvfZaRybgcB43bpzbfu3atdK4cWP3Et9Wc+XgSMcBzTehpNieunBGExaO0GvM5mQ79s9y2rZu3ToXIo4ybItiBqIl2I5ssIUEIAQbobwiIyNd28D5448/dg4K2tWwYUMZPXq0tGrVKoAjfWLV4EU4FMglCDTaCpaVK1d2pEZWOfwZO2bpQRJCDKb2knVibzL/L/DhWEUZBhaQJ154QH4Ho12ck3wgWslHwHFEWzimWfbmm2/KpZde6hQ8KSFAWY551EcQSMmN9YSNNDMEDAFDwBAwBLIrAty/3njjDafmZSLSPffc48LKcg9Lj/2yKTY9m+WKbc6s8O9ngVzRMetErkegXLlyLk8XHeXdEmU6BA/veER6YILYFVdc4aImQNaQ06tGjRoukgSEMOr4VatWuXy31113XRJelMMIaUx572/eaQmzzaRGtiWktWcLFy50ExuJdpHSczTRKni3Yh1hJr0JYd729m0IGAKGQFYiYARPVqJv+zYEDAFDIIAIMNv/4osvdg/LzZo1c6GpUCOQA4cwXRA2PJiisIlWRQwEC6oa/mb2FOoAFDwNGjRwCgfUHyxDiUO4LwgE1CsYocogNyCLcFKjQKC+p59+2jmsUbagWEDdA+kBQUOun8cff9y1BbUL+8HYHoc32xCCim/6gmwfmbzXdtoKsUI96XUGuB2m4x+UUJ988olrM7POUINApvGCAZlDP3G+Q5T997//Tcce0r8JODVp0sS9oIAf4dq6d+/uyLknn3xShgwZ4nBL/x7StyUvV4TdQ0307LPPpq+SDG4FsQPpyMsiYwNWEC2EQ4OAQuV01VVXOcIH0oflHO8ZOb48opV981L50EMPOYILopX9cRx79XNc8SLKMcNLJOo1zlfIHLaHpCXsIuos2k2oOy/sWwahsc0NAUPAEDAEDIGgI0D4Ue53OF8nTZokgwcPdvfdoDfEdmgIGAJZigDPxRC9TEjjuRc1D6HMeafs27evfPPNN+7Z99NPP3XXiVdeecVNqOM5mHcyJiCmxe699163D0KPE66N/fHexHM/Ex55D+A5/OGHH04ihaj3q6++cu+1Tz31lHzwwQfyxBNPuGf6tOzTyhgChoAhEAwEjOAJBsq2D0PAEDAEsgABnMYeAQMxgxGey7NGjRp5P6V27drut5evhbwdnnkqHf6G5MEgfiCCPOPvli1ben8mvZx79ZDbB/Oc0V7IKm9//vuAtIGYwCCOPPNisxMKDeOBP6uM2WYQVeRBgTDhRYRZZ+Q1gryCUAF/crngvAimQYahgGJWLFgyw4y/+YZQYH1WGPvlZQi1FuqtrDCOHcKZEeaMDwo1xgmiB8KHGXu8IBIyEHKSsWNsKZNeYz8c34ShgfTDecXxQ73shxdNzyg3bNgwpyqiLd5YURZ1ES+bjCUzB3kJ5fzxP6e9euzbEDAEDAFDwBDIKQjwzIeynJBtPCfyjMAkBu6f3AdR1l5//fUycOBA9/yZU/pl7TQEDIFTI8DEJkIS80x8/vnnu8gHPHfzXM6z7sqVK907Ft88rw8fPtyRP0xSJHclz8WQPy+//PKpd6YlnnvuOTdJkUlSPEcvWLDAETy8IxAqErvxxhtdjlhP9cMyQiITjYLJWEw4fPHFF43gARgzQ8AQyDYIZI2HJdt03xpiCBgChoAhYAicPgKEBOBlxDNeRFB68MkOBrHnkXv+7fGIPP9lwfyNGiUrDRKOl7OUrEuXLkmLIRM9QjFpYTp/EIKGsGueeQRltKrm+Pibf+4ByDnPIHcgVP1JVU/x5pVJ/g2ZhHPMv57kZexvQ8AQMAQMAUMgqxFgEoOX7w51eEr24IMPOvUvJA+h3LwJECmVtWWGgCGQcxAgN87YsWOdgp0csN6zMEoannWZyMTzO9eJmjVrOnX7Rx995HK1QvCg3kEhn9bndqIsQBYRCjm1SXhEkyCktL+xL4zw4Rhh48wMAUPAEMhOCBjBk51Gw9piCBgChoAhYAhkMgKEHMDJ770wZXL1GaqOl7WMqGMytPNcvjG5oFAonYoIyuUwWPcMAUPAEDAEsjECTES466675PXXXz9lK8n/SAhhckVON9ds6QAAQABJREFUmDDBOX1T2ujY4b2yf/tvEhoeKSUr1NfnDFUuh4SmVNSWGQKGQDZBAKIH8gVFD1EhCHs9ZcoUlzt2xIgRbtISTUXlM2bMGBeC+sCBA3LllVc6BWBau4ESZ+LEiUJ0CcJY8y6CoRqEYCY0OMvbas5Yf0NliOoHJT7XreXLl/uvtt//IMCktJTyg+Y1gAg5aGYIBBsBI3iCjbjtzxAwBAwBQyDHI7BmzRoXcgvihLwoM2bMcMk+CYFGjpcbbrjBhZAj3wzJQlNS0wQSBF5WOnXq5BwgtIecRrwQZRfjxYiY+4QqCyYBsX//fjcDmASpQ4cOdbmTwGTmzJnyyy+/uBBqhA8kTMSsWbNciEDyV5FjKb2JVHnAp6/MNublFWOmIaFo2CcvjORtOpmBFw6tefPmuZCA5AYiHAUvohxrhK/xXlCpZ9OmTS6WOOECzQwBQ8AQMAQMgeyIAPet0aNHC7k00mpswz37rLPOElQ9ye3Xb/5PfvxsjCTExbhV+QqXlITYY9J1yNeSr2CJ5MXtb0PAEMhGCBCKmNymCxcuTHp3IYcnk9S2bNniWsr7zYoVK1xeWEKQ83xO6La02oABA9xzPe9mkBHkysQINz5+/Hj59ttvXQg2f2U/63mX+v77751yCJX8tddeK02bNmWVWTIEeC/Jy0Y4cjNDICsQMIInK1C3fRoChoAhYAjkaASQ9BPDmZBnxGBm1leLFi1c7iBmfRFDHvKCkAJbt24NKsGD8wNCAeKJcGCE9WImFTGjIaS8fExZNQC0j0SlvJDVrVs3qM3ghYwXNkgR8iT16tVLYmJiZOnSpfLoo4/Ke++9517wiMHNzEFIIOJzZ2QmGsQOoemWLFmS1FfawbHB7ED/l0NmJ0PeELoN8oY8U7xwYuQG+u233xyRs2zZMoGIIjHsm2++6Y4378WXsBL0CZKKuOTkwTKVVBL09sMQMAQMAUMgmyDAZAdm0qfHIIVIku6vTt61aamsnPOYnFG9lVRvdo2SPMdk1RfPam7E3XJo359G8KQHaNvGEAggAuQq5eMZ71KLFy92fz755JNOvZN8ghVkjj+hs2jRIm/zFL/J/+lvvXv3duHVIHf4eOaFXouNjT3huX/jxo2uCLnBvDbxXuC/rVeHfRsChoAhkJUIGMGTlejbvg0BQ8AQMARyJAI43kl0P2nSJJfsc/78+Y7E4eH//fffF5zs27dvdyoKQqQF01ARQeTQBsgUjJACEDuNGjVyREFWOPwhHSA2iGG9bt06N/Mt2DH0IUX4zJ07V2655RanqmF8mGnGi1rRokVdcmfww2hfavG5XYE0/MMxkbwOXgwJn8YMr+nTpycpiSj7008/CcovCCBPdUXbIOtoD2oeQlJA/HAcguV//vMfF5fcaw7j7jm9smKsvXbYtyFgCBgChoAhkBoC3O8852lqZVJbzmQI7uWoYD3bu+1n97PppSMlqmQ19zt/VBn5atJ1UrBIWTl6cKcsfn+IHDu8R6IbdZea59woa5e8JRuWT9PnpQSp33GohEcWkh//96TEKzmUr0Bx2bV5mVRr2lt2blwkh/ZvlepNekutVjfJoul3yrFDu6Vg0Qry19qvpUKtDvqMc0x2bPhWylQ9V87t9byEhReQo4d2ycrZj2k938vhv/+SYmfUlHI12knddrdLaFiEa+O2NV/K6vkvSkJ8jGsX4eTWfPu6foVL3baDpFK9LrJwyiA5emC7I6la9Hha8hcqJd9NHywHdq2T8HyFJOr6kVrXea4++8cQyC0IJCd3MqtfPG+nZqea1BWoNqXWHltuCBgChkBaEUj9ypbWGqycIWAIGAKGgCGQBxGA1EGlg+MdJzrEBURBrVq13G/UKY0bN3bKEC80VzBgwuH/xRdfuDBdhIcj9ADqkTJlyjjFSlY5/FGfoDYhaSmxtD0CIhiY+O/jhx9+cMTKdddd58aNlzyPhAObwoUL+xcPyG+UNSisUO889thjbvYi+46KinLEz9tvvy2XXHJJim2hHIQPJA6zDFN60aQMx56ZIWAIGAKGQOoIEF6VPApcM+vVqycdO3Z0kwBS3yJtayAtXnvttX8Vbt68uUvsnXwFqlYmjJBXxt9QqaDsHD58uP/iXPObMKUZMS/MqVdHmWotlRQJk28mD5BqZ18hpaObSdlq50rPB1ZJWERBJXb2SkS+KNn2+1e67hy3GYQORMpuJXJYn69gcSVSdsmB3RukaNmz9FNbflv4quSPKi2lKjWVnxeMkwpndXQE0J4/f3KkT7kabWTTTx+5eqIbdZONK95X0mi6nNm8j8Qc2S97tv4ghYpXlLJntnak0OqvXnR/V2vSy7UhLCK/RBQoItt++1L2/vWrJMbHSsmKDSUssqD88L+nlOC5WGKPHRD2R59orx60+n+YWxZVUkNNhZlbxzsO7NsQMAQMAUPAEMiLCBzXJObF3lufDQFDwBAwBAyBdCCAggLHAuHPcLajjiEuM2oMHPOE10KFMXbsWHn88cfTsYf0bwKZdO+997o2kdsF1cxVV13lQo0RpgAlTVYYzjNy4BACAdyywlBVkciZWb/Dhg2TTz/9VBYsWOBCmTGe69evTyLoyJ2zc+fOJBVUetsL/pBuu3driBgldtgvYWlQ4/z666/OoYhzESOM3meffSYPPPCAU4BBhmGQOfyGROSbcG2EBUSZdeGFF7oy9o8hYAgYAobA6SFASFWuxRA7XO9RTXKfyqih2iRPBB/uM4Rz5Td5Y1IylJkQ9smNCRrcQ3KrMUkmI8bEFX8rWqaWtLrqJReabcWsR2XeS5fIzCfOlo0rZ7pikDfnXjXOkUDedih5ml1+/DmtRIWGUr52R1emU/9pqvK5wRVt2fM5adzlPm7Isk9JmNqt+rnlLbqPERRDWJ3Wtwp/5y9SRhU/S92yIqWqS+dbP5IaLa6TIqWrSbmabaVQsQqy8QdfmyhUpuo50qbPG648zwNNLhkpnf7zkbS/cYpcctc3qgTKL+dd/YqSU4UlskAJR0JROKpUtNumRffRUrp8Tffb/jEEDAFDILchwHsQIc+nTZvmJjwQSptIDKNGjXKRD7z+Uo6Q1V9//bXLpeS9R3nr7dsQyO0I2FSP3D7C1j9DwBAwBAyBTEcA1cQVV1zhHO1UfvbZZzv1DmGzUKZceeWVbp/XXHONC/uV6Q04RYUHDx50RAZhBsg1QygwkoMyq/hkYQlOUW2GVuO0GDlypCMmcKhlhRGCjXBmONP4VKlSxTWjc+fOLg9Pz549pVSpUo5Eefrpp506i5cFj4BJb5shtNgXdZHXh9BqEIO8eNx5551J1ULc8DfrCDvjjxPH3MMPP+ycgC1bthRC8eEcC3Yeo6TG2g9DwBAwBHIBApAvbdq0cR9I/pkzZ8r111/vcsVNnTrVTZbo06ePC8PK5AkUqFybX3jhBbn88stdLrQPPvjAPQ+Q8wzjXkOdGM8FzZo1c5MH+HvDhg0ukTeEDs8R5557LovdtZ08cORY69+/v1NzuhX//MP9gwkKqFC5V3n1+5fJab/pOyQNitb0WLdu3U7YLDEhXirU7iQV61woB/dslO3rF8mGFdNk6Yf3OjVOqUpnn1D+5H+EaLi2KC3im4ARWaBoisUj8xdxahpWRqoKJ7nt3LhEvpnyHzmmeYD8rYCGjEvJarS4XsmgPkmrvDBu+QoUcyHlVs9/QRVBP2kIuiry2zevqirofClVpVlSefthCBgChkBuQYDJdpMnTxbUrD8uXyHlNPx4tdJlpHihwnIsPk4WzZkrIx55RHppXiUmOBIVYd2a33V9QZ0ckSD7jx6WjpqzafTo0Q4SIloQtYH7DpEOCJfNO5eZIZBbEDCCJ7eMpPXDEDAEDAFDIGgIkKOlfPnySfuDSKlfv37S36hVstIgFFJSyfAgm5WWfLZtsNsC+VanTp0Ud9u2bduk5Siy+GSGQagRts8zLwRcxYoVhY+/+Ydb8/IAsR6CCTLHf7YzTkkzQ8AQMAQMgcxBAJXl0qVL3WQISJj77rvPhVlbtWqVI94hcSCAVqxY4Yh1JgFwbb7ttttcuWefffaUDYGkgSx6RB1SEP19+/aVb775xm1HYnEmIKAiQgmMKtjfSO7NDGbKsD3Kng4dOvgXyXG/CUtKwvPnn3/+tNveq1evf+W3+27aHZrHJk7VLi9L4RLR7lOuRmv5aPQ5snPDYg2x5kfwaM4dzxI0JFqg7Nv3btPwaZHSsuezqtRpoaHWDsrCqYNS3V3oSSbA1Dqvn6xZNFF++myMlFSlUczRvzVv0JBU67IVhoAhYAjkJARQrULAMAGC+4ObQHn0mNzR+WK5pG9/nVyh7mt953aG6lXvqcs3rpMLnhwpcz/8SAZ17Cz/GXCnRLqQlYmybf8+GfzmRKeeDVMip6qSQ4X1XfBvnfi4de8eKafvYeM02ka79u2zLHR4Thofa2v2R8AInuw/RtZCQ8AQMAQMAUPAEDAEDAFDwBAwBAyBTEaAGb/jx4+XNWvWuPCm7dXRg5HLDpKF0JqEw8Qu0JnA5MtBfYnK53//+58QKobwbjVq1HBlTvYPhBChY37++WdZuXKlFChQwH2zzXnnnSfeRIOPP/5YFi5ceEJVU6ZMkbvuusspgJhEMm/evBxP8IAHylTCp9K/tBqKWMYtuaHaIU/N1+/0l/K12svRgztly+pZrljx8nWTihcocoZsXfOFlKjYQP7euU7WLnnTrdu2Zr4SJ/Vl958r1XGYIH/+Mjdpmz9Wz5FqTX3q7L9+X6DqnoJu3ZbVc+Ws1re439uVRIo+21dm/87fte7fXW4cQsORG2jrb1+49vy9fY2g4NmwYrpUbdzT7WfXlhWujh0bvpMf5j2h5GGo5vrppLl4Gie1ITJ/UamloeFWff6sqpO+c+He/NcnFbQfhoAhYAjkIASY/MB9kckNmzTaBJaon+FdL5f/XnK56ihVSalq2DCNsuBZgpI0iar+XKShvxtWriKf/fchCS1YQEJKldbrbagkqPqnrJJBU267U+6bNlmmLfpOPh/+oBT7J9dqrE4GeHTmdOnZrbtE6nbccwmjyn3JzBDIqQgYwZNTR87aHRAEuJH8feiIHNDcBiWjCkuByH9mCGTi3hL0BrZt9z4X3qFMsSgJtZtIJqKbOVXxkMGMSvJdmBkChoAhkFsQQBXUrl078VcH5Za+WT8MAUPAEEgPAoRdQ0WCYof8aBikDnl5IHHOPPPMpBAu5NEZNGiQy6s2ePBgWb16tcyaNSuJmEnL/smJ16BBAzczGYKHZ87kRg6f5LkDmNXMsj179rjwobklPCcYQLChOh4zZkyKePjj06VLF6eYKlGihP9i97tEhUaOGNm5cVESOUMItbO7PqKhzFonlT+7y4Py7dTb5Lv3bnfh1YqUqSHHDu2VbZA+5evLXiWJEpXgWbvkHanapJfbbsPK6VK5/sXu919rF7j98MemHz+QM5v3dst3blgkB3b87n7/rd+EZ2vS9VFZ/unDvn3pGsglwr0d+fsv+eXrl6Vqox7y++I35a+137jtdm9ZKXwgeFAjJSdware6SdZ8+7rEHFH1Toe73Db2jyFgCBgCORUB7oFMlJg0aZJc1eJc+WyQqhKVYxn9yYcybt5sGazqnQIami20WAlJ2LNXEg4fcIQP1+hYVbLOW7VSZg+7X8KIvBARLol6/9YLqIQULCjh5cpLvKpzR/W8SqZ+t1CWrlsv7erVlQi9f+QrHCWjBtwm/728h4ycMU0aN2okt91+u7sP5VQsrd2GgBE8dgwYAv8gcCQmVtZv3SGfLV8tOw8elqvbtZA6lTKW/DMlcGNi4+St/y2U3QcOS/fzzpZalctLicK+WWAplbdlWYMAMdPJ0WFmCBgChkBuQYBQgjYzLbeMpvXDEDAEMhOBIUOGSKtWrRypg3IHAoUkzuSuI2wMz4SQEPv373e55GrWrCkXX3yxy8njxfc/VXv27t0rhHxDrUJuAfL5eM+amzZtcmFp+BuVz8CBA+WTTz5JqpKZzYRZ7dGjh1DPrl27ktbl9B+QPGBB7rlnnnlG5s+f74gsr1/kNGrevLncdNNNrkxq97Gml450mxBy7eDeze5+V7h4JZ3NfeKEvYp1LpIeD/wk+3eslUJFy0u+QiW9XblvVDL+VnnkpqQ/r/L7nbRQf/gvv2zYYv9VqibqIH/vWCMq55EiJauq8/HEnA9tr3/nhPIn+yM+LtYRP+Vrd5QSGqbNzBAwBAyBnIzAsGHD5K1Jb8rcYfdK00ZNJFRDqCXs3iVP9LpW1qm6c/bK5dK95bmS8Odm102n5lF9Dzob1j/du69E6Dbx+/ZIyLFYN0nAzZ/eoyWKFJVQnTARf+igFFXC50hsjByLiZHwA3+rSDNBwnRdVLXq8sR/Bkmf89rIZc884UKhvvvuuzkZUmt7HkbACJ48PPjWdR8CcXpx/1MVNV+t/FVmLvheVq1eIw3r15b41oFJWMkshX37D8q0T+fLQiWTLm/fQjo3qy+1KpaT/DrrwCzrEeDFkZivZoaAIWAIGAKGgCFgCBgCuR8BFCH9+vVzob+eeuopefnll52KB6KHXABbtmwRQoO1bt06SdEDwfPAAw+4vDlpQQhlTteuXV15QrqRF456yemHsnLo0KFCLp7u3bv/K4/e7Tqz+NFHH5UJEyY4gomcPLnNmjZtKjjWINTA5cCBA06tRL66sLCwNHc3NCxCipSqftLyYeEFnFrnpIUycWWRMjUzVNuuLctU9bNdlUn/k7iYw/L/7J0HfFTF98VPeu8hhNB770VBQJAiSleUIk0URQFBREX0h4KiAgKiIkVFwIIoghRR6aJSpCu99xbSCemb/72Dm39ASogJbDbn+ll29+1782a+L+5L5sy5t2CpBv+pPR5MAiRAAneagKY3/fDDD/HNgMGoXUa+I0WA0UUOjqHivDl5HG936opP1qxEx7p3qSkHKXJvuHgpERZx76jQU1Jq2bnJ/JmDCN9IdTDOy5+370CYnz+KyWf+Mu9mSdN9gUJy/7VY0pAkrh8vizhnReRJk3sMPNyNO6hyzdpY/b830en9sWZxxzT5HcCc9E5D4vlJ4BYIcDb5FmBxV/sioEJLlKRjW7frIH7Z+BfWbNyBmKiY2zZIvdmcPH4Gn879CRt3HkS7RnXQqFpZuRkFwVnyhjJIgARIgARIgARIgARIgARyh8DkyZOvaFhdPNZQ94y6bNzc3DBp0iTrZiOyWN+EhITg5MmT1rfXfNaUv5lDhRlru5m3a/pMDS0urW5LjWeeecY86z/q3tEUNomSRlqFIXsOFXNKlChhz0O8pbElJURjxfSHTUFx64Hblo5C8Wpt4e4dYt3E5xsQSJVJXWdnTn3dABE/IoHbTkBrqRWT1Gv1S5VDmmTTcZb5MY10+X9VRRk/D0/sPHUCxy6Ew9PFFV/8LjXsTh9HsLcvPN3cEST1dEqIkNOyag24qtAj7siW1ath+soVOBMTjfritPXT9uQxsOWD+H33HtQuUcpscJVzODk5wk0EJSQkwkEEoKLlyuO7gUNQZ8QwDBw4EJUqVcpY0CFNMEjA5gnwLmfzl4gdzA0CKalp2LD3MJb9+RdWb94pQstpUfT/nQM7N859dZuJCUnYvGUnDh45iXU1K+H+etXQqGo5BPt5mxvb1fvzPQmQAAmQAAmQAAmQAAmQQO4SUHEnN+JG7VrFneud197FneuNOyvba5fLneuVlXPn7j4FUXnKVsRFnc84jbd/MPyDCme854sbE1A3mDrxKPLcmBM/JYHbRUBF1w9k8cT7XXtLNp0IyWTjAhVdgvx84RwVKd1wgIfcg4e1bo/nZs8QUccb5UILi6unm2S7dDLzZOr22SHpTZf/tQMP1KoJ59BCcBAX0BNNm+HJT6agWtFipk3NznJv+UpYvGUz3vtpMRqULS/ikA8Ki6jj5eoOT3c3+Is7yEW+I4oWKyk1fYajWtWqKBRSEO++Nw6dO3fO8/VLrd+Bt+v68jx3hgAFnjvDnWe9gwT2njyLJeu3Y6WIO/v2H0Wq1MS50yFmIkRJmrhfVm/E3/uPYKMIPQ82qIl65UvCw/XKvNF3uq88PwmQAAmQAAmQAAmQAAmQAAnYCoFn2/jbSldyqR86voq51Hb+aNY6wUmRJ39cb47SdgloCk6tqabCy9s/LjBOmhBx5ajYo6JNaFCgyY7m6+WJCmGFUblwUVQtXAxta9eBj6+3pGNLyxhciwB/k9bT0VrbzNkFzuLMiUtMQPsPxqGOOHZ8pc7bvjOnse/sKZPuc/W+neb4NFngbZGJuGk9nzLp3rT6touPL6oVKw5fTy8s6D8Y4yZPwWxxz/6wcCG0RnNeDRXUrN+BeXUM7PfNCVDguTkj7mEnBC7ExeP7tZuwXNKx7RT3zqWLl2xuZBZZOaBp274/F4Gt0sfGtSuhU5O7UC4sBE5M22Zz14sdIgESIAESIAESIAESIAESuDEBTWWn9YwYJHAnCVgnOCny3MmrwHPnZwKjR4/GiBEj0LhcBZz56BO4/lNfLVnSsz0w9k183meAEXmsddc0Vdup6Ei88EAb+IrwY0mUOTxZHC3akITU3ZH/HB3ljdayFsdP+unTiJeUa9GX4rFgwFDZNx1ac9tB9nETl5CfiEbqlE0TkSgiNk5q8iSj29QP0POeJmhXoxYKizjk7OOHsV0eQ7g4/z6XujyTv5qNBx54AGvXrs3Tl44iT56+fFnqPAt9ZAkTd7IHAu8u2Yyp36/An5KSzRbFncyMk5OSsW/fEcz5cS1GfLNWblDJmT/maxIgARIgARIgARIgARIgARKwaQL333+/6V+nTp3QpEkT/PnnnzbdX3bO/gmoyKMTnQwSIIHbS+C1117DyDfewPfPvYAfhrwMN19fOGpaNUnN5iaPbwYMwbtLF1xR9ybi4kWEiKvGU2rPaZ0cUWowb8N6HDx9FuHRMeLeEfHG1dUoPhZZSBAt26avXolSBULg4+mBIgWDUbxQCIqHFUKhkiXgVbQIXPz9JeWaM8KCA1E4OBjfPDMYC7duxAX5boiMuwjIOR+9qwHixQVkEaHo2e69UNrRBa+//vrtBZYLZ7OKPLnQNJu0AQIUeGzgIrALt4fA7gvpcAquCM/gosYOenvOmr2zqF3VzTcYbiEVcCLODUl3qD5Q9nrPo0iABEiABEiABEiABEiABPIzAf17ZsGCBVi2bBkaNWqEjRs3onnz5ujQoQO2bNmSn9Fw7HeYAEWeO3wBePp8R+DgwYOYOHEiPuzRB/dVqgLHoGBhIA6cc+cAEWnS5X4R4uuHgS1bGbeNQ3ABw8jVyRnxycmXRdmUFDgGh6B5lWr4bd9eDPl6JqYs/wU79h3EoWPHsUCEn34zp2Hdgb0Y3u5h+HpLSjU3VzgVLgLHsDCki2vn0vlwRJ45i4gYEXotaVKjxwmhgQHoXr8xPv99NVL+Kd+gKd8CvXyA2Fg4ivjUv017TJ8+3S6uG0Ueu7iM1xwEU7RdEws32iMBB2dXuPmHwsXTT0SeIog9tQ/JcVrAzbbC2d0L3gVLwUMKZzp7eMsNTv43vexBta2OsjckQAIkQAIkQAIkQAIkQAIkcB0CjpJiunHjxrjnnnuwfPlyjB07Fr/88ot5ra6eF154AdWqVbvO0dxMArlHgOnaco8tWyaBzATSJU1a586dcVexkugszhh4yRyXzG8liXiSJiUK3FWEMYIPUMnPzwgq6Zp2TcLf2xs7jh9FdPwl+ERHwVlq8gQUCEa3hg3RompVvDz3S5yMjEBaugVOIso8UrcBmlWuIqnYvOAgtXccRDSyyHkSIyMRHhmNb/5cZ0of1CpZEneVLgtnSeumqdsala+ERds3m5o81r7XKVUa6ZLeTR/lihRF+qVEhIeHo0CBy+KTdb+8+GwVeXx8RMRi2A0BCjx2cyk5kKwQ0BuJk5uniCdF4eoTJAr+McSdOYi05ISsHJ6r+zhKQTiPwMLwLlQGrl5+cJDVCrqqgUECJEACJEACJEACJEACJEACeZWA1lNo1aoVWrRogR9//BEDBw7E3LlzzaN3794YMmQIypUrl1eHx37nUQIUefLohWO38xSBi5LybNeuXfiq7wA4yr3AyT8AqadP4XxklGSqSUPp0FCkJyTAUcUdScWWnpiE9PPnzRh9PNzRqc7deG/pIoyXVGmOly7BMSQEbuLmKSRtFQ4IxIDmrUy6NQcReBytC6Ollo4oM+IQOoNkceVsP3QYU39djmFtOqCCOHr+P9KNqJOQnGSEH50vtIa1rXRJ+5Z+MR5VReQ5evSoXQg8OkaKPNYrbT/PTNFmP9eSI7kFAg6ymszZ3Rs+RSqgQOXG8AopIU4Zp1toIQd3lZuIpmMLKncXAsvUNsKTg5OLnOD/by7ZOZuHrFgIkDylOfHIzvl5DAmQAAmQAAmQAAmQAAmQAAlYCehK7ujoaJzTtDwS6vCZOXOmcfEMHjwYJ6WGAoMEbicBpmu7nbR5rvxIYP78+UhNTkHVosXEVSMCTkoSHORe4CTf/z2nfSjunIuwiAiUJqJP2skTSAs/hxSpk5UsIk5Sago61q6L9YcP4FREJNLFQaPHOvj6yPEOuLtMeYxduhAnL4iLR8QidQRFXozDpUuygFuEIouUOjgnQtLQ777A+G49M8Qdi7SRLOeIiU/ACUnbNuHnJahSpDj0HnVJjtNUbfpaI0EcQrFSi0en5y6JwGRPYRV57GlM+XksdPDk56vPsRtRx9U7AIFl60ratmKIObFL0rZF3DYyzm4e4tgpJ4/SYkWV4nAMEiABEiABEiABEiABEiABErAzAjqRNGbMGLz11ltmZN6SemfevHlYv349JkyYgKlTp5rH888/j1deeQW+UoCbYV8EbFVMoZMHxllnXz9t9jmaAwcO5LmBffjhhygg3+f+/6RNs0hNHY0gcew0KlcRzd4Zic+f6A8/T0+opJIqIk20OGbeWPgdjkWGI07dPeLO+XrD7xje/iFYkkQgcveQfaPRukYthPr4od3Ed5EkxzmLq+fxhk0xuNWD5hzxiYk4cP4cXmndAUH/pCOzSMq1YyLqfP7baizZsQURIi6pkOTj5o71B/diRLtHUFBSuzmJCyjE3w+frFmBg2fPyGcHoG4kPV4XJ9ypaN26NfShERUV9Z+7YRV5cipdW0REBA4fPmz6pZxq1679n/vIBrJGgAJP1jhxLzsnoO4dj6AwcdIEIv7cETi5e8qI/5uD5vrIHKD1gLxCSsK3aEWpCcQ/Xq7Pip+QAAmQAAmQAAmQAAmQAAnkZQLq2rnvvvuwe/duM4xQScmzdetWBAYGokmTJnj55Zfx2muv4f333zeFuLUY94gRIzB8+PC8PGz2PQ8RyK8iT9myZaGiQV4UDvLQj1eOd1WvW14IFQ927NiBciGhl7srZQkckpKNkHMs4jzC/ALw0WNPYMSCb7DpyGGTYk2FmFfbdMSErr3gK46fVBFUJorD5q8TRy+7aiR9W7qZqkuHm6sL7q5QDhveeMfU4XGT9lWAUYEoRQQfFYs04uWcZyOiRLRxgpe7G4oEB2F4h4fwv4ceMedMFwEpTUSemIRL+GnHNvT89CN88eRAc77eDZtAU8WN6doDb0z80CxI0Fpyzs72M52ekyKPLtD45JNPLl9v+XfTpk2oU6dOxnu+yD0C9vMTmXuM2HI+IuDo6g6/4pXhUyoATl65I7w4yEoA9+IVUSAqCJY0i+QGvWz9zEeYOVQSIAESIAESIAESIAESIIE8SEBXMGc1dNJI3Tljx441K5/1uI4dOxqnjp+s3raGrvIdPXo0XnrpJbzxxhvm81GjRuGDDz4wIo/W7MlcG8F6HJ9JICcJ5EeRR1MjUtzJyZ+i3G8rr4g7SkLFFr0PeLm5XZ73EiFFVBZT96bntI8wo88zKCRlBb7sNwiXpA6Ozox5urrBzcVFBBQnI76oSFO1aHGkSE2dC7GxIsRYzL6OkqLNS1w3Hm6yeFoEmPiEJFyQejl/HjmEH//aiuMXLhjBxiLHnRKnS/3SZVC7ZBl0rFUPPlLrR4UeX3UVSX8c5L1rYBA8khPRU9xGLatVx8Pvv4c3OjyCSg5F4C798QgpiHc6d0Pb997FaHGivi73KnuKnBB54uPj8c0331yBZfr06RR4riCSe28o8OQeW7acxwhoXR4nV2c4e8jNxEccPPJFnzvhAGdJSeDmdxGpiclIS0wxNz5zN8udE7JVEiABEiABEiABEiABEiABEvjPBLZt24bly5ejRYsW121r//79GD9+vNnv9OnTZr/SpUvj448/RqNGja6Z3kYFHH9/f+PgGTJkCN5880188cUXRvTRFD+6Kvjxxx+n0HNd6vwgJwjkR5EnLwkGOXGN2cbtI6Df6/qIS0qUmjdp8JQ6OQ4i3KSJ8HPw3FnJmeMATxF/XEVA8S4UKiUURAAyIpAjLCLmQI6Lk3o6O08ex4W4WAz/7msU8guUqTppU9Kv/XXimNTdsaBlleqIkfo4m48eQoOy5dGpXn2UE1HIX8QaXUCQkJiAM5KqbYukDhu16Duo02fYgx3E5WORVHE+WlwH6dInx4IF5RGKQtKv7wcPRc8pH+LTx59GlCxsCBFByKlIMXw96HnUeuVFPPTww6hatertg3kbzvRfRZ5vv/0W+h2aOebMmWN+H8ipFHCZ2+brKwlQ4LmSB9/lQwIOovw7yk3GyU2EHXnkWma2q9jqeV085Cbh4nxZ6ElJMzcVCj1XgeJbEiABEiABEiABEiABEiABmyCghadVaFHxpWnTpkiWegqagmfFihXYsGGDScN24sSJjL7Wq1cPL7zwAlq1agU3XcV9k9DJwGLFikFX/Q4aNMg4exYsWIBnn33WOHpeffVVPPTQQybVzk2a4sckkC0C+VHkyRYoHkQCNyGg4oqXuGQiRSDRejh+Uj/HURY7q/sm1M/fbC+cEigCj0xNX4xDuovUpRbHDZJlEbQ4dy7ExGLJti04GROJl1q3R83yFWTuTubsZNIsXUQZ3eejFb/go2U/S32ejhjxWHd4BgRC7D9mbk0zuaXLPSVA3hcqWw41a9VB75gWWLN9u0nDNr5zL1R3LQ5vDw9ARCBt01Fepzqki5AUgBIFCmDrsaOoW7I00lNSkC73O/8ixdGqWg1MmTLFLFq4CYI89/F/EXk+/fTTjPFWrFgRe/bsMe7dr7/+Gk8//XTGZ9YX6vDSh4Y15Z2+37lzp/m5KVWq1HUXdSTJz5L+rnHmzBmzOER/b8jsDNY2dSwa+nuFpuezhv4ek/ZP+j79GdWHNTL3SY/RYzPH0aNHcUHcYZUqVYKn1I26OjK3bT1+7969CBCnWkEREHMz/n8UuXkWtk0CtkhA/kd1dHESx44rXL3d4ex++8SdDBzyXaF90PO7ernBWXKImlULGTvwBQmQAAmQAAmQAAmQAAmQAAnYBgGdCNHJjbZt26Ju3booIBNg6soZOXIktC6BTriEhYWhf//+WLt2LVavXo327dtnSdzJPEKdVKlSpQp09e+aNWvQsmVL6CRJjx49jLC0bNmyjAmazMfxNQnkBAEVeayTgznRHtsggfxIQCe4GzRogAgReKIvxSMxIgLpcg9xFhFm9tMD0Utq3Ww5dBg7j57A34eOID0hQVw7yUbkCY+Kxsci3oTHx2LOoBdQp35DOIUWgqPUbnPwD4RF2vn019VYsfMv7Hh3Ah7v0Akevn6wREXCcvYM0sWxY9F6PeIUSj15AmkiBMhNA85BIWhWuw6WDBmGp2dPw6mIyMu1fWSld7q4hGRGP2NSv5TUDlp3YJ/53CKiQPqFcJPO7dn7W2PO13PE+HPJLi+rVeS5lcGpmLNu3TpziIo777zzTsbh06ZNy3id+cVbkurORdxb+ti4caP5PUKFkOrVq6NMmTIoWrSocQJnPkYXlaijNzg4GOo+bNy4MapVq2beawpYq4MoQX6W1BWsbdesWTNzE9AUsNbzPvfcc1d81qRJE/OZq6urEY+sH3711VcICQlByZIlze8+6ki69957cVhcYZlDU9RZ2541axYeeOABKI9ChQrh+eefz7xrjr+mwJPjSNlgXiCgjh2Xf4QdF083ybt5h/9XkJuIOohcRORx8XQ1qeL0xsIgARIgARIgARIgARIgARIgAVshoKtTNXQC6O+//4auotUJmYYNG2LAgAFYtWqVmfDQFG3q3sm8aja7Y7j77ruxaNEiLFy40LT5559/GtGoU6dO2Lx5c8YK4Oy2z+NI4FoEKPJciwq3kcCtEVDHZ5qkZjsZGYno2Dikieji6OOLGuLG+UJEnvG/LMb0X1cgRNKpWd0S4dGx2HjooKnB83Kb9iLqFABSkmA5eRJpJ44jTQSb+ev/EHFnBxYOfQWeoaFIiwhHpDo6zl9ApJzHYhFBRtxA50Qo2rBvPw7LZymyOCH97Gmki5soWM6nDp4NB/dL/Z5EM6j0lGRx6aT+I/jApGaz3vNM39RtIvfAsmGF4SHTdZs2bbo1GHlo71sVeT777LOM0elCjAcffNAsANGNmtpV79U3Cq21pzX4rAKN7nvq1CmzmOTs2bMZhw4dOhTvvvuucQbpghMVgfRZ+/vDDz9A64ppeIgT6/777zev9XeVk/KzY42VK1daX5pFKNY3UVKrySpS3XXXXWaxin6mQlT37t0RHh5u3dX83qGLWGrUqGF+F8r4INMLFZJ+/vlns0V/jurUqZPp05x/yRRtOc+ULdowAUcRchzF/unkJg+pt2NroUKTOorU1eOYlIo0taZKETkGCZAACZAACZAACZAACZAACdxpAu5SjFpXZG+XFDfW1cv33HMPtE6OrlDNzdDJGn3MmzfPTLjoxIm6hrp06YJu3bplTA7mZh/sqW11RDFuTEAnG3WltjV90I335qckQAJXE2jTpo1xNOw5fQoVCoUhKvwCAkTwcZLX9f0bY2GF8nBIS//n+zsdUeKK+XDZUszZ+Ac+ebyfiDUXEQip15OxAFqUFUmhNvGXH7H4+WHmdBYRbrYdPIJen32ERJlD69+sFTpLHZ6o+HgM+PIzzOw7AEE+3rLgQBd2a+Wfy9GoQgW0Hv8OmleuCk8pn+CY5gDLmZNmD52Q33joAAZIW86OkqpL/tPlDfpIlD6GSQo3FS7UxWGvYRV5blY/J0XS182ePdtg0Ov02GOPmWuu9+VJkyaZ7eriuZHAoWKZCin9+vWDCjrq2tUafrqIRBd3aIo3Tas2Y8YM054KMJoa1ltS/sXExBhBSdtQIenQoUPQun/q6Jk/f77Z/6effkLfvn0RLz8Tmk7WGrt378a5c+dM+rTMzmBNBatx5MgR0y99rY7lCRMmmJSz6s5RsUnvEf/73/+MuKT7ZA49Vhe/PPLII/jtt9/QoUOHzB/n+Gvbm+HO8SGyQRK4TMDZ3RXO8oV9O+vsZJe9Oowcxc6qQo9FavPI3Sy7TfE4EiABEiABEiABEiABEiABEsgRApqW7eWXX8bx48fNJIemJ1myZAm2bNmCuXPnmgmczPnsc+SkVzWizp2HpcC1Tii9/vrrJo2b5vhnkEBuENAJPHWpMUiABG6dgNbgCRWHzeRVP6NdzdqmFo9DRBQCtD6K1MZxTFfBRo0x6eg/+zMs/3uH1NN5CGO79sTE5UtwNjoGk7r1Rq1SpaRWj9bfAU5EXECVsCIIkLbFSmFcNX7envjtf29dFgHWrMYL38zGgfNn8ePgYSgY6A8HT2+kp6VcTgH3zzA83d1E3KmGH6TOTwfpm967/Lw84SkLGf4QATxJ+qj1d7xE/FHhwkHOZ5E0bZfiLyFWXUCSJszeIysijwowVneLCl5aD0ejV69eGQKPpltVceR6YlHr1q2hNfY0goKC8MwzzxjhRN+rAKOhfVExSWPXrl2mPU0XqynddLGHplXThzWs4qIeYxV41HVjbcO6n6aS1YUiP/74o3WTqfWnb9SZpCKThjqT1MmjobUFv/vuO5NaTsevvwtpzaDMoWNVgUmFoatTwWXeL6deU+DJKZJsx+YJeIUGIDXyElIlp2e62DVtPRxkcYGbt4fUBnJlXR5bv1jsHwmQAAmQAAmQAAmQAAnkIwI6gbNjx3ZxzjxmJkW00LHmwtdJEl2p6+bmlqs0dLJNJ490skUnZ85L2h9G9ghYJ+Oyd7T9H6WuNQYJkED2COh39dtvv20mx+dv+ROP1L0bF0UcSZbJel8RU9zdZL5Lmp74yxIclbo5299+D566zckF7SWl1U/bt+HxGVMwpceTuLdyJRFaJOuNOGrSZE7PIuKOk5c3HETcKRsUbOrjWMLP4ammzdBj6m7UKFoCQZr6LaQg0sV1k34pyaRjW7VrJ6oXLY7iBUPwkqSAe+27ORi5cB7uKlUWHWrXhYfcv95cNA/PNGkhr13hJd8BqVq/J15qCV28hF2SIu5kdKRdu3cyX21Nd3ajyJyeTV07KsRoVK1aFZUrVzZijDpntI6NOnSuFZqKNXMULFgw4601bZv+XqHOHl1UclHqOukCD30ESl2mVq1aGceOOm+si0y0Bk+TJk1MHR91+6iwY03PVqJECfj5+cnvMTvM7xCPPvpoRjo1FYzUAaSxb9++jH5oCjdNP2sNFS+toa6hqwUeHZOKO7crKPDcLtI8zx0n4OLtDneLI9JE4EmOTxSh57Lye8c7do0OOGmNIKkN5CwrBUwqObkpMkiABEiABEiABEiABEjA3ggsXboUBw4csLdh5YvxuLi4GtfOmjVrzMSKrnLVAsO6mlVz6Q8aNCjXOWiNn+bNm+f6eXiC/ElAxZ2bTW7mTzIcNQlknYBO+r///vum3k7FQoVRpUhRI/BciIk1dXZSYcHUFcuwYeQ74p7xgKMUs3fQtGipKXiwRm24ubqg38xPsHb4SITIpH0hcdSFx8Zg/YH9qF+unGS/kfRpsoY7TZw1LlLiQB04LcSZs+Hwgctp2UScSb8Yh/NRMfj015WoXrwEiofKOTw94SIuojF9n0FURARUynUXB0hicrKkiotH/TLl4C+T+EcvnMfBM+dQuXAR7Dh+DK/M+wpPPvWUqf+SdQp5c8+bpahUN6+mNrPGU8JFH9eK6dOnX1fgudrZc71FIl9++aVJAae/O1rrI0VKfSd18eqjZs2aJnWbij4amqZt+fLlJpXaH3/8kSHwNGvWzDgzrQKPpnezupCs6dn0+GPHjumTic8//9z68l/PWi/o6ihcuPDVm3L1PQWeXMXLxm2NgKlt4+wOR7lBpCYkGaHHkiop0GwktEaQi4g6ziLuqLDjIBZRBgmQAAmQAAmQAAmQAAnYMwEKPHnn6pYvX/6KzupK2fvuu89MgnzwwQdmpXZCQoJJ4zZx4kSTN1/z1Lv8k1rnioP5hgRsmADFHRu+OOxaniKg94lvJEVXw0aNMOCLzzChay/UKlHSjMEiyszHK35Byyo1EChuHFFUYTl7xnzmKO4HB7l3NK9UFRXDCmPr0SNoKe4KR3HxjOnSAy9/8wXuPVQZlUUw0sn+cgULoYS4chxlgbS/pxeOSyo3jXR5joq7iHE/LsRD9e5Cg0pV4KgOEXF0OKSlIt3FAYHFigPJSUgXl4bmjHOSPnuJY8TN0wMlPYvgh62b8c6PCxCbcAmno6OMyGAat+N/bibu6NBV9FAnVVZCaxapkKKpXq+OrNY5U9eNOnj098ZFixYZwUbr26ijR0PPMWrUKCMo6vv27dujf//+5udD07qqoKOhC0M09eZ7771n2lLxyRqZBZ4QERutoSnkatSoYX17xbO6fq6O64lUV++XU+8p8OQUSbaTdwjIl72KJ47OjnASu2VKfAJSE5NhScval1JuDFRtq9on14yUbOLYkW0MEiABEiABEiABEiABErBHApUqVTKrLe1xbLY8Jp0ES5bVyTkdmgplxIgR6NmzJ6ZOnYopU6aYQskjR440Ofi10PDgwYNxzz33mFoGOX1+tkcCOUmA4k5O0mRbJACUKVvW1EnRWiu9PpssNW/qokeDxgj29sGBc2fwYLXaBlO6pEGTajfyWlKwnQ839wt15zSrWBW7T51EU0n55ebiigrijvh+8Is4Fh6OoXNmY9rjT8PXUxZzizCj97h4EWtU6LG6PNbs2SWp39zRoFx5OEqNl7TTJ+Eg9X9UYJKdLt+XxL3jIHVT3NJjUa1wMeyTGj6Fi4TBKSAIQySV2wsPtjP73j/2TVMD7q677rLbS5sVcUeFncyuFk2beq2UZJoWTYUXDRVSriXwZAWkpmJVgUjTphUpUsTUwdFaOHq9dYHJiy++aJrZuXNnRnNhYWHQ67RhwwbMmjXL/Dzo/KsuTPH29jbpZNV9PHPmTHNMOXGEValSJeN4fW+tzRMbGwutAWgNdSxrzR9NQ3d1ejbdhwKPlRSfSSCXCag7xtldRB4Xyd+ZnIqk2Mv1eXL5tP9q3kksoa4+7iYdmzp4KOz8CxE3kAAJkAAJkAAJkAAJkAAJ5AECJUqUwLvvvotnn30WWlRZJ3M0dYmuuNWHft6gQQM88MADpmZP5jz7eWB47GI+IEBxJx9cZA7xjhBQl8PRo0eNw3PSpEn4YdumDAFmaKt2SJKJejeZME+X/3YfO4FSoQWNYBMeHY1Jy36UujsWdKnXAIUKBBoRyNHRAR5OzkiRrDxBvj7GdaMDSxRnzr6zpxHq52+OjxdX6Yy1qzCwxYMy3+Yo57QYced0RCSmr14BH0np9mD1mnK+ULjJsSoMjXqoM7pNnYQ6pUrBT5xFTmFFRNyxIE1EhophRTNSfd0RkLl80qyIO9oFrWtjTWEWKuy0Fo+mTb06NJ2aCnsa+nuB1rHxldpItxr6s9OmTRtzmNa/0RRo9evXNwKPOoetUbFiRetL86xp2lTgsTqNtDaQ1Zmjv49oHT/rZ5ndO3pw3759jRtIhUKtIaQp4Dp37oxVq1YZF5cep7/XbN++3dT0yXxiFZJuZzD/0+2kzXPZJAEHEVWc3V3gGewLjwD54pb6N7cjVMxx8/GAZwFf49xx1PPe5i+A2zFOnoMESIAESIAESIAESIAESCB/EShWrJiZxDt48KAReLZJoWydEFFnT48ePRAcHIzdu3dj48aN2LNnD86dO5e/ANn4aHXSSgtS57eguJPfrjjHe7sJaCouneCPFtHmyJEj5vtf027FSOqzaKmhI34aI954SumCbpPfR3+pvXP/+LcxtHU7DGz+IPp8PgXbDx3B6fAI7Dx6HC9++yWeaNw0Q9yxWNIQLenYFm7dhIZldaI/3QhDKvp4qXgk323plxKMsBTs54th7Tqg171N8MbC7/DNut+R/M/3XoEAP3EF9cNzsz7D4+++iQu7d8Fy8iQcPdzhI2nkIqRmj1UUuN0Mc/N8WRV3tA+ffvppRle0ztK1xB3d4f777zdijL6Oj4/Ptnu7Xr16ePLJJ7UZ0446g4PEjaWp1tRBrKHvNS1s5lCBJ3No/R1rtGjRwvrSPF8t8KhYNGzYMPOZ1vrp06ePEafatm1rrr/+PGvtH/0ZvtPBFG13+grw/LZBQIQVBynM5urjedlJ4+UESy7Jn+ICRZqrkwg7fuIekv8FKerYxs8Ae0ECJEACJEACJEACJEACJJCjBHQFq0646IORdwjoauXbvfr4TtPJj+KOph7S1fWMvEVg8uTJeavD1+itpsfSh0ZZSd/2677dqCK1dCJj4xDo442SIQXxRf9BOHbhPN7y8kFBmUCPS0hE2YKh+Gjlz4i5dAklC4SgX9MWuKdCRRFsIOnW0nAhOhYrd+80Kdra1byc9s1V6vgE+/hi+/Hj4r4pgkA5p07DucrkvIa71Oie+FgvNBj5mtT7qYaw4AA4Suq40qHAtD5PI1rOpfV4pJic1OlJwSmp6aOOFXv7jrwVcefChQtYuHCh4af/6MKN64UKP5q+7e233za7TJs2Df369bve7jfcrq7gChUqGFfNSRHcVCjU0Guhoss777yD4sWLX9GG/nxpGrVdu3aZ7ZkFHq3FM3z4cLO9aNGiqFOnzhXH6hvttwo9r7/+uhEl09Iu13HX8+hn6iKyhaDAYwtXgX2wGQIODhZx8yQhxjEB355NkS/8UJTyEtEnB0QYrfBzMTUVc0+dw4HU8+LacYYlxVtuRLfHMWQzkNkREiABEiABEiABEiABEiABEiABErARAvlR3FH0VnFHJ0AZtk9AC8tr6LM9XbOmTZti9rTp6HlPYzO+VJlA95N5OC+pmVNZauHA2wsOHp7wS0vFg0HBaFG3Liwyt+Ykqo6LiAeaui1OavfEJybiVFQkxv+yGA1Kl0NYkJFypGaPM55p0hLDv/8aD1StjmQ5Vuv4SNUGEYWAVHlvSbGgXqkyeFX2ef+xx+Hn7ia1eoLhFheLgpLCTYqtwEFqzcVJmrD1h/ajfZcudiXw3Iq4oxdJXbhauyarMXr0aOgjc6jrxuq8ybxdX2s9P31cHSrkaN2d559/3tT5U5FH072p2OIhzqrrRea6PJn30XpA1jpNmbdf/VoFLH3ExMRg//79ptaQOpW15lPm6Nq1K/RxJ4ICz52gznPaHAEHB1mh5JgMR+eL8kiWL2pHnLoYjymHTqB2kD+aBQci2M0FLtkQetLkppMgN5z1ETH45Ww44lPV6m4R9048HJ2SkZbihfQ0d/lS0fyMtzdHo81dCHaIBEiABEiABEiABEiABEiABEjA7gjcaPItNwcbFxd3w+bzq7hjhaJCweDBg61v+WzDBN5//30j7thwF7PVtVGjRuGnn35CV6l583633igTEorw6BiTds1DnDPO4qBxEEHGQYUWqbuT7igZd5CGS5LWLTkpGaky36ax+ehhjFgwF84y6T6ua084SxkEFX+SU1JRUeq13FexKgZ8OQPvPNwVQT4+/+pr9/qN8MTnU9H/vlYoJ3N/PjJH5xQgIpGcL+WiCEiHDmLEvG9xVib5b+RY+VfDObhBa8noQ89fokSJLLV8s+/AWxV3snTSXN5JhZWwsDDzyOVTXdG8pmJTUcgWgwKPLV4V9uk2ElBhJ1VEnUvySBBh5/KNwdqBJFk5sO58BP6OisF9IcGoE+gLf7FxOmVB6NGVABfl+H0iFP18JhxnxU4qm64IB8cUOLvFwJKWKCsGPCUfqOQETb9SAb7iAL4hARIgARIgARIgARIgARIgARIggTxGQGsV2Frkd3HH1q4H+5M/Cej/hytXrjS1Wjp++B461qqLNtVro0RwCAK8vIxgc3U6NHVdqHgTdSkeB8+fxWdrV2Pj4QOoLCnYPnysD9ydnHFK6vRkjqeaNIPXBjc0f+9N9KjfGHVKlIaHm87BpZsaQEu2bEQZPx+0nDAaXz31HCqFFYbLqdPyORCbeAlzNvyBWet+xWuvvXbH0nJp7R91cCUnJ4uA9d+/U/OiuJP5mvL1/xP47z8N/98WX5FAHiKgwk6aPBLFSSOrAUTkuVHEieK/8NRZbJZVBC0KBqO0lwcCJP/mtaQYFXEupqbhpAg6ayOisDs6Din/rCi49jnSZUFAovQhWWym7vIQW2G6C4Wea8PiVhIgARIgARIgARIgARIgARLIIKD58H/99VecP38+Yxtf3BoBTbejtQjyU1DcyU9Xm2O1dQIhISHYtm0bvvzyS5OCa+H2LbDWOvFx80CpggUR4u1rxJjzF2NxVGrzxCYkmGGZPDiyCHtEu05oVaU63KSmjtXVY0m3ICklBRHi5Jv26wos/3s7Opctg8aejpi5einCJa2bn+zfrlQpjKhWQUSj8ui36lc8Nm0SPMU9VL9UWZyKlnm90yeNoPLaq69i5MiRto4zS/2juJMlTHlmJwo8eeZSsaM5RcDBQYQdSY2mrh0HxyRx7WS95VPxCZh95CQqB/iigaRuKyU5Hn3E9mmNRBFyTiUmYYsUdtsaGYNYKcCW1VD3kIpNml9/NO0AAEAASURBVLbNkuoprh43cfTo/6K30MGsnoz7kQAJkAAJkAAJkAAJkAAJ5EsCuhI6KznnbR2O1k34448/8PTTT+Oo1EVgZJ9AgwYN8pXAQ3En+z8rPJIEcpNA9+7d0UXq21yUlGhHjhzBhx9+iLVr1+KwCPgHRNTRcJK6O4WKFsXjDzyAEydO4KdFi/DV/c0xffsGjF+6UFK4Ocgs2uV5NL3XpYvIU9zXG0Nr1sSAdg/CQ9w9GiPr17u8l9wTNeWXj8zvqaj0afOmiE5KxOQdO/HHgb1mwXa7du0we9Ys+EqKrqvdRKaxPPYPxZ08dsGy0F0KPFmAxF3sg4AKKI7OicYto+nYxCaTrYFZ5Abxt4g3Ry9ekpRt/qjh541CouzHimtnV9xF/BkZjTOXJOWa+jizEeomcnKNg0OaCj0eUp/HhRpPNjjyEBIgARIgARIgARIgARIggSsJ6MSUqxSLvpXiyFe2cOff6QTcrl27TCHjQ4cOmcm2mjJx98orr9jFxNvtJLx3715o/Yv8FBR38tPV5ljzIgFNPebv7w/9Xp8xY8YNh7B582bMnz8fM3btwZgmjeEvKd0+3/4XCotYEyqZd9xEzHEVQUiz7/h4esDX0zMj5Zuj1N52cnKUsj6OSElLRbSISpcke4/W7qlesBje8vFFfHIShq/biM8++wx+0id7CIo79nAV/z0GCjz/ZsItdkqgVJAzDsZcEkU+MUdGGCfunN/OX8Dh+EsoL3k6tcbO/pg4JIrQ89/jcto2Z5dUBLgHyU2JLp7/zpQtkAAJkAAJkAAJkAAJkAAJ5GUC586dQ6tWrbBnzx4zjJIlS2Lp0qXQZ8atE9DUbLoaPr8ExZ38cqU5zvxCQEWgvn374pNPPoGLCDNv3tsILUoWR79fVuCDxg3hKmKRhyzI9vZwFycPkCi1a/5/Lba4ewSUk7h3XOR70F0WPwT6+MBZXofHxCBRUrt9t/8gUgoXgX5X2kNQ3LGHq3jtMVDguTYXbrVDAr1KBuK3SAdJnRaJ8+K0Sb9hXZybA9DVb05ys4gSu+dmUfrTLFLXR24oTnK3SEuz3LyBG+yhbfvIyoKKskKgcVAAfJ2vVe3nBg3wIxIgARIgARIgARIgARIgARKwEwKR8jdc165dTUo2Tc0WFhaGr776CnXq1IGL1EZlkMDNCFDcuRkhfk4CeY+ACtSTP/oI8fHxmDNnDtacOIW2pUpIGjYn9Fi2Es9Wq4ImxYqYtKSJKclGvPEQIUdFHEeH/xe3k+W+kirzeHGycNtZBJ9kcfSskvRv08QZtHjJkrwH5ho9prhzDSh2tIkCjx1dTA7lxgSKu7uhWFgoqknuzY1RMdgSEYk4uQlkJ3RlgKubC9xcnM2NQduQTfIQ5V9EnyRx9ySLtTM7ua3dZHVB+YAA1A/wRy1xBnnLjYdBAiRAAiRAAiRAAiRAAiRAAvmNwNmzZ/Hcc88Zl44KOwHyd9L06dPRokUL6IQ9gwSyQoDiTlYocR8SyJsEXESwUcH//vvvx6effop5W7YgISEBxcV186qkV6sjqTx7V66EpqVKwsvVzaRls8iCb31o6ByepoXTvDkqAl0UkWfOX3swYdsOjBk71rhGzY55+B+KO3n44mWx6xR4sgiKu9kHAf3CrubtjbLijqng7YXfpV7OflkNliw2zayEFl7zEGHH1fWysHN1cTW1djq5OZqcnS4i8KjQo3+I/L8F9PpncRQhp5CvLxoEB6G+ny8KuLmaPKHXP4KfkAAJkAAJkAAJkAAJkEDeJdCtW7e82/l82vP27dujQ4cOuT76U6dO4fXXX8e3335r/lbzk8LWY2Wi7dFHH4WH1FZgkEBWCVDcySop7kcCeZtAz5490bNHD2zbvh3169fH4FZt0LRiFXT6YByeW/MbnNf+jq5ly+D+EsXgLwsEAuThIP+lidCjj1ipt3PmYjwGyX4pUpdnytSpeOqpp/I2FOk9xZ08fwmzNAAKPFnCxJ3sjYCHCDEN/P1QRoSe7eKS+e1CJE5ER8MiBTuvFSrkuImw4+7qctnK6Xjjmjjq5HFydJGVAGLtTHFCYlLKddO2OUhbflIIrr6sLrhL+lRUnEbu0j8GCZAACZAACZAACZAACdgrgd27d5uhlS1b1l6HaHfjOnDgAPbt25er4zp58iQmTJyA2bNm46Kkwda6By+++CL69esnf4+55eq52bh9EqAgaJ/XlaMigWsSkLk7rcszadIkDHluEE59OA2bR41BpGTvue+dkZi5Z5956Iyeqyyy9hDnjpPMyV2SxdmJMh+oNXlUEDl1+LDd1N1RdxLD/gnwKtv/NeYIr0PASb74C4lLJsg1EBVFYPlTxJW1588jWhT7zOEqadjc3V1NwTZ16GQ1VBRSq6fmBHVxdhE3T7IRejKnbdM/UmrJHy1NggNQQlaiMR1bVulyPxIgARIgARIgARIgAXsgoAJP69at7WEodj+G/v37m+wES5cuxaZNm7B3716Eh4fLQrY0yXDgaibDihcvDr2mVapUQcWKFeEt2ROyEpqKTVPrfCS1FKJl4V2BAgXw7LPP4uWXX4aX/K3GIAESIAESIIGsEujevbtZHPD0jGn45IlnEOjljfe69sDg+XOxfPlyHDt2zDwv++UXpEjWnYYNGxp3aqVKlVCsWDFo9h4GCeQlAhR48tLVYl9zhYCrCDElPNxRUMSeGj7eWB0RhU0REXCSgmveIrponR39cpfdshWOcqCri5MIPe4mtdulhGQ4WlJRJjAAbQoGo7S4iPxECMpm89nqEw8iARIgARIgARIgARIgARIggVslsG7dOrz55ptZOsxT/s7p0qULdKKtXr16psbB1QdGSrpsTcP2xhtvGGEnMDAQTzzxBN5++21oWjYGCZAACZAACdwqAV0Y8PHHH6N3r14Y160n/D280Lh8JSTFxsHf3x/ly5dHy5YtMW7cuFttmvuTgE0SoMBjk5eFnboTBDRtWxkvT4RJirRGQQHYfPESjiclITUrBXSy0GG1fWratjARjep5F0I5OZevuHvUScQgARIgARIgARIgARIgARIggbxAQIWXQoUKmRoHlStXRkBAAGJjY6Hp1dTZc+LECcTExBjBZsaMGdCH7vPll1/i3nvvNUJPvKTLWbBgAQYPHmxSselkXMeOHY2DJygoKC9gYB9JgARIgARsmECLFi1gkfm8yDgRdUTgcZWF1fdVrIwdO3YYcceGu86ukcAtE6DAc8vIeIA9E1ATpqZJqyDiS2lPDxxISMCv0XG4kJJicnH+l7G7i8Bzj68f6vp4wkVEndwWdhKk74mJif+lyxnH6h9kDBIgARIgARIgARIgARIggfxNoEyZMli4cOE13ThXk9G0axMnTsQnn3xixB5NxVeyZEk8//zzGDJkiEn3pumsmzdvjtmzZ0PdOwwSIAESIAESyAkCulhAHwkyn2eNsmFhxr1jfc9nErAXAkwqaC9XkuPIUQL6P4abiDCVJa1A30IhaBbgDxVoshNOclhNby88GxaKhn7e0o5jros72eknjyEBEiABEiABEiABEiABEiCBGxEIk8mxrBZsDg0NxZgxY3Du3Dns27fP1OU5cuQInnvuOSPu6OrqAwcOYPGSxRR3bgSdn5EACZAACdwyAb1XjRo5Eu5SE9sa3t4+0FpxDBKwNwIUeOztinI8OUpAJR2t0dPQ1xv9RaCpJs4eZ3mfFalHHTrF3d3Rq2BBtJWUb5qOLSvH5egA2BgJkAAJkAAJkAAJkAAJkAAJ3EECy5Ytg4o5u3btkrqmDqa+qTp3NM1bQflbKWt/Xd3BAfDUJEACJEACeZJA585dUKBYUVnB7YJ0V2fUadZUsvOk58mxsNMkcCMCTNF2Izr8jAQyEfCRP0IeCg5E7aRkLIuKQXhKMpIt/74xqADkJ/vWE1GoughC6thhkAAJkAAJkAAJkAAJkAAJkEB+IpAutQ/atGmDVatWmWE/8cQTeOmll0x9nkcffdSkZYuOjsbXX3+dZVdQfuLHsZIACZAACfw3AoFBgQjs2QX4p7b2fTJfxyABeyRAgcceryrHlKsEiru5iisnGNsuxmNzXDyiU1ORIjcLdex4OzmivKR1qycp2YJcnOnYydUrwcZJgARIgARIgARIgARIgARsmcDu3btlXi0ds2bNgoo66uDR+Pnnn6E1eRYtWoR27dpi7txv4ePjY8tDYd9IgARIgATyKgEKO3n1yrHfWSRAgSeLoLgbCWQmoGnb6vl4o7SkYNsiQs9JcfX4i6BTUxw7KgCp2MMgARIgARIgARIgARIgARIgARKAqXlgFXeUh9ZAWLFiBVq1aiUOn9V49dVX8cEHH2QbVVpMTLaPtZcDnfz87GUoHEc+IzB//nxs3brVCMBVqlRB8+bNERQU9J8pHD16FJ9++um/2qlXr54Iy+3+tZ0bSIAESCCvEqDAk1evHPt9xwmohBMsos79AfxF+o5fDHaABEiABEiABEiABEiABEggTxEIDQ3FZ599hvvuuw/Tp09HnTp10LNnz2yNITUuDvlZ5HGULBIUeLL1o8ODbIDAypUrERwcbL4L/v77bzRq1Ajr16+H338ULQMCAkz9Lx3i4MGD8cILL6Bo0aIICwu7pVH36dPHfDc1adLklo7jziRAAiRwuwhQ4LldpHkeEiABEiABEiABEiABEiABEiABEiCBDAK1a9fG559/biZPdQJWRZ5KlSplfM4XJEAC+YNAhQoVcO+995rH4cOHsWDBAvTu3Ru//vqrpHCcCxWEe/TogZIlS2L48OF488034SS1j9X516FDBxQrVgw//PADChYsiPr16xtoKhBpmxpeXl6oW7cuypcvb95HRUVh6tSpiI2NxeOPP45y5cph8eLFSElJwUMPPYRt27aZc3t4eGDLli1mP39/f9SoUcMcz3+uTSApKenaH3ArCZBArhKgwJOreNk4CZAACZAACZAACZAACZAACZAACZDA9QjoZOrChQvNJO6kSZPQr18//Pnnn4iMjDR1eVQEqlixonmtE7oMEiAB+yUQERGBTZs2oU2bNjhy5IhJ36hp1nbu3Innn3/eiDgqAKkAU7lyZYwfP96kdhs4cKBJxzZx4sQswVGxSL97WrRogccee0xSRa5CgwYN0LJlS9x1110YMmQIRo8ejbJly2L58uV4+OGHM8ShLJ0gH+5ksVhw6dKlfDhyDpkE7jwBCjx3/hqwByRAAiRAAiRAAiRAAiRAAiRAAiSQbwmoe0dX6c+aNcs8rgbh6OhoVs4vWrTIpHK6+nO+JwESyNsE3n77bUyZMgX79+83Dh1N3aihwsvmzZsRHx9v0rbpNhVh1Nlz9uxZ4/JRAaZv374IDw83gozuc6M4ceIEDhw4gJCQENNGmTJlsGbNGrRt2xYjRowwtcEefPBBI/hoO56SAlGdQermYZAACZCALRJwtMVOsU8kQAIkQAIkQAIkQAIkQAIkQAIkQAL2T+DChQvo0qXLDQeqK8O1CHvp0qUxc+bMG+4bER2N3yWl0p9//YWU1FTosQwSIAHbJqBp19auXYsnnngiwwWioo6mWNO6PEWKFIEKvRrqutF9ly5datw3rq6u5nVWa+REy3eEi4sLDh48aB7q2FGxR0NdQefPn0e1atXMe/5DAiRAAnmBAB08eeEqsY8kQAIkQAIkQAIkQAIkQAIkQAIkYGcEDh06hKZNm5oJ1awMTes79O/fH8nJyXjqqaf+dcgHs2fhnSlTkSSfawQHBkKP2SIp4IKl4DqDBEjAtgm88MILuOeee4wzR507Krg8/fTTOHr0KNLS0oxgW7RoUcTExCAxMdHUzmndurWpyTNu3LgsDa5q1arw9vZGp06djHC0d+9ehIWFmbYHDBiA6dOnm/aaN29unDsqBmltHgYJkAAJ2CoBOnhs9cqwXyRAAiRAAiRAAiRAAiRAAiRAAiRgxwTee++9LIs7Vgw6yTt27FicPn3ausk8b9i+HW9M+gB3SRH0z8e8iylvjoK3pFSKExfASUnlxCABErB9AoEiyj755JPQlG0qsGgaNn3W2jo+Pj7Q9GoajRs3Rv369c1rFXg0PVujRo3M+6z8o+0/++yzJi2buodUCP74449RsmRJtG/fHpo2Umv+aDzwwAN47bXXTFq4rLTNfUiABEjgdhOgg+d2E+f5SIAESIAESIAESIAEzB/LOkk3aNAg6B/m14thw4aZtDy6wvuVV1653m7o2LGjSbOhO5QoUQKLFy/O2Ff/6LfmcteNnTt3Nn+oZ+zAFyRAAiRAAredgK7AnzNnTrbOe/LkSfzxxx945JFHMo7/e/8+83q83CtKFytmXhcMLoBHxPFTSNIvnZPi7f1ffx2Rkp7p0dYPol/Xbpgx7zvMWbQYaZLG7RWZ7FVB6M3Jk5Esk73+fn7YJGneenTsgHVbtuLkuXPoKa/1uGekTkdEVBSKhIZi9caNuL9hQ+Ma+k0cB43q1ME0Kc7u6e6O85GRIjq9jw3bd+CspH2qKLU+mjWoj6FP9oWruAI0lsk4JkgR+WRxCGi/NA3V1K++hrOzM4Y80QftmjXHkzKms3IvC/T3x+SRIxEik+D9RvwPB44chZfUBxnzv9dQRyamGSSQFwlMlv/nMoe6eKyxZMkSI764ublh0qRJ1s0YNWpUxmtNr6bfCTeK33///YqP9fdCfagLyF3+X9VQ9441unfvDn1oqNNHf1fVPjBIgARIwBYJUOCxxavCPpEACZAACZAACZCAnRNYsWIFUqU2wsMPP3zDkW6ROgq6b1BQ0A3302K5u3btMvvs3LkTZ86cQaFChcz71atXQ7dZo6FMxDFIgARIwEpAXSRak0HrOOiKcF0Fbp3ws+6TnWf97lm5cuW/DlWRWVME5ffYLo4bnVzNbmzYsOEKgadh7TpwEnGk90svoVu7tri7Rk00FrHlqNTq8BLhRmvz+EpaplXr1+OeOrXNaVUccXJywha5f0RFRyFIBJTzIgQdPn4clcuWRSURZD7+8iuEyD3oLqnJMXHG57i/UWMj5mzfs8eIPvfdfTfmL1sGZ2mnc5vWmLN4Cb5Zshh9Oj2C6NhYbN21G8Uk/VPT+ncjIjIK4z+bgaLyvkf7DqYPHjJp7CfOhOUi9OySe5nWDaotaam0oPtbH01G++YtEHvxIvR8d9esac7j4OBgxqrbSomY5eJ8WSzKLkseRwK2TCA3hZWsftfr/48MEiABErBVAkzRZqtXhv0iARIgARIgARIgARK4ZQI6UaeReVJVBSIN62fmDf8hARIggX8IfPHFF6hevToaNGiANWvWmPQ8OQGnVKlSphi4FvD+8ssvzWstDh4qro+sxu7du9GmTZus7p6n9rOIa+a/xNXHVyxdGp+PHYNEcd+8On4CmvXogfLC+9uflprTqHgz4913jTBiPW/nB1tjoqReskatSpXQqnEjs8/Szz7D0127mI+mvvUWRkm6pvT0dOw+eADPPNbNbP9QHEHv/eMuHdS7Fz4c8TpCg4PFsbPdfF5OHKUrZ8/Gk48+grLFi6PZPQ1QVBYffLf0J+spjeNn7gcfmPfq3hk37GUsl2MWTZuGrYsWQQWg2SJCeosYFSSuIh2Hhgo7Gh9JH8rL2BkkQAIkQAIkQAL5kwAFnvx53TlqEiABEiABEiABErBLAjVldbOGVdTR11axx/qZbmOQAAmQQGYCd4sLQ+s8jJbUWlGSeuvgwYPm41mzZpl6EFqbIS4uDqdOncIH/0zGqwtRU0fqpL+GFvjWlJDWKC4T+vfeey/UNajuIH2tjwIFCkCLer/44ouYMGECLly4gISEBLz88su4KE4NjXfeecfUe3hXBAl1KOqzvUUdcdcol+xGrVq1rjhU06w9cG8TbFqwAJt/+AHvS9qyMnINXhj9Nv6UVGu3EuqQ8fHygj5r+IvD5lqh2637+Pn6/muXdVu3oqbU8+g59EUMf288Bo16E8eldtD1CrY/2flRPPHIoxntWNO4BYqw069bV/woAqS6dtTRM/mLL9FExMO7peYQgwRIgARIgARIIP8SoMCTf689R04CJEACJEACJEACdkegSZMmZkxWgefo0aM4fPgwND97JVmZzSABEiCBGxHYI5PnWtS7YMGCmDt3rhFZXn31VVPfYerUqWb79OnTzQT9ekn1NWPGDGiqMS3QrS6dYHFv3CzOSy2W3r17G2dOWUkD9thjj5l0XJqKcsyYMaaG2J9//mnSxfXs2dOkm+zVq9fNms1zn2vB9D59+mSr33p9rN/31gaekuv0uIhkGqWKFkXPDh0xXQqpa6zfttU8W//J7P7RlGi5FX2lT25Sa2fKm6OwXWqJ/PHtt6ggzq7rhaZ5u148+1h3k2LubREbP/7qS8SI4PjKM89cb3duJwESIAESIAESyCcEWIMnn1xoDpMESIAESIAESIAE8gOBChUqmPRHuspeV8hbi+rqqnnrKuv8wIFjJAESuDUCjzzyiDnguNRe+e6776Dig9bKadWqFdatW2eKa//222/GdVOvXj1sFWfG0qVLMXDgQPOszpvGjRtn6XtGXYVaI0wdQfq9pA4efTwvKcD0fD///DO+//57427R/dTlYq0pdmujsv291QGlKfLi4+NvqbODBw9GkSJFrjjmyIkTxt3SQwq0t2jUEOGRkVi4fIXZp2r5Chn7FhLBf9lvv6OW1LnZf+QIZnw3z3y2UgS7GhUrYevOXbCIK+vHNaszjlmyehUea9fevF+9fgO8PC/X49DtA3teFt/WbdmKbm3bmX32HjqMvbK4wMXZ2aRUU+FmxR+/Y+GKlWa79uHrxYtl/7bmPJv//tsc98fmLRj54QfQVG2tGt+LuplqNfmLQ+jZ7o/h3anT8PvmzWguKQUzf57RWb4gARIgARIgARLIVwTo4MlXl5uDJQESIAESIAESIAH7J2Bd1a0uHmt6tqZNm9r/wDlCEiCBbBNQUUcdOSoSW90dmzZtwsMPP4xIEQpUYLGmYtM6OmskVdaWLVswdOhQ4/JRMblly5ZZOn90dLRxAGkaOH30kFox6gByEadHuXLljOijDpX8EDpOTUGnKfKyEs4imGgqvEGDBv1LTKtZpTJqi2jzh4hvg998C6Mnf4wTZ87g3ZdexH2Z2h/9whCzve/wV/Hep5+hQGCgOfXy3//AivXrsENcXPoz8Pm87zO69M3iJYi/dMm8X71hA5au+dW8nvfTz8ZJo29UdFHBSEOf12/fZs4dFRMDPdfQdyTdnrhKVag5Iy6uD6XOjv5MfTr3W0yaOcsct3nnTvNa3y9etdJsy/zPM90eM8cnJidjGN07mdHwNQmQAAmQAAnkWwJ08OTbS8+BkwAJkAAJkAAJkIB9ElCB55tvvsHy5cuxQSbiNHTbxo0b7XPAHBUJkECOEFA3zetSsH7EiBFYtWoV5s+fj+7du5sUapquzSrwaK0e/U5RQcfd3R1FJR2Y7jtgwIAs9aNdu3aYM2cOnpEJejc3N2wWYUAFJBWl1X340EMPmVRt2g9172itH3uOQBFYFi5cCE19N3bsWONsutZ475J6M5rCTh1U6nC5OsYPe8VsSk5JwVHh6CT7FAsLMy6azPu2va8ZDq+5x4gwhUVgsgo81n2eFRElc0RuaZHxNlJEvWtF5u07f/rpil1aNWpsXDva5zLFiv2r7wumTLli/xu9SRJhR+v3tBK3WC2mHb0RKn5GAiRAAiRAAvmGAAWefHOpOVASIAESIAESIAESyB8EdOJVQ9Mn6cRoaGgoKlasaLbxHxIgARK4EQFN5+jv72/StHXr1s2IMAsWLEDx4sWh6ds0ChQoYL5XWrdubd63lTRbmmJN07plJQoXLgxtu02bNkYgKl26NEpJXZYXX3zR1P3Rc2m6N/28Zs2aSEtLQ9++ffHJJ59kpfk8uY+fn58Zv9YaUnfUX3/9BXU6eXh4oGTJkqYeUVVJV6YOnpuFqzqhSpS44W6eIszVuI33hRvV3blhR//58E/hcTb8PH769VfEJySgUd06WTmM+5AACZAACZAACeQDAjf/7SgfQOAQSYAESIAESIAESIAE7IdA+fLlzWr4M5KaR0MnbBkkQAIkcD0CO3bsuOIjFXSsoanXksU1oU6azGFN/6jbOnToYB6ZP8/8OiAgAPv378+8Cf369cNTTz1l3Bjq4tHYtm1bxj6ZHYcqeFz6Jz1Yxg52+iJEatM8+uij5mGnQ7zlYUVKircH+vTJcJBpA6+On4COLe9HaHDwLbfHA0iABEiABEiABOyLwL99zfY1Po6GBEiABEiABEiABEjAhgloiiJNW3P149tvv72i15py7ep99H3//v2v2M/6xuri0fesv2OlwmcSIIHsELha3MlOG9c6Rr/DrOLOtT63btPUcV5eXta3fM5nBALF3fS3OFJXfvFFxuMveU9xJ5/9IHC4JEACJEACJHAdAnTwXAcMN5MACZAACZAACZAACeQ+AWtNi6vPdK3t19pmLYZ+9fEq8GiNC43MYs/V+/E9CZAACZBA3ifgJjWMxLqZ9wdynRGUqVABZa7zGTeTAAmQAAmQAAnkbwIUePL39efoSYAESIAESIAESOCOENAi0VmJ5cuXZ2U37Ny584r9NPWRPjLHzJkzoQ8GCZAACZDA7SWgdYRyNZyccrV5Nk4CJEACJEACJEACtkqAKdps9cqwXyRAAiRAAiRAAiRAAiRAAiRAAiRgBwRat26NuXPn2sFIOAQSIAESIAESIAESsC0CFHhs63qwNyRAAiRAAiRAAiRAAiRAAiRAAiSQ5wlo7aDNmzfj+eefR2JiInr16oUyZcpAa6oxSIAESIAESIAESIAEcoYABZ6c4chWSIAESIAESIAESIAESIAESIAESIAEMhEICgrCO++8g/3796Nfv344efIkevfujRo1amDevHm4Vm21TIfzJQmQAAmQAAmQAAmQwE0IUOC5CSB+TAIkQAIkQAIkQAIkQAIkQAIkQAIkkH0CxYoVw/vvv4+///7bCDx79+5F9+7dUb9+fSxatAi5XqMn+13nkSRAAiRAAiRAAiRg0wQo8Nj05WHnSIAESIAESIAESCD/ENCV3JrG5+zZs9i2bZtJ7XPmzBkkJydnQNB9UlJSzH66nau/M9DwBQmQAAnYPIGyZcti9OjRqFmzpunr9u3b8eijj6JFixZYvnw5UlNTbX4M7CAJkAAJkAAJkAAJ2BIBCjy2dDXYFxIgARIgARIgARLIhwR05fbXX3+NYEnl4+XpiWJFiqBR/QZo2rARihUtCk8PD8yYMcOk+KlevTrc3Nzg7eUFd3d3FC9eHAcOHIDFYkFCQgJOnTqFqKgoThLmw58jDpkESMD2Ceh39AMPPGBEfO1t3bp1Ubt2baxfvx7t2rVD+/btzWs6emz/WrKHJEACJEACJEACtkHA2Ta6wV6QAAmQAAmQAAmQAAnkRwKHDh1CxYoVUb90GWx4dSRCfPyghbkzx7oDe9Gmb184OznhrU5d0HfgUOgeaeLmmbJiGcqVK5exu7OjE1ItaeZ96dKlse6PPxBSsGDG53xBAiRAAiSQfQIXL17M9sF/yPdxx44dERsbCyf5PldBZ9asWXB1dcX333+P119/HatWrcLKlSvRtm1bjBw5EhUqVICjI9elZhs6DySBmxDQRTL9+/e/yV78mARIgARIwJYJUOCx5avDvpHAfyDg4uJi0tf8hyYyDtVUCc7O/LrIAMIXJEACJEAC2SawZ88eNGrUyEzwqZCj95hNo95BmZBCSJf3zoULQ2b+TPuWJEnBdu4sdkpR7vplymLhkGFw8RMByD9ABB6ReGKjMaBFK3SoXQfVhw/F5jfeRQkRc9Ll6JS0VLwx/zsUKlQIhcLCoPUevL29s91vHkgCJEACJABTQ+fgwYMoU6ZMlnHExcXhvffew5gxY8wxHuLKfPHFFzFs2LAM8ebhhx/GQw89hK+++govv/wylixZgsWLF5tt48aNQ2G9NzBIgARylMCDDz5oXNA52igby3UCmuqSQQIkQAKZCXDGNjMNviYBEiABEiABEiABEsgVAlorRycSli1bhnFdeuDRuxsgVUSYZm+PxIZ9+1G6QCicwgojXdKspUVHQVUah3QLUlLT8NNfW42441akqKm5ky6ThXBzhaNfAODtAzkKT93XApOW/YQxXbvD3d8PzgGBGDPweYzo1BmDZ36KwMBA/Pjjj6bOQ64MkI2SAAmQQD4goDXQmjdvboQeHx+fG444KSkJkydPxmuvvWbSaOrOYSK4r127FkUkFefVoaJ/9+7d0a1bN8ycORMDBgzA/PnzzUPFn+nTp1Oovxoa39sMgUqVKpm+qCMmr0Tr1q3zSlfZTxIgASFg/X6xft8QCglYCdDrbCXBZxIgARIgARIgARIggVwj0KNHD6xesRIHx3+Ivp27wb9kaQSKOPP7/97C20sWIFmcPJYzp2CJiIBDmgUOUlNHRZ6T8n7a40/DSVZ8p547I/uchkVq7FjOnUPayeOmv44yydihVl2cjYtGTNxFpMdfQlr4eThI2h+vkqUw/aVX8ddb49CudRtTyyfXBsmGSYAESCAfEDh79iwaN26M8PDwf432yJEjGDFiBMqXL4+C4qgcPny4EXf8xH2pAv/+/fuvKe5kbkhTsvXp08e0r84fTeemQk+xYsXQ75l+iI+Pz7w7X5MACZAACZAACZBAviZAgSdfX34O3p4J5GRKNS1azSABEiABEiCB7BLQybhvvvkGW94aY0Sd9AvhSI+LhUNIQbi7uqB60eI4fP6cCDrpSJP6OeHRsTh9IQJnLkQiWESgAiLgOEi6Noe0dCTK80e//IRjZ8/j4sV4WE6fhoO7OwqJY0c/S067XH9H9087ccKIQCr0FKlSFXvf+wAvPjcIq6S+w+0ITT/HIAESIAF7IqDCjaaC1nSbd999NyZOnIgOHTqgZs2axp2jq4rHjh2LY8eOQd0+nTp1Mo6dU6dOGVHoVv5G8fLyMi6eo0ePYtSoUUiT7/eZn8804pG6ghITE+0JLcdCAiRAAiRAAjckYHXw3HAnfpgvCVDgyZeXnYMmgVsjYIsTVLbYp1ujyr1JgARIIP8QUPdOvRKl4eXihtQUET3EoZOuhbolfY+TrNQe3vohfLJqOSJi4/D2Dwvw8IfjsHznX9h67AjeWbRAts1Hguyr4ebmhn7NW+KL9b8hKi7eTPhZwi8gVOrybDt+FBYRgVQcOhsRhShpz5IsziARgSwxMQgUN8+610dDU5KcOXPGpHvLzaugk5tae4JBAiRAAvZCQN00JUqUMMNR0eaVV17Bzz//bASfyMhIuIvgrqLOnDlzcOjQIXz55ZeoV6/ef6rnWaBAAbz00kv466+/jOBjrelTvXp1fPTRR+Y+YC98OY68TUAFTk7A5u1ryN6TgK0T0Jp1DBK4mgAFnquJ8D0JkMA1CdiaoKKuIjqLrnmpuJEESIAEbI7AihUr0LFWPRwXIUaFFxVgUmUldnp0tOlriNTMqSMC0L1vv44SQQUwb+BQdG3YCC1l8u5/HR9G1/oNzbEWqcnjJHUbXKSezsP17sa4nxbhXGSUPKIRLULKkkHD0GzcKHSeMhFvLf4e248dxanwCHO+FDlXukw+FgoJQbsatfBw43tRvGhRzJ07N1d56f2TIk+uImbjJEACt5HA7t27rzmBXbduXfz6669Q0UdFnY4dOyI0NDRHe1a8eHGMGzcO69atQ5cuXXBaxPuhQ4caAUm/y7XWG4ME7iQBrRWlQZHnTl4FnpsE7JOA1hJlkMD1CDhf7wNuJwESyPsENAVCTgkzKqbcrJDq7SZmTcvgIXUZGCRAAiRAArZHYNq0aXjjjTeQqk4dV2ccCD+LJ2YswMiOj6IWSiIsOEg67YACfr7wklXfXe9uiIfq3QXvoECkX7pkBuQi97JSMkmYLuIOpAC3pnFDcgqcHZ2w+eghzN/yJ347sAcFvH1RrmAhTOjSE1q/QfeLTUxA+4/GoXbxkni9/SPQqUbX0IKY/PhTqDR0ELaOGY9nxozD1q1bMWbMmFwDaBV5bO0+mmsDZsMkQAJ2S6CoCOPfffcdqlWrhu+//x6jR482NXE2bdqESZMmYfz48fD09My18TvIfaBy5cqYOXMmnnvuOYwcORLLly9Hr1698PTTUq9NHEaMrBPQ+1PSPw7ZrB/FPW9GQCdiBw8efLPd+DkJkAAJ3DIBOnhuGVm+OIACT764zBxkfiWgwkdOrRrWX/5V5LE1MYUiT3796ea4SYAEbJmArqJu27Ytfl76ExYPHYb6ZcqJNuMgUg7QSZw3XSZPRBH/IBQSIUe3JyQni1CzEa+2fQhegQFId3JEkmxLSU2Dl6c7HETkcQwIhqhEsMTGIjX+IjYePIB7y1fCk03uw5NNm13GIW25yLGO8qzruPX4eiXLYOnf2zBy4Ty80eERhMnnriIOlS8cBnc3d3w98l10fX0YPq/wOR5//PFcw0qRJ9fQsmESIIHbSKBkyZLm+11POWTIENStWwd9+jyBE1LzbP78+diwYQOmTJmCZs2a/ae0bFkZUq1atbBw4UJs3rwZWpMnIiIiK4dxn2sQKFWq1DW2ctOtEtAUbfpQpxuDBEiABHKSwNKlS3OyObZlZwQo8NjZBeVwSCA3CaiYokVVb6U4am72x9o2RR4rCT6TAAmQgG0QePPNN7Fi2XIcnPAhAry94RgiDpyYWEAcNQ4O6fjimeew5eAhY8gRqw0sFgtiLyXAW1w8cHURQccFp6Oi8NmaVehcrz5C/P3hLGnYHMSYk5ZqwabDBzHxlyVYOOglEWlcjSDkGBQMBxGA0tNSgIvxsIgI5OrijNDgALStWQefrV2FU1GR0pafbHdEoLcPLEnJBtj0QS+i1gsDTN2I3HTZUOSxjZ9P9oIESCDnCDRq1Bj79u1D7969MW/ePJM2rX379qbW2YQJE1CsWDEj5OfcGf/dUp06dUwdoH9/wi0kcOcIaJq2smXL3rkO8MwkQAJ2Q8Cano3uHbu5pDk+EAo8OY6UDZKA7RBQISYn07TpyNQRpJNfFHls5zqzJyRAAiRgSwTCw8MxUtKy/frqSAR6+cCxcBGknTqJ5KQUuEqaNseAQLjExODu8uLqkdfp6g5Ns8DfyxM7jh9HqBTTdpY6O8WLFMagVg9goazMXrRtEyRBG5zEmXM6Jgr3lCmPb/oPRmFN8ebuCaeQArBIm0lRZxEdE4e5G9eZWgy9GjWBn7eXSQE3vnMvHA4/jwphhUXgccHA5q3gJvdJh5Qk+AYFoWXFyvju22/R54knchUnRZ5cxcvG8yABXZHKVal58MJl6rKmxZw9ezaGDRuGDh064Lh8l+tklD66du1q6uYEyfesOjYZJGDvBLQOjzp4mKbN3q80x0cCt59AxYoVb/9JecY8QYACT564TOwkCdgWAYo8tnU92BsSIAESsCUCI0aMQPHgAqhatDgcZULPIilzYuMuIl0m9tzUnSMuHWdfX1MjxyK1dBAdDWepmfB0k5YY99NCNKpQAb7ivnEuUBAFJMVa7yZNsf/cGQxofj+8PNwlBZszPFxd4eIsdRakLXX1WCQ1kJ5j+a6/sfnIITzb4n4UDgw0qdqUjU4qhvj6YOuxQyZ1m26rV/ryqtp0cfGky3kqFy6KvbIK/XYERZ7bQZnnsHUCmsZIV6Lu2bPH1rtql/1T52R2onz58tc9TK/prl27sGTJEjz11FNmYdicOXNMzR5NAfbVV1/h/9q7D/ioquyB42dKegIkdALSUUREVERRsWFZwIa77ipW7FgWy7J/FdeGXRQsuKuiq7tWRCyIiuKqoKJYUQlFBKTXEEghZWb+57zJC0NIMEACM5Pf3c8w7b03937fzMS9Z8653bt3r3Z/nkAgHgQiy7SRxRMPZ5QxILBnBSxYbD+Esf9msu8XGgJVCRDgqUqFxxCII4HaXIcnksWCPMk6scaaPJEq3EYAAQRiX8CCDzVt9qttu7jN1mp78sknZdSZ52ngRdfcSUsTXRRB19MplbsnTZSHz71AgitWOIGZUCgonvKFnXXJHumanS1Jmlkz9aef5NTUFCfzx6Ol2Xzr10vzBg1l5sJfZUDPAyVZg0TWyjTrx6+lQ0O6so+t4fO/nJ/ll9Wr5O4/nx3xK/GQFJeWyqbCQnnsg/fkxB4HiF/7a2sEhX9Irmv15Oc7x2uo69YtLr/tPFDNPzviU80hnIcJ8mxPh+diRcA+S7vSKDWyK3q7tm9x+ffvrh1l272tnPPpp58uxx9/vHz44YcydOhQWa/f4/PmzZPevXtLixYt5JhjjpFrrrlG9t9//4jv622PxSMIxKoAWTyxeuboNwLRJ+BmOfPfTNF3bqKpRwR4ouls0BcE6kCgLsq0ud20tW/sEm2BHtbkcc9QfF2XLl0igVWrajwon04gJGhpqGhuIZ1IL5k3V0p++tGZZPZ32VuSunUTn5atqtyCOvFcMneO87C3UaYkduxYeZM9dj+wIVdKFyxwXt/XtJkkaL19WuwKWJCmpkGMpKQkSU1NrRhsrq6bo0vsyPH77af11Hy6Hk44WNRAy6Styd8kN738klzff6D4bY0cDfCUlpbJLC3l8/5P38vqTXn6WEhe+eoz6X9AT0nQHxJ4df0ejx7zoqOPkzHvvSNvffuVtGiUJT4N0vTdu6ucdMABTrBmlW5z+5vjZeYd91ZMFq7T/T/Sz9ZMzehZk79R9/1ayrTQ2zKdaOzXrbuWZ0uQjNRkWbhujUybmyOTvvtGehx/XMVYqrtRUKB938lfvlc+JkGeyiLcRwCBeBFI1+9vK9c2cOBA+VpLbVr2jl2WL1/uXL/44ovSvHlzscweC/z069dPevXqJQ0sw5OGQIwLkMUT4yeQ7iMQJQKWvWON4E6UnJAo7gYBnig+OXQNgdoSqKssHrd/kYEe9zH79d6ebAR59qR+3bx2/r/GStm742t8cH//MyXzjrtrvP3Oblgw9UMpmz9XGl5+Zc0PoZPYeTqekmce1dpSW7IlivUIBVpvyj/gTGk4/CbxRkycF8/6QfKvOd95De9+h0jjf79Y89er4y2LPv6fFI0c7ryK76gBkjVqTB2/IoePVgGbuPNq5k6iBk88ySkiVoJNW7G+zxskJUtjnfAb+NC9kpqYJAENkqRrJujx3faXizWAk67BoknffycPvT9JistKJWGjBny0VJvWcpNGGiD6+ymnycbCItlcWiKZepxEDSBtLCh0Mnksm8eCLhsLijT4Y6+va9B5fXJI505ynP5CPFHLuo0+d4jkaqDnfzmz5fLnnnJe90+9DpWWDTLl8uNOEFuv556335Bzzz1Xnn322d221hxBHuctsnP/WKAtIoNs5w7CXgggUJcC9mOzQw891LncOfJOmfHFDPnggw/kvffekwX645CVK1fK559/Lg8//LDsoyU6v/vuu4pAfV32i2MjUNcCbhbP6NGj5fHHH6/rl+P4CCAQhwJk78ThSa2jIRHgqSNYDotANAnUZRZP5DjdoIo9Fnk7cpvdedvtQ7SVkdudBrxW3QmUrVguecOvlWDON+I7emCNXyioWQzrLxwsoYWzq95HsxrKJr0s66ZPlcZvTNEMhoyqt+NRBKJQIF8zzWy9G6dsk66T406+D/nX43LnaX+WFpmZcuFRx0peYYGz7k4DLYuWosGehASfrpfjlTMO6S0vfjlNinRdnGLN7inW0mt2vBQN/lhptmaNGkh+UbHkanbOZ7/Mk581s2/J+jVSqEGfXA32jHzzNWnXpLlmEHWXBhpgapya4azb40nwi0c/S5mpnaR9+44azDlKHtKMoDFTJstfT+ivASGfZDRtKnf8ZbAMe+Yp+fvw4TLqoYd2m3C0B3mK/zZMPLO/3a6Hd/Q48XcOr2u03Q2382Qwb4NoWpdzrmytpuraZj1H3imTNLtLszo1eBds1V68V18vCVqCj4YAAtEr0KhhIznppJOcywMPPCCLFy+WGTNmyMyZM8XWKjniiCMI7kTv6aNnOyhgWTz2q/sJEyaIBXmGDRu2g0dgcwQQqM8C9r1hjeyd+vwuqPnYCfDU3IotEYhpgbrO4olWHII80XpmdrxfKWecKaX7atmn8hbSX/gXjxnp3pWka24WT4JOKJe3hK51uwBh0eefOcEd9/Vqep33wL1bgjuaYZBw+rmSeERfCa5dI8WT3pTgrC+cxedlwxrZcOvNmg3zSE0PzXYI7HEBrxPcEc3O0Tpttr5DWrh824xf5jtr9TTU+z7NvGmc3VLX6PFVBIBCeXlSotv/ouvzeHVNnTveGC9e3c6nx7PgzfyVy6VRSpoM7HGQ5KxYJrOXL5FDO+0tx2uJtvYtsyVds90CWvJw9bq18rlm1N39zkRJ9Pjk+j+cIs0bNpSsBhkS2rBBc4H0JRs3Fo8GEW74w8ly8bgn5LvFC6Vn2w6SlrxRfK1ayz26TtDBN14v551/vvTo0WO3mUZzkCcUDIhXg89OCwTEY5mHdq79W75zLdNqV1vpsCvEt/QXCZwzVJLOv7DKw22+9Sbxf/6B81wwI0s8RfniWzxHQsMvl5LbH5LEQ/tUuR8PIoBAdAnY+m3t27d3LmeddVZ0dY7eIFBLAu7ErAV5rNTSgAEDaunIHAYBBOJZwL4v7IcP9h3ifo/E83gZ264LEODZdUOOgEBMCOyuLJ5oxCDIE41nZcf7lKxrctjFbSGdDI4M8KT/8c9blTRzt3OudUKy2Nav0QnJxM5dxKPlQrZqWuYnZKV+ylvl522tHLd5rByQLSodsb3uLLaN89x2ygWV/PLLljJz2pf0R5+TlN6HuoeW9NPPkNybhkvZlNedxwLTpkhAy1T5dIH56lqJljcJFW+WpL33cdY82Wo77WdIx26tqr6547IMCVsvxWnV7FOqv7INal8S9+mqgbSal2B0X6OqPtg5LF26VAIrdVK/USPxt2mz3bGGO8i/0SyQoWsn2No6ZRoQCBUValZOY33zibRt0lRsTZw2JY2djBp7TrSMmy7241zyNxdLzm9L5LGP3perTviDnKKT9EnlJQqDuuaNvY9vn/iaPPnph3LXmWfLfZdcJt60DPtIO5/FkF5bwKhZp07SrefBcnFervw0f55c/MwT8sCZ50rPpPZOFpDoelGetHTxN2kigWVL5eHBF8il4/6pAZ72TtZQumbYZbTKliv7nSj33nuvvPTSS7uVO1qDPMmjtJxkeSv+7/Pie+5RCabrWmCvv+8+vNV1SDO5yhb+Kv4OHdU7zXkuVFjorDUW0hJ6Pn0/WAvZudWLpnOFv1M18OY8rqX0guvXiTdL3z+RTb8zfDP+5zwSuOLvkjToj853b8kVQ8S3KEcC4/V86XsnqOfZygN6bA2n8vdRQIPoHg08ejTg56wPpWsxuX0Jrl7tvJ7P/j7Yd6F+v5ct+EV82a0r9rcXtUC8vsHFq+8f63fZooWataT7lGcbBZYtc74fvc2aRfbauW3jd0w6dRaPZqS5LaRrXoX0syH6mEcz2sr0b5XP1jHTLDa3f7atfZeGtM/WPFka2Kr8d8x5hn8QQAABBKJNwCZnc3JyxC21RJAn2s4Q/UEgugQsuGPfF24WYHT1jt5Eq0ClGa5o7Sb9QgCB2hCor1k8ZkeQpzbeQbF3jJBOFm4YeZsGVV7TSblwoEPTAsTX+1hpdM8DzgLuNqqSRYsk76z+mnYQDuQkXnxtxZo6hZ9+LAXXXaJbaVBH920wbrzkj7pXgj99Zbs6LfDJZFl76GTxHXuKZN1ffVmngn8/7Uwc2k7e/Q/bKrgTPpJIw5tukQ1WvkonqZP7HF5twKPggylSeNsNOgmoE+XWtExR0hXDpcEFQ8L39V9nm5vCawN5dcyNH3+y4rliXXx+4wWnO/c97feVJuPfcm4XfjZdCoZd6NxOOOcKSeiyjxTefZN+iHQS1poaJJ43VBpeeY3OMtrsevUt75GHpeT58prjCUlq96ok7dtNQlp2a8M9d0rZ5PEV5s5RtESXd98DJfNxLb9EabrqYaP4mRYtWjhr68zRLBsrv9a4eal4mzWX8ddcJ33v/IeMPusCXYcnQ5pqIKhTK83i0bdQoQZ3vv/1Vxn78RR57IKLpYkGW0Qn5T0a1Anp2jo2AX73GxNk0+bN8ukd90piZlY4MLBmVbgUnEWQ9PMZ1OBkUCM9Pl2vx8qxddcsvil/v0UOuf1Geevq4dKhZQt9PY8ue1Um3mSbYPdIhk7Mf62BCFu/x9YECtmaPzp5fkafvnLvzdfLJp14z8jYvWUSozXIU5O3nQV2SiwL57d54tFAX0A/0wFdDy1p2PUS1PWZQkMHO+eq7NHnxd9lbykdcpZ416+QsoFnie+918VXZiuRabznjf9KmQbzEl9527m/9T/lmUIaMLFmgQ7fzXdIYM7siu/00huucbJ6An+5RJIuutTZLnDBH8Wr35fBEfeLt2UrkSvPkZB+L5V27Sn+WTOcbUpTG4j87VaR+24V72YNUun3aqDvHyT5pn+Ej3He6Zr9VSwlh5/oZBHZGMv0OzF0/e0SeunfTvZRSN9jpR33k8T7x4hH3zsW2Cm59Ubx/fCFmoTCx9ynpyTefLt4tSxgyasvi++/YyXQsr14NmhQq2ijlF16vXifHKXft14JTZjqBKpKP3hfvA/dpllTCeqjQS4CPM454R8EEEAgFgRGjBghd955J0GeWDhZ9BGBPSjgBnesC/a9QUOgpgL6M2QaAgjUFwHL4kneTk37eHewIE+RThTS6o/AurPPkLJ3XtkS3LGha6An8MUHsu7UfhLS94S1xA4dJPlqDWKUt5JnH5XS334TWy+nYMR1+mh4QjHxgqskqfv+7mY7fB1c9GvFPonHHl9xO/KGBTayRj8uDbQ8kWUbVdWC82ZJoQVunOBOeZBFg1PFj90tBVM/rGqXnXqs7P23pfBWrRfuBHfKX0f9Sv79qORPqmridcvLbJowXoM7Y8MP6CRp+iPPOsEdeyD3lhul7G39pb0F1GyGP0Un0O1aJ0uDP38tG24cvuVA3IopgYaWHaHt3599Ivn6+SrRLBmPnv/s9h3k3Rtukpn6GViwZrVkN9EyaXrOdb5b19PJl5Fvvy5jzhkiWY2bOpkKweXLJLB0iQT0+pVpn8oGzZYYdeHFkqDr6hRpoMDWbVi6eq3uqxkgehB7/qXpn8mX8+ZKUW6uhDTTwj6/aRrIeeq8y+TLBfN1TZ9wdkgwd50E1mgmhrYyDSA5j2tfPJpZYh91O14TzShrrpk+X321JZDr7LCb/nGDPLvp5WrnZdSt5JpLncBKMK2hlPU6WkJ67n3vvCwl77wtPs2usmCJnfTAg3dL8YRXneBOMCVdki4bKsFB50gwRQMs2gIaIAmdXkXJJs1yCfQ+xtnGsoiKh5wtxe9pcD07W5JO0rWUtNzljjQL1vhyvpOyg/o6gRNv4Ubx3n69hNp0lLJuvZwgo//jSU52T+RxrURcoMehYuP06Hei94ER4tHAvDNmr475lx+lZPqnzi7Fjz4svu8/l2DDplJ21qUS0rJy/p9nSsm9d0QeUnwrFmpQaZMEE1PFe8hheq1rEGnQsVR/ZGAt9PFU5zrQ47CtMoCcB/kHAQQQQCDqBdwyS/bLfJvEpSGAAAKRAgR3IjW4vaMCZPDsqBjbIxDjApbFYxNHdqmPjUye+nPWN41/RUILZzsD9jRrI6k33iYJe7WTjfffJcEvPxLJWysbdaHuhkOvdrbJOOc8KfnfBxLUX1lb4CFv+F/F20J/5a0TftY8OuHY8IqrnNtpV18nRW++rhkor4af67y/pOjktL9dO+d+df8Ely+ueMrXunXF7R2+UbJZvAccLg3+caeWqUqVDboAerD8F+ibX3tZ0o7rt8OHrGqH0Jql4tXJxAz7pblmRWy4+jIJLfjJ2bT4rQmIv9TYAAAyIUlEQVSSfvIpVe0mhdOnyeb7RuhzOluuv25Pu2+spPQ6JLytZmUEPg7/n1pPi7aS9fKbzq/urRRd7jlabmmFloKbq5OjmtFhgTdabAnY3xhbJPuzb751Or5yXa600iwOX9MmsveBB8ntrbN1Qjw8pqXr18rzOgn++VxdP0X/N31OjhyjGQsN0zQDxwJ+mmFj5d7GfTJV3rru/0Q0QFNWtFmenPqBTJ8/Rzbr2jwnde8pf9i/pwx/9b8y8oy/SDvNiEhMDJcQ1JCN80IHd+oolz3/lPTRbLTsxlni1Ql5n7fEOf6/9FgDDzhIS8l5JNnW8ArvIps1E6W5lkacO3euHHfcceEO7+Z/3SDP7s4g2tlhBhYuFN+S+VpWzCf+Z1/RsouZUvyf58T3/GMSelOz9QacLAlXXyuBzz90yqmFnv4l/FKXX++UOEu65HIp/vwTkaX6nXtYX0n6y9lVdiX5dg0OjWki3nfHO68no26V0kfukdBl10nSqeGsxCp3rPJBzfh66R1J0MDk5rvvEP//3paA/r1IGvu0s3XJH/uLN2+NlOl3WuIpp1UcIagZO0knniQlH+paQPfpjwN0zAnjXnQybYpvv0V809+T0Gca4PnDAPH2PVqC++2v2Yma1dO+vZRoRpuM0e/uBXMqjmc3gklp4n/mVfGXl3crtkDWtHfDgR0NXnl/nOls7xt46lb7cQeBnRVI2IFyqzv7GuyHAAJbBKzc0osvvigjR44kk2cLC7cQQEAFRo8e7ay5Y98TZO7wltgZAQI8O6PGPgjEuIBNFlnZGYI8KTF+Jun+9gSK/zuu4umUq26Q1COPcu5babb1/Xo5v4wueekZkfIAjz3ZaNQjsv7kY0WKNklIf4Ed0IvT9JfUjR75ZzjLRB9IPuhgKdW1F9wAjzd7L0kbMDC87fb+LcireNa7nXV1Kjaq7oY/UTIf/ad4dTLdWtpV18qmS//s3A7ppHmttcQUfZ1/VaxtlPbXv0n+Nedv93WCixdIwQ2Xqa+WxNNJ85RbHpDUo9W0vDlr8uikvbXQmmWy8fFHJFknL5M1MyrrxYniSUx0LuWbcxVjAhaYeeihh6R3796So9k3XXU9m5Vr10lTDewl6DorbtBl9rIlctrD98vVJ/SXpy+9Qn5bt1YemPSmvPTldLn/zHOkbbOm+vbxyppNeU45txR7X/i0hJ9mNFx+wklyjU62F2oQ5saXX3CCPVced5J0ad1KvBmNnACtlVqz7Jwyfd1kDfj834BT5bWZM+RPBx+qVa880iIr0ynL9sy0j+ShP58vqZrpY/2zdYECuh5L/sZNsjR3vXTs2HGPngH7O22ZpxY4i/ZW9t03ooJOK7vYSrHpV4BmyDjXq1c4114NpJSedYkT9PEESrUsWTtJ6l+D705n7/J/9D3mlHwbcomUvv6aeN54SbwFG0Q0g7FEn4sMxETuVt1t65M1T9Nm4U3Sw1lEdifUMFN/DLBGv6tWh58r/9fTqHyf8mCMruzjBHecfXR9HmsefU9bS+h5kBQ/+YTIc/+S0vwNmhVU5jwuZeGMsvAdfa2mLbWcYXkf9EGvBXI0wOP96Wsp/fYbtdwswYRkSTjscHcXrhHYJQGvBtRpCCCw+wVs8nbChAnOxRZRtzV5OnfuvPs7wisigMAeF7DvAMvcsWvL8nMz/fZ4x+hAzAnwX3Uxd8roMAK1IxALk0W1M9Kqj0K5tqpd4unR0OrlFcMp/flHyXt0tHPJf+5ZjdCkh58rynfWg3E39OkvztPuHu0EJtzH7Dr1Hw+Iv3mLyId27raW83FbYNUq9+YOX3sym1YEd2xn+6V8RdPFyWureVq1qwju2DF99stztxUVure2ug4t0l+ll1l2hDadoE/cr3v4dvm/tri4t3OP8D2d6Cwd/4xsuuiPsubw7pJ7/dVSMHmSE3zbaifuxJTAAQccIM2bN5eLn/2nBlG0HJeW5Fq5PlfWbcyXzbr+kq13M2jMg/LmtX+XqzQLokV2G+m9Xw9dp+d6OaRTF7nqP+OcbW3QDVJSZWNhkbOf+JPE37qNJLdtK57GjSU1KVGG9jtBps+bI/vomireNF3vRP8X0NJsa/I2asDoLVm5IdexG6ylu5rojxvunjxRVmm2mGXsrNUgTvsmzaSj9rWRZqitydskpRpMWbd6jUz5aZYUaDbPkUceuUftraxqzPy91nPrNJ00DrZuH76030dLnR0iwR7lGXy6QWjxwgpTz4b1WmoyHASqeHA7N8pyZsvmZ8dpebfx4tV1nJIuGCKJr78vgS4HOHuFxr+wnb33zFMlt94k/rdfdP6uBDW4FTzxjBp1JEEz3qxknRPYGftweHy9j9ZApxtGq9Fh2AgBBBBAIAoF3Ilcm9S1X+5Tsi0KTxJdQqAOBdzPvpu5434n1OFLcug4FyCDJ85PMMNDoDoBW4/HzeSpbpt4f5xybfF7hoO2+Lb+2tltpa+Ey+2497dc64LXK1ZIgk4Yuy318COloIH+Alt/te00X4IkHXSQ+/QuXXuz20lw4zrnGLa+SHVt47inJLHngZKsE3xVNU+TrYNNnoSIP+c6mV5l00yGyBZyJ2MjH6x026OlsiKbRyfUd6hpFs+m226Uxs+9vNVujZ54WjZccZGWYvtBHy/vr/7SP/jNNCnSy+Z/jpbMl94QX2bWVvtxJzYErOzPtGnTpEuXLk6Q56kLL9OSaF4p1DV57DJtXo4c2aWr7KPrpniz9LNm70UNvthaPTf8YaDMXb5U/qPlrf6qmV2pmrlzXLfucs9bb8g1GgxK27hBSrRUm09LYqVrVkvzhpqxY03fRiEt2RbaXCy/6do8N7z8vLykmW0J+h72Zuo2euwhZw2WCzZqAFSze2wdmPWaAXR8t/2d4I5Xs4Oe+vgDyW7YWOauXC5PfDxFnnrqKUlNTQ0ffw/8G1PBHfXx7tPVUbIMFf/Qa8TfZW/nfslnWt6szxHO7cAvv4j/03ed4K+tX+PNz5XNmt2SfPUw53nL+gvvVE3QR4NB/hf/qZt4JHDAgeKzDCvLQmiZLTLvew2GlAeZNHAni7X034pwoD+oay5VZM6EX6H6f6v7Dq1+D32mmu9dfcb7Y3gdJ49mkiZqJmnJu1qiUglq0oJ9jxfv+xPCpejsWJRnqwkb2yCAAAIxIeD+Uj8nJ8cp2WZr8/Tv39/J6ImJAdBJBBDYKYHItXasJNugQYPErmkI7IpAxIzQrhyGfRFAIBYFLMhjE0huoCMWx0CfEahKwGuTsjqha2vp2IRhsq6X4NH3elXNLc/jPrfh4Qe3BHfsQS0jlDfsSmn8gq4hsYvNq9kJwZxvnKOU2JoUF1+6zRFLl/wmxU/cL8U6YbgpJUMy9dfpfrd0kLv1zvyCu9K6W0FdlP53mxnuRPOfMlgCX3yiZY2WSvDnr6Xw44+2KtPm0/J0jV94TUp0srfo3UlS+vmnEvp1dvh86euF1i6XvPvvkSwtp0eLTYFOnTrJf/7zHznvvPPklDH3ydhzL5Y2jXXdFP083jf5TXn0nIuccm0hy+AIaN7Npo36GbVgikcuPeZ4Gf3eJCnUyXwL4gzToM/rX82QK559ShqnZ8h+rfeSs8sDBkGdjLeycCX2/i4ukQ36vr7i30/KpL9p1kSaHk+3t8l9rUnqfBd4UnV9n2YtJLh6lTTVoNI6DfIkaxDJp4GmG08bJB/+OEvySgqlk2arrVy5co/hx1pwx6ASNChd3H5f8enaZ6Grz5fiDt1EtMSeb9ViKf7zxZJ08WVSdu/t4tMSjWV9jhefls2Tm68S36RXJPDHP4uvZUuRbprdp+v4eN54UTYvWSzJd9yz1Tnw9zhAilu2F9+KhRIaerYUt+qgAcJi8el3jbVg337OtUffQ/LNp+Kb9p4UX7pQvIvn6tpPWwe5nQ13wz+Brj3Fr2ukhR59QDZ/+L74vvo03EcrX/c7wST/yaeJaIDHWlD/HiRWE/R3NuAfBBBAAIGYE3CDPG7JNgvyuIEeG4yVb6MhgEBsC1i2jjW3FJvd7tq1q1OOjcCOadBqQ2DnZm5q45U5BgIIRIWAlX6xS31ckycWJ9Ci4k0TI53wZDZzAgU2gebRgE96+QLZtgbMxqefFH+nzpKo/2Hla1SeAaDj2vz9d1L68tPhEWqGgBMkssySud+JZdU0uOiSitF7ImvX13DiMEPXGsmdPN4JZIR0gnKjlotrcP6FFce0rJq84fZL9vCvwT2ZTbYN7lRs/fs3fBHrOYRyw+tBuHtZ2bq6aN6uB0nmP26XwunTpGBYeGyFd90SXgNJA1NlupZF8TffSNkv88Sn67M0tF/u68XGvvGfj0vJ84873QrO11JvtJgWGDx4sPNrtFNPPVUGjL5XmuraJifoYvNr8zdJk/R0/WjqZ1Pn3Est+KLNr2X/rHxbztKlsmjtWinQbJz0lGQnKPTH3odKX836GTt1ipZ1+0OFS54GdAo0EJSkWWx2vIUrV0mmHtsyhjwaUArqsSz4s1iDPAmapdNCyxkmaQaPrQWUqkHfbxctkBV5ubJX06bia54tJ6SmS7/NRRLQftj/CbvpppsqXmt33Yjlv02Jox6T4vvv0iDGx+L7ZZaENAMycEAfSTzrHCn5YIr4LKtGH0v86/WavdVYNuv3hV+D3qUP6D4PPSa+QWdK4OsvxLd2mch3M7Yl12Be4rgXpPjBe8U74xPxLf3F2Saka9MEBvxRkq+4yrmfePQxsnnaseL7epoGnHK0hFsP8az4TbybNKC4m1vCldeGA1tavtL7xVQJHH6C+Ka/pwGnMilbtGi7vfHvvY+UNNSSnJpRGjxCg1c6fhoCCCCAQPwJuOWZIgM9NkoL9tj6PO4aPZYdbc2979zhHwQQiAoBN5DjdmbevHnO2jqRj1tAxw3uuNtxjUBtCBDgqQ1FjoFAHAhYuTZbyLm+ZPPE8gRaHLzddssQEs88T4rH3uu81ubHR4lXf8mffOhhkqcZOmUTnxcr5LO5Wy8tH/aSs01Q3/+brh+qsZWgcz/x/CvFq5kzm+8f4dwv/tcoKel7lCR2Dv8fK0/Sloyg4JyfpOjzz5xSQSn6GtU1v64Tkjj4soogRvGjd8m6D94Vf+8+Eli8SAKffaCz3W5pIo+kXG7Bnp1v4b7ahKBmSPw2T/K1zFXqsf00o+Z/UvLsozt/4O3sadkR1lKPOFKKuh3sZPCEclfJxmeelgaX6C/4Fy+WwhFXh4+gGUI+NUnRyXsL8IR0Yt1t3g6d3Ztcx7BAz5495bfffpOPPvpIRo0aJb/m5kqhnmuPrs9UrIGXZC3n5vN6ZMy7k+XQjl0kT4M8j3w4WXq37yzjPvlILtKJ+rQk/RGCll386OefnICPy2GBoaXr1ztl3Px+nwZ4gjJv9Qo5rWd4vZeQlnKztmz9OinTIOzTn34kuZopdOcZf5FmWrYtIy1Fxp5/iQx55gm5of8pcrBm2KVrNo+VdGuq67vM+ViDERro2SqY6754HV3Hwt+mpHPOE7FLFc2j/y2RfKd+76qbZU55NXDmlFDTbROPP0HELhEt+RErt6bBvfLH/O3bi1/LMwbWrZWEjAblj2595dH3TPKNt+iDt0gwL88JmFuwKCFyM13rK3nkfc76PvY+8GvQb5v2wcytHkq65HIRvbh9sSeTNJhkzX0sYfJ05777T8L+mnFU6TjJV/5VRC/uPr4OHcT35HNanlOz1LRffr2I3OkeQmzMooF+d/uKJ+yGOrp/k5xsnq2e5A4CCCCAQLwJuIEeG5cFe6x82+zZs51JYnvMAj40BBCILQG3DJv1moyd2Dp3sdTbKv+/RCwNgL4igEDtCbgLOcd7kCcWJtBq76zW3yNlDD5XSt6ZqAt6z5XQuhVSMPwyXTRdgx1uSRwtd9NozNgKoA233lxRms3Tqr00tF+CaxZAydsTw2XV9NfWG/96mTR+c4rYBGNit/2kULMAnODJ6iWSf8354t3/UNlegMderMHQqyV3+TIJfPims29wzrdSopfKLUEDQWn9d60sg1fXH/G07yohLZlk4y664wa9hF/Jo2WOQisXV0weVn792rifcctIyTurv/Maxc88IoE/nSnJB/eSQv0Fe+CzKc7EbP7V50m+P9G57U5kSkKSZFy1a8Gt2ug/x6g9gWOPPVbsYlk29oOCdZrFk5GbLC2bZGmGjleG9R8oz0//VPIK82X6P+6SwsJieWTKZDli5D+cUmp5GoA9s1cfuenU051OBTVgsy5vo7w2c4Zm5Wiwxsoy6uc7S0uwTfzmKzlBJ94bykrnE9pey61Zu+/Pg+W+SW/I01oy8Dot+5aSnCTNshrJy1cMk4WrV8uqNWslvZWWCdO98os2i60ltDszJuLqb5N+d3qbh90d/B38x6fZVzVplUtsbrOPBlMsqBINzatBwx1pxe/pJN74F8Sn67YFG+uPA7ruuyO7sy0CCCCAQIwLuOXbbBgW5LFgT2SrfD/yOW4jgMCeEbDsHGvuNQGdPXMe6uOrEuCpj2edMSOwHQEL8tikVqn+4jUeAz1xNYG2nfPIUzovq5N6jV95UzbcNkLKPpqk6zRsdoIcZuNp2U7S77hfy7NlOlQFH34ggY/eCrNpabYGo7RMmE5QWmt4/8OSe3o/XcNDM0w0kLPh3rsk85bbJKFtW/GfcpaUvWUZQOULbJeFMwacHav5x6NrX2Xd+6AUTD1JikbdFS4jF1HizdN8L0m5Zrik6YLytdGynvmvrL/o3Iogj5Wd8x50pGSOekTWHaW/Pq9+bfBdfvlEXYfF1+9UCXww0clMylM7G3vW6Mcl77ExUvLKvzWNShe8V9tw84i364GSfrP67rXXLr8+B4g+AVsvp61+dsZ9OlVuP/1MWbF2nTTVAE1iol/O1wXoQ0mJzmc3Oa1YbtGA4HX9T3ZKuCUnJkhq+UR9vgZ7NmwqkPmrVshzn38ijw0eIomawWOBmS4tsmXidzPl/waeLpt1TZ6sBuFybYFgSNf02Sxn9zpcet95k5y4Xw85sJNmVmQ2lnQNMHVPbesEczzal4AGn6bM+l569eql8R0L4tZ9429T3RvH2iuEPp4qfs28tPJz3ntGx1r36S8CCCCAQC0K2CQxE8W1CMqhEEAAgTgTIMATZyeU4SBQGwJ+nYC2i9viJdDDBJp7RuPj2gI4Tb8Or79Q3YgsmJI5UssFyb1StmK5lC1fLv42bcRf/ot+d7+0fsdLWjXHsrJqTWdoBkwVLfMfd0jgmuukdOGvktBeJ4sj1vOpYvOtHko7rp/YxdYEKpk7xwk+JbRrr6XkqiglpHum9DlcUqrpY0Kbvaq18GqZoyavvilBXUze+mm/AjcXa02/mudcR/5jpdVSd/B10nVxertU1bLueUDELpFNJ82dtXeuvEbKVq2UsmXLxH7d7tdxeDXITItvgenTp0szXR9qv+y95IyDe8vqDRt0wB7J0PV2UkuTxKtr6ngaNhKPllVMywxJQAM6xVqKbdX6XCkNBKzgoMxbsVL+9MRDcnC7jvKnw/o4+1t2UMvGmXL/n86RU8fcLxOuvl5Kc8Pr+1SIarxmH/1M3zVpgow99xJppgHOhOxsrcGla/hoOaySlStk8owZMjXnJ3lhxI0Vu9XlDf421aVu7B47cdjfJLD6PPFrtqhH1y+jIYAAAggggAACCCCAAAJVCWyZwa3qWR5DAIF6LeCWbDOEWA/yMIFWr9/KzuAtUGOX2m4W1PH1PHCnD2vBliSdwKvrZoGjpO771/XL7NjxNUuqrs7LjnWErSMFrIRaXbbMzEyZoUGUPn36yN1aRvGpCy6V/VrvJUFdQ2eTBnOctnrtVl2w4E2ZBmBW6Zorf3v1P/LTsiVyeKe95Z/nXSKrcy1AtKUd07WbZvsky0G3/l2uO+lk6b9fT7EMIDvGyo15siYvV35Yslie/GSqnH94X/GvW++UiSsLlOlxl8qQcWOlf//+cvbZZ285aMSthg0bRtzb/s3fW9uuLv42uSUhtt8zno12AW+LFmIXGgIIIIAAAggggAACCCCwPQECPNvT4TkEEHAELNBjlzLNNLDSbXZtl1hpdTGBFitjp58IIIBANAocdNBBUqyZOieeeKJc/O8nJaCZOdb8WiKxo2bY7aVrsCTo7bUFm2S+ZtXkFhZUDCM1MUnevPpv0q5pMwl5rMZg+VpYuh59UWmJvDvrO7n9zfFyaMsW0iNUJMOfe0LWbi6WZqlJcnn37vKffkfLWl1j5y/vviMPvf+2HNi2vTROS5eP5+ZohlCZE9yZNEnLOtZx429THQNzeAQQQAABBBBAAAEEEECgHggQ4KkHJ5khIlBbApVLt9kvk61Fc8CHCbTaOvscBwEEEKh9gffff98J7tjfkRdeeEHGjRvnLCS8bPGvTsaNV7O8WmS3kksHDZLvv/9efv1yhjxwxGFy0wtPydwNGzXrxwI84WZhnoZJCXLrIYfIR4NOEZ8GiKw9dNThTghIF9SRBC111UQzcHya9TPl9JPl+zVr5YFvvpO5y5dK673ayMyZMyUrK6vO197ZHX+b5s+f74yffxCoSsDeH6znUJUMjyGAAAIIIIAAAgggEFsCBHhi63zRWwSiSiCyhFtkx2yizjJ96rLVJKi0OybQ6nKMHBsBBBCoDwI+DbrYZciQIc6lujFPnTpV+vXrJ+8sWixv/mmQJCckylmvvyG3HdpbvLqTRwM4FuRJ8Psku4lmAOkxPV57zB71aMAoKEUlJbIqN1eCwYC0btpUOrVsKQfqekATflkg7c8fIo0bN67u5Wvt8br+28Skfa2dKg6EAAIIIIAAAggggAACCES9AAGeqD9FdBCB2BOonOlTFyPYtGnTdg9b1xNo231xnkQAAQQQqHWBY489Vi677DJ56sknZVNZQB4+8Xi5qGcPeez7WfLXA/YXvwZ02jRtrIGdBCnWUm2FpWVi+T0h/Z+FeOxvU3JCgrRr3lyze7xSoGXaFq1aLSu1/NvYWT/J8sFVr7lTmwPhb1NtanIsBBBAAAEEEEAAAQQQQAABAjy8BxBAIO4EmECLu1PKgBBAAAEnQ+exxx4T+44fM2aMvDF/gWQmJznr6XylgZrnTjxOM3NCsmjtSmctHa9HM3jKs3pcPivplpTgl0xdcyclKVFK9f6FUz6Se+67T5pqRk9dtt35t8myeGbPnl2Xw+HYMSzwzjvvOL3v2rVrDI+CriOAAAIIIIAAAggggIAJWEULGgIIIBA3ArtzAi1u0BgIAgggECMCloXz8MMPy6JFi+Ta4cNlVWGRZKU3kB6du8jxr78lx73wiqwsKJD05BTp0KK5dGrVQjq0DF866nWX1tnSStfY2VCQL49+8aX0n/iWXHTFFXLttdfWqcDu/ts0SNcsssY6PHV6WmP+4GeccUbMj4EBIIAAAggggAACCCBQ3wXI4Knv7wDGj0AcCezuCbQ4omMoCCCAQMwIWFZO27Zt5e6775YTTzxRTuh3vIw550K5+eQz5MKnxsrpb78rGVqKrU2DDBm8d2c5olUrp0SbDbBMM3ae+vFn+d/SpVqarUgeeeQRGTp0qLMGUF0B7Mm/TZapMWzYsLoaGseNUYHJkyfHaM/pNgIIIIAAAggggAACCFQWIMBTWYT7CCAQkwJ7cgItJsHoNAIIIBAHAkceeaScctqpcurD98tnt4yUL2+7Wz6dm6P375NAk6Zy8+dfire8TJsFhgIa4AnpZcSIEXL55ZdLdnZ2nSqkpKTU6fGrO7iVaKNMW3U69ftxtzwb2Tv1+33A6BFAAAEEEEAAAQTiR4AAT/ycS0aCQL0VILhTb089A0cAgXou4PV65cEHH5ROHTpIbmG+ZKVlyBF77+OozJo1ywnmrFmzRuxigZ3mzZs7a+0kJibGvZytr2Lr8NiE/oABA+J+vAwQAQQQQAABBBBAAAEEEKiPAqzBUx/POmNGII4ECO7E0clkKAgggMBOCLRp00baaYDnzomvOXv7PF75y6GHy8cffyxJSUnSunVr6dmzpxx44IFOxk59CO4YhJuhQTmunXhTxekuFuyz94O9N9z3R5wOlWEhgAACCCCAAAIIIFBvBAjw1JtTzUARiD8Bgjvxd04ZEQIIILCjApbFM3LkSPl64a8Vu57d58iK2/X5hjuJ75blqs8WjF2c4I45uO8LTBBAAAEEEEAAAQQQQCD2BQjwxP45ZAQI1EsBW9dgT61tUC/BGTQCCCAQxQJHH320JPp9FT3MSE6Wvn37VtyvrzdsIt/W4rGsjfnz59dXBsatAm6Qj+AObwcEEEAAAQQQQAABBOJLgABPfJ1PRoNAvRHw+1lCrN6cbAaKAAII/I5AVlaWDDzqGBHN5hEJyYJNeZKYFP/r7PwOi/P0oEGDnGt3gr8m+7BNfAm4pdlsVAR44uvcMhoEEEAAAQQQQAABBJgh5T2AAAIIIIAAAgggENMCCQkJcuszT4kEgxIKhWSI9xwRXYuHJk4Gj03qT5gwQUaPHi3Dhg2DpR4JWOaWuw7TiBEj6tHIGSoCCCCAAAIIIIAAAvVDgP/nWz/Oc1yN8oILLpC2bdvG1ZgYDAIIIIAAAgjsooDHI+LziccyPJ1Mnl08XhztbgEeu9hkP5k8cXRiazAUC+pZs/Nv5fpoCCCAAAIIIIAAAgggEF8CZPDE1/mM+9FkZ2eLXWgIIIAAAggggAACNRewCf6cnJyKbI4BAwbUfGe2jDmByGCeG+CLuUHQYQQQQAABBBBAAAEEEPhdATJ4fpeIDRBAAAEEEEAAAQQQiH0BK9Flk/1WssvN7Ij9UTGCygIW3LHza9cEdyrrcB8BBBBAAAEEEEAAgfgSIIMnvs4no0EAAQQQQAABBBBAoFoBm/C3ZmvyXHnlldK/f38hm6darph7wkrwuWvuENyJudNHhxFAAAEEEEAAAQQQ2GEBAjw7TMYOCCCAAAIIIIAAAgjErkBkkMcNBthoCPTE7jl1S7LZtTXL1mLNndg9n/QcAQQQQAABBBBAAIGaChDgqakU2yGAAAIIIIAAAgggECcCFuTp2rWrsy6PZfO4rUuXLtK5c2f3LtdRLlA5sEPWTpSfMLqHAAIIIIAAAggggEAtCxDgqWVQDocAAggggAACCCCAQCwIWIaHm+WRk5PjlPZyM3qsdJs1Aj7RdSbdDB0rxebeth7aeRw0aFDF+YyuXtMbBBBAAAEEEEAAAQQQqCsBAjx1Jctxd1rg/PPP3+l92REBBBBAAAEEEEBgxwQiS7bZnpbR4wZ63Gt7nMweU9j9LTKQE/nqFtSxLCy7uIG6yOe5jQACCCCAAAIIIIAAAvEvQIAn/s9xTI2wV69eMdVfOosAAggggAACCMSLgBvosevZs2c75dtsbJbdY80eo+1+gcrBG8vUsVb58d3fM14RAQQQQAABBBBAAAEE9rQAAZ49fQZ4fQQQQAABBBBAAAEEokzAggcEEKLspNAdBBBAAAEEEEAAAQQQQKCSgLfSfe4igAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAghEuQABnig/QXQPAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEKgsQICnsgj3EUAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIEoF6jTNXjmzZsndqEhYALz588HAgEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBGpBoE4CPF27dnW6Nnny5FroIodAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBCIFKiTAM++++4rI0aMiHwdbiNQIWDvDxoCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgjsvECdBHisO0zi7/xJYU8EEEAAAQQQQAABBBBAAAEEEEAg2gVCoVC0d5H+IYAAAgggENcC3rgeHYNDAAEEEEAAAQQQQAABBBCoE4FOnTo5x/3111/r5PgcFAEEolfA/dy73wPR21N6hgACCCCAQHwLEOCJ7/PL6BBAAAEEEEAAAQQQQACBOhFo166dc9wFCxbUyfE5KAIIRK+A+7l3vweit6f0DAEEEEAAgfgWIMAT3+eX0SGAAAIIIIAAAggggAACdSLQpk0b57juL/nr5EU4KAIIRKWA+7l3vweispN0CgEEEEAAgXogQICnHpxkhogAAggggAACCCCAAAII1LZA69atpWvXrvLVV1/JwoULa/vwHA8BBKJUwD7v9rm3tZfte4CGAAIIIIAAAntOgADPnrPnlRFAAAEEEEAAAQQQQACBmBY444wzpKioSF599dWYHgedRwCBmgvY590+9/b5pyGAAAIIIIDAnhUgwLNn/Xl1BBBAAAEEEEAAAQQQQCBmBewX/DbJ+8MPP8jEiRNjdhx0HAEEaiZgn3P7vNvn3jL4aAgggAACCCCwZwUI8OxZf14dAQQQQAABBBBAAAEEEIhpAXeid/z48TJ9+vSYHgudRwCB6gXs822fczewW/2WPIMAAggggAACu0uAAM/ukuZ1EEAAAQQQQAABBBBAAIE4Fbj44oulffv2MnbsWHnnnXfidJQMC4H6K2Cfa/t82+f8oosuqr8QjBwBBBBAAIEoE/CEtEVZn+gOAggggAACCCCAAAIIIIBAjAmsX7/emQCePXu2DBgwQAYPHhxjI6C7CCBQlcALL7zgBG6tJNuVV14pWVlZVW3GYwgggAACCCCwBwQI8OwBdF4SAQQQQAABBBBAAAEEEIhHgYKCAifI891330mfPn1k0KBB0qpVq3gcKmNCIO4Fli9fLq+//rp8/vnn0rNnTxk6dKikpaXF/bgZIAIIIIAAArEkQIAnls4WfUUAAQQQQAABBBBAAAEEolygrKxMxo0bJ5988omkp6c72TwDBw4Un88X5T2newggYAKBQEAmTZrkZO3k5+fLUUcd5ZRl8/v9ACGAAAIIIIBAlAkQ4ImyE0J3EEAAAQQQQAABBBBAAIF4EPjiiy/kjTfekCVLlkjHjh3Fgjy9e/eOh6ExBgTiVuDLL790gjsLFiyQNm3ayGmnnSaHHXZY3I6XgSGAAAIIIBDrAgR4Yv0M0n8EEEAAAQQQQAABBBBAIEoFrGTbxIkTZfLkyU4PrWxb//79pUOHDlHaY7qFQP0U+PXXX53PqZVjs2af09NPP52SbPXz7cCoEUAAAQRiSIAATwydLLqKAAIIIIAAAggggAACCMSiwKxZs5xsnjlz5jjdt4yAI444wlnXIxbHQ58RiBcBWy9r+vTpYhl31vbZZx8na2f//fePlyEyDgQQQAABBOJagABPXJ9eBocAAggggAACCCCAAAIIRI+Arcvz6aefSk5OjtOpfffd1wn0WLCH9T2i5zzRk/gWsHWyLKhjl9mzZzuD7dq1q/Tt29dZbye+R8/oEEAAAQQQiC8BAjzxdT4ZDQIIIIAAAggggAACCCAQ9QIzZ850Aj3ffPON09dWrVrJkUce6azR06JFi6jvPx1EIBYFVq5cKbbGzrRp02T58uXOEA466CAnsNOrV69YHBJ9RgABBBBAoN4LEOCp928BABBAAAEEEEAAAQQQQACBPSPw888/i2X1WCaB23r06CEHHHCA2DXBHleFawR2TsCCOj/88IN8//33zrV7FMuaO+qoo6Rbt27uQ1wjgAACCCCAQAwKEOCJwZNGlxFAAAEEEEAAAQQQQACBeBKwBd6//fZbsfVAFi5cWDE0gj0VFNxAoMYC1QV12rdv76x7deCBB0qHDh1qfDw2RAABBBBAAIHoFSDAE73nhp4hgAACCCCAAAIIIIAAAvVOYO7cuU6gx4I9S5YsqRi/BXts4XdbBN4mqmkIILBFwAKjc+bMkVmzZm2VqdOmTRsnqNOzZ0/Ze++9t+zALQQQQAABBBCICwECPHFxGhkEAggggAACCCCAAAIIIBB/Aj/99JNTWsqCPStWrKgYYHZ2thPo2XfffZ1ybikpKRXPcQOB+iBQVFTkfDZmz57tBHaWLVtWMeyWLVs6QR0rdbjffvtVPM4NBBBAAAEEEIg/AQI88XdOGRECCCCAAAIIIIAAAgggEHcCto5ITk6Oc5k/f37F+LxerxPs6d69uxPsadu2bcVz3EAgngQWL17sBHV+/PFHJ6gTDAYrhte5c2fp2rWrc7FsNxoCCCCAAAII1A8BAjz14zwzSgQQQAABBBBAAAEEEEAgbgTWrl0rNsntBnzWrVtXMbbMzEyxzJ6OHTs6F1trxOfzVTzPDQRiQSAQCIitTbVgwQLnYpk6ubm5FV1v3LhxRUDHgptNmjSpeI4bCCCAAAIIIFB/BAjw1J9zzUgRQAABBBBAAAEEEEAAgbgUWLRokRPwsZJuFvip3Nq1a1cR8LHAj61LQkMgmgRsvSk3mGPX9p6u3CyQYyXX7Nre0zQEEEAAAQQQQIAAD+8BBBBAAAEEEEAAAQQQQACBuBJwMx/s2i42eR7ZEhIStgr4WNCnadOmkZtwG4E6E1izZs1WwRwL6JSWlm71ehaEtOwzu9j7065pCCCAAAIIIIBAZQECPJVFuI8AAggggAACCCCAAAIIIBBXAjZ5bpPobsDHrleuXLnVGDMyMiqCPtnZ2eJebI0fGgI7I2Br5Cxbtqzi4mbobNq0aavDtWjRoiKY4wZ0LAhJQwABBBBAAAEEfk+AAM/vCfE8AggggAACCCCAAAIIIIBA3AkUFBRsFfCxyff169dvM06bfHeDPZHXSUlJ22zLA/VToLi4uCKIExnQqRxENJ2srKyKjBw3QyctLa1+wjFqBBBAAAEEENhlAQI8u0zIARBAAAEEEEAAAQQQQAABBOJBYMOGDc7aJ5GT9Ha7sLBwm+HZovatWrXaJviTnp6+zbY8EB8C+fn52wRyli9fLmvXrt1mgKmpqdu8N2zdnEaNGm2zLQ8ggAACCCCAAAI7K0CAZ2fl2A8BBBBAAAEEEEAAAQQQQKBeCFjgp3LQx+7n5eVtM/6GDRs6gZ/MzExnMt8m9CtfCAJtw7bHH7DgjZ3n6i7bO9+RmV3ubQI5e/yU0gEEEEAAAQTqhQABnnpxmhkkAggggAACCCCAAAIIIIBAbQtYUMAyOCoHf6rK6Ih8bVtfpXLQx71fOTAUuR+3d1wgMmCTm5tbbQDH1mnaXrOMLTd4415bBhfBuu2p8RwCCCCAAAII1LUAAZ66Fub4CCCAAAIIIIAAAggggAAC9UqgrKysykCCZfxEBhzsdiAQ2K6Nrc9i5b7sOvJ2VY9FPm+3ExMTt3vsWHiypKREbL0ku1ipvO3drur57Y3R5/NtE2izDCw32BZ57ff7t3conkMAAQQQQAABBPaIAAGePcLOiyKAAAIIIIAAAggggAACCCAgsmnTpm2CPpFBoMigRVFR0Q6RWVAiMjCUlJQkXq+3Vi/BYFBq81JcXLxVIMeCZTvSUlJSKsZsY68uYGPBm4yMjB05NNsigAACCCCAAAJRJ0CAJ+pOCR1CAAEEEEAAAQQQQAABBBBAYFsBC6REBnyqymapnOkSuc3vZQtt+4q7/xHLqonMTooMULm37bq6bSyARUMAAQQQQAABBOqLAAGe+nKmGScCCCCAAAIIIIAAAggggEC9FrAAj2XE1ObFsoRq82IBHhoCCCCAAAIIIIBAzQQI8NTMia0QQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgagRIHc5ak4FHUEAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEaiZAgKdmTmyFAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCESNAAGeqDkVdAQBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQqJkAAZ6aObEVAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIBA1AgR4ouZU0BEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAoGYCBHhq5sRWCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggEDUCBDgiZpTQUcQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAgZoJEOCpmRNbIYAAAggggAACCCCAAAIIIIAAAggggAACCCCAAAJRI0CAJ2pOBR1BAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBGomQICnZk5shQACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAghEjQABnqg5FXQEAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEKiZAAGemjmxFQIIIIAAAggggAACCCCAAAIIIIAAAggggAACCCAQNQIEeKLmVNARBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQKBmAgR4aubEVggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIIBA1AgQ4ImaU0FHEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAIGaCRDgqZkTWyGAAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACUSNAgCdqTgUdQQABBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQRqJkCAp2ZObIUAAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIRI0AAZ6oORV0BAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBComQABnpo5sRUCCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAgggEDUCBHii5lTQEQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEEECgZgIEeGrmxFYIIIAAAggggAACCCCAAAIIIIAAAggggAACCCCAQNQIEOCJmlNBRxBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQACBmgkQ4KmZE1shgAACCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAlEjQIAnak4FHUEAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAAAEEaiZAgKdmTmyFAAIIIIAAAggggAACCCCAAAIIIIAAAggggAACCESNAAGeqDkVdAQBBBBAAAEEEEAAAQQQQAABBBBAAAEEEEAAAQQQqJkAAZ6aObEVAggggAACCCCAAAIIIIAAAggggAACCCCAAAIIIBA1AgR4ouZU0BEEEEAAAQQQQAABBBBAAAEEEEAAAQQQQAABBBBAoGYCBHhq5sRWCCCAAAIIIIAAAggggAACCCCAAAIIIIAAAggggEDUCPw/pPp0xAh2Y0cAAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "CjYEhHD8VDua"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cL4x2EBVVDfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain 및 필요한 라이브러리 설치"
      ],
      "metadata": {
        "id": "4hniXcElvMif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain unstructured[all-docs] pydantic lxml langchain_openai langchain-community chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FSTi00anFGqf",
        "outputId": "bab483de-0f86-4ad7-e322-d1f3895a0e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured[all-docs]\n",
            "  Downloading unstructured-0.14.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.7.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.59-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (5.2.0)\n",
            "Collecting filetype (from unstructured[all-docs])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from unstructured[all-docs])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.12.3)\n",
            "Collecting emoji (from unstructured[all-docs])\n",
            "  Downloading emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-iso639 (from unstructured[all-docs])\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from unstructured[all-docs])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured[all-docs])\n",
            "  Downloading rapidfuzz-3.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from unstructured[all-docs])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (4.11.0)\n",
            "Collecting unstructured-client (from unstructured[all-docs])\n",
            "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (1.14.1)\n",
            "Collecting pikepdf (from unstructured[all-docs])\n",
            "  Downloading pikepdf-8.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six (from unstructured[all-docs])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx<=0.6.23 (from unstructured[all-docs])\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.3)\n",
            "Collecting pdf2image (from unstructured[all-docs])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Collecting pypandoc (from unstructured[all-docs])\n",
            "  Downloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
            "Collecting onnx (from unstructured[all-docs])\n",
            "  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting google-cloud-vision (from unstructured[all-docs])\n",
            "  Downloading google_cloud_vision-3.7.2-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.6/459.6 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-docx (from unstructured[all-docs])\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf (from unstructured[all-docs])\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.3)\n",
            "Collecting pillow-heif (from unstructured[all-docs])\n",
            "  Downloading pillow_heif-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.6)\n",
            "Collecting msg-parser (from unstructured[all-docs])\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured-inference==0.7.31 (from unstructured[all-docs])\n",
            "  Downloading unstructured_inference-0.7.31-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (3.1.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured[all-docs]) (2.0.1)\n",
            "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]) (0.20.3)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]) (4.8.0.76)\n",
            "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.31->unstructured[all-docs]) (4.40.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.18.2)\n",
            "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
            "  Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m862.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx<=0.6.23->unstructured[all-docs]) (9.4.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.23->unstructured[all-docs])\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured[all-docs]) (2.5)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.11.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[all-docs]) (3.20.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured[all-docs]) (1.16.0)\n",
            "Collecting olefile>=0.46 (from msg-parser->unstructured[all-docs])\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[all-docs]) (42.0.7)\n",
            "Collecting Pillow>=3.3.2 (from python-pptx<=0.6.23->unstructured[all-docs])\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pikepdf->unstructured[all-docs])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Collecting mypy-extensions>=1.0.0 (from unstructured-client->unstructured[all-docs])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (1.16.0)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->unstructured[all-docs])\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.63.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (4.9)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]) (3.14.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->unstructured-inference==0.7.31->unstructured[all-docs]) (0.4.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unstructured-inference==0.7.31->unstructured[all-docs]) (2023.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (1.11.4)\n",
            "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (0.17.1+cu121)\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[all-docs]) (0.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.7.31->unstructured[all-docs]) (1.3.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.31->unstructured[all-docs]) (3.1.2)\n",
            "Building wheels for collected packages: langdetect, iopath, antlr4-python3-runtime\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=557872c16645d710badc6de82ac2d41f4bdab576b921387d40e3c64ae2feeb1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=23a9a7e4b1eb4d768f91f1a35ec09e18e6e36c1891456a19f912bce822b52793\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=377652eb3ff4b3393bb58404d72cb7be9f3ac06c1177616d274782e7cb5fda9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built langdetect iopath antlr4-python3-runtime\n",
            "Installing collected packages: filetype, antlr4-python3-runtime, XlsxWriter, rapidfuzz, python-multipart, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, portalocker, Pillow, packaging, orjson, ordered-set, onnx, omegaconf, olefile, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, langdetect, jsonpointer, jsonpath-python, humanfriendly, h11, emoji, Deprecated, backoff, unstructured.pytesseract, typing-inspect, tiktoken, python-pptx, pytesseract, pillow-heif, pikepdf, pdf2image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, msg-parser, marshmallow, jsonpatch, iopath, httpcore, deepdiff, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, langsmith, httpx, dataclasses-json, unstructured-client, pdfplumber, openai, langchain-core, unstructured, layoutparser, langchain-text-splitters, langchain_openai, google-cloud-vision, timm, langchain, effdet, unstructured-inference\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Deprecated-1.2.14 Pillow-10.3.0 XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.6 deepdiff-7.0.1 effdet-0.4.1 emoji-2.11.1 filetype-1.2.0 google-cloud-vision-3.7.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 humanfriendly-10.0 iopath-0.1.10 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langchain_openai-0.1.7 langdetect-1.0.9 langsmith-0.1.59 layoutparser-0.3.4 marshmallow-3.21.2 msg-parser-1.2.0 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 olefile-0.47 omegaconf-2.3.0 onnx-1.16.0 onnxruntime-1.18.0 openai-1.30.1 ordered-set-4.1.0 orjson-3.10.3 packaging-23.2 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.0 pikepdf-8.15.1 pillow-heif-0.16.0 portalocker-2.8.2 pypandoc-1.13 pypdf-4.2.0 pypdfium2-4.30.0 pytesseract-0.3.10 python-docx-1.1.2 python-iso639-2024.4.27 python-magic-0.4.27 python-multipart-0.0.9 python-pptx-0.6.23 rapidfuzz-3.9.0 tiktoken-0.7.0 timm-1.0.3 typing-inspect-0.9.0 unstructured-0.14.0 unstructured-client-0.22.0 unstructured-inference-0.7.31 unstructured.pytesseract-0.3.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "google",
                  "pydevd_plugins"
                ]
              },
              "id": "d581bf5707fc4405a29bd75d2c6b32cb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpsHmYrL_dVY",
        "outputId": "01e167b1-96f5-4a70-d91d-0f2a80c7dbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 1s (361 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121965 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh0zQ4xUCTzF",
        "outputId": "68551f68-9215-4403-bb1b-327109b7924a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3CfBijwAUDs",
        "outputId": "bdc287c4-a1df-4005-e139-440fcb428ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unstructured에서 PDF 분할을 위해 다음을 사용합니다:\n",
        "\n",
        "*   OCR(광학 문자 인식)을 위한 Tesseract\n",
        "*   PDF 렌더링 및 처리를 위한 Poppler"
      ],
      "metadata": {
        "id": "2Q48UnzmgfuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Key 설정"
      ],
      "metadata": {
        "id": "2J_c-XcBVGSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_KEY = \"여러분의_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "nnq2JVEwLPZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "M-tizIw98MuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF 표(Tables)와 텍스트(text) 분할"
      ],
      "metadata": {
        "id": "l91YMcnp8Wvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLaVA 논문에 적용하기\n",
        "\n",
        "Unstructured의 **partition_pdf**를 사용하여 레이아웃 모델을 이용해 PDF 문서를 세분화합니다.\n",
        "\n",
        "이 레이아웃 모델을 통해 PDF에서 표와 같은 요소를 추출할 수 있습니다.\n",
        "\n",
        "또한, Unstructured의 청킹(chunking)을 사용할 수 있으며, 이는:\n",
        "\n",
        "*   문서의 섹션(예: introduction 등)을 식별하려고 시도한 후,\n",
        "*   섹션을 유지하면서 사용자 정의 청크 크기를 준수하는 텍스트 블록을 생성합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "l-LE95lPlguW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   LLaVA Paper: https://arxiv.org/pdf/2304.08485.pdf\n"
      ],
      "metadata": {
        "id": "GgSCO0k3vfA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = './'"
      ],
      "metadata": {
        "id": "WPFDk8ah-D-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=path + \"LLaVA.pdf\",\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "9606b2062ac142a489ec2b04a303e119",
            "e4b19bafa7124b5b992d4b05b13d5aec",
            "8a2d2aa5da9a41e0aa46bd251ec2e824",
            "15c8ef041c214a69b73627a882c4d778",
            "abd29f1409ec491c9a10665cf8002105",
            "2970027c94354189ab7de6b5a22500db",
            "8b396514f7714d4f8b805c08f2cd7eb2",
            "d4e136b9d61a4d4db133ea93967121cd",
            "9bde184577cf4ce9911f680cb455769a",
            "b22e04d6542548d792ff0ebf39c0cccc",
            "57b27d4e421d49d0bb01c7c9d5689f97",
            "659de5be65054832a2dbf780a674317f",
            "603db6d78b4c4d35b17007c86edabd4a",
            "1082b235ef4a4366835712a5f752a576",
            "25ca5f8e228049518123b629acde9f9c",
            "4815789dfea3435793441cfcc6171157",
            "a230aa17cc3d4d778443ec55b8d18e34",
            "8f5488ff5af34e6f9f9c17329a5523fa",
            "6b15adb50e1e4bb88b0ee79a4ee5b9a1",
            "fd3c2f197f4f4e7dbec5774f3a000af2",
            "66d58fe7703548969151f5e9cda05159",
            "ee7b026ddf134cb99afd84c6a29bcf9b",
            "5496e9b72b574313a8c076756176c86c",
            "bfea25ce1fdc445ea6a333bf83cce607",
            "6d742190ffd846f4ac3762d8df15de3c",
            "71259ddb61084f988932a2030dfc7363",
            "8cab9cc08ce6425babd9c7425c97a20b",
            "8d6646f615384b1ca055386116b1a3ed",
            "393fda27fc604ba1b55ba84c3479722c",
            "0a38242828114fbfad1d217f55fb89ff",
            "7dfc5c3242ff4a26ba31a091478cb7c4",
            "be40d098058b44a7979dc83286b91519",
            "2c401367e4b047a597d4505fc90bc6fe"
          ]
        },
        "id": "qqwmCeUS8L8k",
        "outputId": "b5056878-af12-4830-ed73-8f3d6ff9c094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9606b2062ac142a489ec2b04a303e119"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "659de5be65054832a2dbf780a674317f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5496e9b72b574313a8c076756176c86c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "partition_pdf로 추출된 요소를 확인할 수 있습니다.\n",
        "\n",
        "CompositeElement는 집계된 청크입니다."
      ],
      "metadata": {
        "id": "UOcuDF2xon2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbJGVKqI8L6B",
        "outputId": "6e9a17ed-ffd9-46fe-f634-c383392b5ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 31,\n",
              " \"<class 'unstructured.documents.elements.Table'>\": 4}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q8IwTB3FYea",
        "outputId": "5096c690-4bd6-4f7f-8582-ca2e8e87e3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FCAUgNDEhfZ",
        "outputId": "c55ea439-9110-4cd5-a5d5-21deabaa5f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Element(type='table', text='Conversation Detail description Complex reasoning All Full data Detail + Complex Conv + 5% Detail + 10% Complex Conversation No Instruction Tuning 83.1 81.5 (-1.6) 81.0 (-2.1) 76.5 (-6.6) 22.0 (-61.1) 75.3 73.3 (-2.0) 68.4 (-7.1) 59.8 (-16.2) 24.0 (-51.3) 96.5 90.8 (-5.7) 91.5 (-5.0) 84.9 (-12.4) 18.5 (-78.0) 85.1 81.9 (-3.2) 80.5 (-4.4) 73.8 (-11.3) 21.5 (-63.6)'),\n",
              " Element(type='table', text='Conversation Detail description Complex reasoning All OpenFlamingo [5] BLIP-2 [28] LLaVA LLaVA† 19.3 ± 0.5 54.6 ± 1.4 57.3 ± 1.9 58.8 ± 0.6 19.0 ± 0.5 29.1 ± 1.2 52.5 ± 6.3 49.2 ± 0.8 19.1 ± 0.7 32.9 ± 0.7 81.7 ± 1.8 81.4 ± 0.3 19.1 ± 0.4 38.1 ± 1.0 67.3 ± 2.0 66.7 ± 0.3'),\n",
              " Element(type='table', text='Method NAT Subject SOC LAN Context Modality IMG TXT NO Grade G1-6 G7-12 Average Representative & SoTA methods with numbers reported in the literature 90.23 Human [34] 74.64 GPT-3.5 [34] 75.44 GPT-3.5 w/ CoT [34] 84.37 LLaMA-Adapter [59] 87.52 MM-CoTBase [61] MM-CoTLarge [61] 95.91 Results with our own experiment runs GPT-4† LLaVA LLaVA+GPT-4† (complement) LLaVA+GPT-4† (judge) 84.97 69.74 70.87 88.30 77.17 82.00 89.60 74.44 74.68 83.72 87.88 95.26 87.48 76.00 78.09 84.36 85.82 90.82 73.45 95.95 95.50 96.74 84.06 90.36 90.36 91.56 87.36 88.00 88.55 91.09 81.87 89.49 89.05 90.62 87.50 67.28 67.43 80.32 82.90 88.80 70.75 88.00 87.80 88.99 88.10 77.42 79.93 86.90 86.83 92.89 90.73 90.66 91.08 93.52 91.59 76.80 78.23 85.83 84.65 92.44 84.69 90.93 92.22 92.73 82.42 68.89 69.68 84.05 85.37 90.31 79.10 90.90 88.73 92.16 88.40 73.97 75.17 85.19 84.91 91.68 82.69 90.92 90.97 92.53'),\n",
              " Element(type='table', text='Visual features Last Before 90.92 - 85.81 (-5.11) 89.84 (-1.08) Best variant Predict answer first Training from scratch 7B model size 89.96 (-0.96) 89.77 (-1.15) - -')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBOESL3qEUL-",
        "outputId": "822b8e16-760d-4faa-d29f-89f8e9511b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Element(type='text', text='3 2 0 2 c e D 1 1\\n\\n] V C . s c [ 2 v 5 8 4 8 0 . 4 0 3 2 : v i X r a\\n\\nVisual Instruction Tuning\\n\\nHaotian Liu1∗, Chunyuan Li2∗, Qingyang Wu3, Yong Jae Lee1\\n\\n1University of Wisconsin–Madison 2Microsoft Research 3Columbia University https://llava-vl.github.io\\n\\nAbstract\\n\\nInstruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we in- troduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general- purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demon- strates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% rela- tive score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.'),\n",
              " Element(type='text', text='1 Introduction\\n\\nHumans interact with the world through many channels such as vision and language, as each individual channel has a unique advantage in representing and communicating certain concepts, and thus facilitates a better understanding of the world. One of the core aspirations in artificial intelligence is to develop a general-purpose assistant that can effectively follow multi-modal vision-and-language instructions, aligned with human intent to complete various real-world tasks in the wild [4, 27, 26].\\n\\nTo this end, the community has witnessed an emergent interest in developing language-augmented foundation vision models [27, 16], with strong capabilities in open-world visual understanding such as classification [40, 21, 57, 54, 39], detection [29, 62, 33], segmentation [25, 63, 58] and captioning [50, 28], as well as visual generation and editing [42, 43, 56, 15, 44, 30]. We refer readers to the Computer Vision in the Wild reading list for a more up-to-date literature compilation [12]. In this line of work, each task is solved independently by one single large vision model, with the task instruction implicitly considered in the model design. Further, language is only utilized to describe the image content. While this allows language to play an important role in mapping visual signals to language semantics—a common channel for human communication, it leads to models that usually have a fixed interface with limited interactivity and adaptability to the user’s instructions.\\n\\nLarge language models (LLM), on the other hand, have shown that language can play a wider role: a universal interface for a general-purpose assistant, where various task instructions can be explicitly represented in language and guide the end-to-end trained neural assistant to switch to the task of interest to solve it. For example, the recent success of ChatGPT [35] and GPT-4 [36] have demonstrated the power of aligned LLMs in following human instructions, and have stimulated tremendous interest in developing open-source LLMs. Among them, LLaMA [49] is an open- source LLM that matches the performance of GPT-3. Alpaca [48], Vicuna [9], GPT-4-LLM [38]\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\\n\\nutilize various machine-generated high-quality instruction-following samples to improve the LLM’s alignment ability, reporting impressive performance compared with proprietary LLMs. Importantly, this line of work is text-only.\\n\\nIn this paper, we present visual instruction-tuning, the first attempt to extend instruction-tuning to the language-image multimodal space, to pave the way towards building a general-purpose visual assistant. In particular, our paper makes the following contributions:\\n\\nMultimodal instruction-following data. One key challenge is the lack of vision-language instruction-following data. We present a data reformation perspective and pipeline to convert image-text pairs into an appropriate instruction-following format, using ChatGPT/GPT-4. • Large multimodal models. We develop a large multimodal model (LMM), by connecting the open-set visual encoder of CLIP [40] with the language decoder Vicuna [9], and fine-tuning end-to-end on our generated instructional vision-language data. Our empirical study validates the effectiveness of using generated data for LMM instruction-tuning, and suggests practical tips for building a general-purpose instruction-following visual agent. When ensembled with GPT-4, our approach achieves SoTA on the Science QA [34] multimodal reasoning dataset. • Multimodal instruction-following benchmark. We present LLaVA-Bench with two challenging benchmarks, with a diverse selection of paired images, instructions and detailed annotations. • Open-source. We release the following assets to the public: the generated multimodal instruction'),\n",
              " Element(type='text', text='data, the codebase, the model checkpoints, and a visual chat demo.\\n\\n2 Related Work\\n\\nMultimodal Instruction-following Agents. In computer vision, existing works that build instruction- following agents can be broadly categorized into two classes: (i) End-to-end trained models, which are separately explored for each specific research topic. For example, the vision-language navigation task [3, 19] and Habitat [47] require the embodied AI agent to follow natural language instructions and take a sequence of actions to complete goals in visual environments. In the image editing domain, given an input image and a written instruction that tells the agent what to do, InstructPix2Pix [6] edits images by following the human instructions. (ii) A system that coordinates various models via LangChain [1] / LLMs [35], such as Visual ChatGPT [53], X-GPT [63], MM-REACT [55], VisProg [18], and ViperGPT [46]. While sharing the same goal in building instruction-following agents, we focus on developing an end-to-end trained language-vision multimodal model for multiple tasks.\\n\\nInstruction Tuning. In the natural language processing (NLP) community, to enable LLMs such as GPT-3 [7], T5 [41], PaLM [10], and OPT [60] to follow natural language instructions and complete real-world tasks, researchers have explored methods for LLM instruction-tuning [37, 52, 51], leading to instruction-tuned counterparts such as InstructGPT [37]/ChatGPT [35], FLAN-T5 [11], FLAN-PaLM [11], and OPT-IML [22], respectively. It turns out that this simple approach can effectively improve the zero- and few-shot generalization abilities of LLMs. It is thus natural to borrow the idea from NLP to computer vision. More broadly, the teacher-student distillation ideas with foundation models have been studied in other topics such as image classification [14]. Flamingo [2] can be viewed as the GPT-3 moment in the multimodal domain, due to its strong performance on zero-shot task transfer and in-context-learning. Other LMMs trained on image- text pairs include BLIP-2 [28], FROMAGe [24], and KOSMOS-1 [20]. PaLM-E [13] is an LMM for embodied AI. Based on the recent “best” open-source LLM LLaMA, OpenFlamingo [5] and LLaMA-Adapter [59] are open-source efforts that enable LLaMA to use image inputs, paving the way to build open-source multimodal LLMs. While these models present promising task transfer generalization performance, they are not explicitly tuned with vision-language instruction data, and their performance in multimodal tasks usually falls short compared to language-only tasks. In this paper, we aim to fill this gap and study its effectiveness. Finally, note that visual instruction tuning is different from visual prompt tuning [23]: the former aims to improve the model’s instruction- following abilities, while the latter aims to improve the parameter-efficiency in model adaptation.'),\n",
              " Element(type='text', text='3 GPT-assisted Visual Instruction Data Generation\\n\\nThe community has witnessed a surge in the amount of public multimodal data such as image-text pairs, ranging from CC [8] to LAION [45]. However, when it comes to multimodal instruction-\\n\\n2\\n\\nContext type 1: Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: ...<omitted>\\n\\nResponse type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV) ...<omitted> Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another person in the middle, and the third person on the right side. They are all working together to pack their luggage into the SUV for a trip. ...<omitted> Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of fitting all their luggage into the black SUV. There are multiple suitcases and backpacks to be packed, which suggests that the group has a significant amount of belongings ...<omitted>\\n\\nTable 1: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT, and the bottom block shows the three types of responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\\n\\nfollowing data, the available amount is limited, partially because the process for creating such data is time-consuming and less well-defined when human crowd-scouring is considered. Inspired by the success of recent GPT models in text-annotation tasks [17], we propose to leverage ChatGPT/GPT-4 for multimodal instruction-following data collection, based on the widely existing image-pair data.\\n\\nFor an image Xv and its associated caption Xc, it is natural to create a set of questions Xq with the intent to instruct the assistant to describe the image content. We prompt GPT-4 to curate such a list of questions (see details in Appendix). Therefore, a simple way to expand an image-text pair to its instruction-following version is Human : Xq Xv<STOP> Assistant : Xc<STOP>. Though cheap to construct, this simple expanded version lacks diversity and in-depth reasoning in both the instructions and responses.\\n\\nTo mitigate this issue, we leverage language-only GPT-4 or ChatGPT as the strong teacher (both accept only text as input), to create instruction-following data involving visual content. Specifically, in order to encode an image into its visual features to prompt a text-only GPT, we use two types of symbolic representations: (i) Captions typically describe the visual scene from various perspectives; (ii) Bounding boxes usually localize the objects in the scene, and each box encodes the object concept and its spatial location. One example is shown in the top block of Table 14.\\n\\nThis symbolic representation allows us to encode the image as an LLM-recognizable sequence. We use COCO images [31] and generate three types of instruction-following data. One example per type is shown in the bottom block of Table 14. For each type, we first manually design a few examples. They are the only human annotations we have during data collection, and are used as seed examples in in-context-learning to query GPT-4.'),\n",
              " Element(type='text', text='• Conversation. We design a conversation between the assistant and a person asking questions about this photo. The answers are in a tone as if the assistant is seeing the image and answering the question. A diverse set of questions are asked about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects. Only questions that have definite answers are considered. Please see Appendix for the detailed prompt.\\n\\n• Detailed description. To include a rich and comprehensive description for an image, we create a list of questions with such an intent. We prompt GPT-4 then curate the list (see detailed prompts\\n\\n3\\n\\nand curation process in Appendix). For each image, we randomly sample one question from the list to ask GPT-4 to generate the detailed description.\\n\\n• Complex reasoning. The above two types focus on the visual content itself, based on which we further create in-depth reasoning questions. The answers typically require a step-by-step reasoning process by following rigorous logic.\\n\\nWe collect 158K unique language-image instruction-following samples in total, including 58K in conversations, 23K in detailed description, and 77k in complex reasoning, respectively. We ablated the use of ChatGPT and GPT-4 in our early experiments, and found that GPT-4 consistently provides higher quality instruction-following data, such as spatial reasoning.\\n\\n4 Visual Instruction Tuning\\n\\n4.1 Architecture\\n\\nThe primary goal is to effectively leverage the capabilities of both the pre-trained LLM and visual model. The network archtecture is illustrated in Figure 1. We choose Vicuna [9] as our LLM fϕ(·) parameterized by ϕ, as it has the best instruction following capabilities in language tasks among publicly available checkpoints [48, 9, 38].\\n\\nLanguage Response X, a a a Language Model fo CAG &é4 H, AH, xX, Image Xq Language Instruction Projection W Zy Vision Encoder\\n\\nFigure 1: LLaVA network architecture. For an input image Xv, we consider the pre-trained CLIP visual encoder ViT-L/14 [40], which provides the visual feature Zv = g(Xv). The grid features before and after the last Transformer layer are considered in our experiments. We consider a simple linear layer to connect image features into the word embedding space. Specifically, we apply a trainable projection matrix W to convert Zv into language embedding tokens Hv, which have the same dimensionality as the word embedding space in the language model:\\n\\nHv = W · Zv, with Zv = g(Xv) (1) Thus, we have a sequence of visual tokens Hv. Note that our simple projection scheme is lightweight, which allows us to iterate data centric experiments quickly. More sophisticated schemes to con- nect the image and language representations can also be considered, such as gated cross-attention in Flamingo [2] and Q-former in BLIP-2 [28]. We leave exploring possibly more effective and sophisticated architecture designs for LLaVA as future work.'),\n",
              " Element(type='text', text='4.2 Training\\n\\nFor each image Xv, we generate multi-turn conversation data (X1 a ), where T is the total number of turns. We organize them as a sequence, by treating all answers as the assistant’s response, and the instruction Xt\\n\\n{ Randomly choose [Xj,Xv] or [Xy,X4], the first turn t = 1 t x Xi, the remaining turns t > 1 instruct — (2)\\n\\nXt\\n\\nThis leads to the unified format for the multimodal instruction-following sequence illustrated in Table 2. We perform instruction-tuning of the LLM on the prediction tokens, using its original auto-regressive training objective.\\n\\nSpecifically, for a sequence of length L, we compute the probability of the target answers Xa by:\\n\\nL p(XalXv, Xinstruct) = [] po(wi|Xv, Xinstruct,<is Xa,<i); (3) i=1\\n\\n4\\n\\nXsystem-message <STOP> Human : X1 Human : X2 instruct <STOP> Assistant: X1 instruct <STOP> Assistant: X2 a <STOP> a <STOP> · · ·\\n\\nTable 2: The input sequence used to train the model. Only two conversation turns are illustrated here; in practice, the number of turns varies based on the instruction-following data. In our current implementation, we follow Vicuna-v0 [9] to set the system message Xsystem-message and we set <STOP> = ###. The model is trained to predict the assistant answers and where to stop, and thus only green sequence/tokens are used to compute the loss in the auto-regressive model.\\n\\nwhere θ is the trainable parameters, Xinstruct,<i and Xa,<i are the instruction and answer tokens in all turns before the current prediction token xi, respectively. Please see Table 2 for an illustration of the prediction tokens. For the conditionals in (3), we explicitly add Xv to emphasize the fact that the image is grounded for all answers, and we omit Xsystem-message and all previous <STOP> for better readability. For LLaVA model training, we consider a two-stage instruction-tuning procedure.\\n\\nStage 1: Pre-training for Feature Alignment. To strike a balance between concept coverage and training efficiency, we filter CC3M to 595K image-text pairs. Please see Appendix for details of the filtering process. These pairs are converted to the instruction-following data using the naive expansion method describe in Section 3. Each sample can be treated as a single-turn conversation. To construct the input Xinstruct in (2), for an image Xv, a question Xq is randomly sampled, which is a language instruction to request the assistant to describe the image briefly. The ground-truth prediction answer Xa is the original caption. In training, we keep both the visual encoder and LLM weights frozen, and maximize the likelihood of (3) with trainable parameters θ = W (the projection matrix) only. In this way, the image features Hv can be aligned with the pre-trained LLM word embedding. This stage can be understood as training a compatible visual tokenizer for the frozen LLM.\\n\\nStage 2: Fine-tuning End-to-End. We always keep the visual encoder weights frozen, and continue to update both the pre-trained weights of the projection layer and LLM in LLaVA; i.e., the trainable parameters are θ = {W, ϕ} in (3). We consider two specific use case scenarios:\\n\\n• Multimodal Chatbot. We develop a Chatbot by fine-tuning on the 158K language-image instruction-following data in Section 3. Among the three types of responses, conversation is multi-turn while the other two are single-turn. They are uniformly sampled in training.\\n\\n• Science QA. We study our method on the ScienceQA benchmark [34], the first large-scale multimodal science question dataset that annotates the answers with detailed lectures and explanations. Each question is provided a context in the form of natural language or an image. The assistant provides the reasoning process in natural language and selects the answer among multiple choices. For training in (2), we organize the data as a single turn conversation, the question & context as Xinstruct, and reasoning & answer as Xa.'),\n",
              " Element(type='text', text='5 Experiments\\n\\nWe assess the performance of LLaVA in instruction-following and visual reasoning capabilities with two primary experimental settings: multimodal chatbot and the ScienceQA dataset, respectively. We train all models with 8× A100s, following Vicuna’s hyperparameters [9]. We pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaVA-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and a batch size of 32. See Appendix for more training details.\\n\\n5.1 Multimodal Chatbot\\n\\nWe developed a chatbot demo to show the image understanding and conversation abilities of LLaVA, and to study how well LLaVA is able to digest visual inputs and exhibit instruction-following capabilities. We first use the examples in the original GPT-4 paper [36], shown in Table 3 (more examples in Appendix), that require in-depth image understanding. For comparisons, we quote the prompt and response of the multimodal GPT-4 from their paper, and query BLIP-2 and OpenFlamingo model checkpoints to get their response.\\n\\n5\\n\\nVisual input example, Extreme Ironing:\\n\\nSource: https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg What is unusual about this image? The unusual aspect of this image is a man ironing clothes on the back of a minivan or van. This is not a typical place to perform this activity, as one would usually iron clothes in a more stationary and safe location, such as a home, using a regular ironing board. The scene depicted in the image is peculiar as it involves a makeshift ironing setup on a vehicle, which can be both unsafe and unconventional. Additionally, it is not clear how the man is able to maintain balance and stability while ironing clothes in such an unstable environment. [Start a new conversation, and clear the history] What’s happening in the scene? The image depicts a man standing on top of a yellow SUV in a busy city street. He is holding a portable ladder, seemingly ironing clothes while standing on the vehicle. Around the scene, there are other cars, a traffic light, and a fire hydrant. Additionally, there are two other people in the vicinity, one close to the man on the car and another further away. The man on the ladder appears to be attracting attention due to his unconventional choice of ironing his clothes on top of a moving car. The city street around him is bustling with activity, adding to the unique nature of the scene.'),\n",
              " Element(type='text', text='User LLaVA\\n\\nUser LLaVA\\n\\nUser What’s happening in the scene?\\n\\nUser GPT-4 [36]\\n\\nWhat is unusual about this image? The unusual thing about this image is that a man is ironing clothes on an ironing board attached to the roof of a moving taxi.\\n\\nUser What is unusual about this image?\\n\\nUser BLIP-2\\n\\nWhat is unusual about this image? a man is sitting on the back of a yellow cab\\n\\nWhat is unusual about this image? User OpenFlamingo The man is drying his clothes on the hood of his car.\\n\\nTable 3: Example prompt from GPT-4 paper [36] to compare visual reasoning and chat capabilities. Compared to BLIP-2 [28] and OpenFlamingo [5], LLaVA accurately follows the user’s instructions, instead of simply describing the scene. LLaVA offers a more comprehensive response than GPT-4. Even when merely asked to describe the image, LLaVA identifies atypical aspects of the image.\\n\\nSurprisingly, although LLaVA is trained with a small multimodal instruction-following dataset (∼80K unique images), it demonstrates quite similar reasoning results with multimodal GPT-4 on these examples. Note that while these images are out-of-domain for LLaVA, LLaVA is still able to understand the scenes and follow the question instruction to provide a reasonable response. In contrast, BLIP-2 and OpenFlamingo focus on describing the image, instead of following the user instruction to answer in an appropriate manner.\\n\\nQuantitative Evaluation. To gain a systematic understanding of the performance of LLaVA, we propose a quantitative metric to measure the model’s instruction-following capability on multimodal data. Inspired by [9], we leverage GPT-4 to measure the quality of generated responses. Specifically, we create triplets consisting of image, ground-truth textual descriptions, and question. The candidate models (e.g., LLaVA) predict the answers based on the question and the image. To provide an approximate theoretical upper bound, we create a reference prediction based on the question and the ground-truth textual descriptions, using the text-only GPT-4. After obtaining the responses from both models, we feed the question, visual information (in the format of textual descriptions), and the generated responses from both assistants, to the judge (i.e., text-only GPT-4). It evaluates the helpfulness, relevance, accuracy, and level of detail of the responses from the assistants, and gives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance. It is also asked to provide a comprehensive explanation for the evaluation, for us to better understand the\\n\\n6'),\n",
              " Element(type='text', text='Table 4: Ablation on LLaVA-Bench (COCO) with different training data. We report relative scores w.r.t. a text-only GPT-4 model that uses ground truth image captions and bounding boxes as visual input. We prompt GPT-4 with the answers from our model outputs and the answers by GPT-4 (text-only), and let it compare between both responses and give a rating with an explanation.'),\n",
              " Element(type='text', text='Table 5: Instruction-following capability comparison using relative scores on LLaVA-Bench (In-the- Wild). The results are reported in the format of mean ± std. For the first three rows, we report three inference runs. LLaVA performs significantly better than others. † For a given set of LLaVA decoding sequences, we evaluate by querying GPT-4 three times; GPT-4 gives a consistent evaluation.\\n\\nmodels. We report relative scores w.r.t. the text-only GPT-4 model that uses the textural ground truth description as visual input. We create two benchmarks to evaluate the model’s performance.\\n\\nLLaVA-Bench (COCO). We randomly select 30 images from COCO-Val-2014, and for each image, we generate three types of questions (conversation, detailed description, complex reasoning) using the proposed data generation pipeline in Sec. 3, totaling 90 questions. This benchmark studies the model’s alignment behavior and capabilities with consistent visual inputs. We vary the training datasets to study the effectiveness of different types of instruction-following data, and show the results in Table 4. First, with instruction tuning, the model’s ability of following user instructions improves significantly by over 50 points. Second, adding a small amount of detailed description and complex reasoning questions contributes to a considerable improvement of the model’s overall capability by 7 points. Furthermore, it also improves the model’s performance on conversational questions, suggesting that improvements in reasoning capabilities complement conversational abilities. Finally, we show that having all three types of data yields the best performance at 85.1%.\\n\\nLLaVA-Bench (In-the-Wild). To evaluate the model’s capability in more challenging tasks and generalizability to novel domains, we collect a diverse set of 24 images with 60 questions in total, including indoor and outdoor scenes, memes, paintings, sketches, etc., and associate each image with a highly-detailed and manually-curated description and a proper selection of questions. We compare LLaVA, BLIP, and OpenFlamingo in Table 5. Thanks to visual instruction tuning, LLaVA achieves significantly better performance compared with BLIP-2 (+29%) and OpenFlamingo (+48%). Compared to the text-only GPT-4 that has access to ground-truth labels, LLaVA achieves an impressive 81.7% performance on complex reasoning questions, with an overall score of 67.3%.\\n\\nLimitations. This LLaVA-Bench (In-the-Wild) is designed to be challenging and to reveal a model’s weaknesses. We provide two examples with associated captions and questions in Table 6. For the ramen example (left), to correctly answer the name of the restaurant, it requires the model to have a large knowledge coverage and multilingual understanding capability; to correctly describe the side dishes, the model may need to retrieve relevant multimodal information from Internet. For the fridge example (right), perceiving the correct brand of the yogurt requires the model to process high resolution images and possess extensive knowledge coverage. We also observed an interesting failure of LLaVA, as it responds with yes when asked if strawberry-flavored yogurt is present, even though the fridge contains only yogurt and strawberries. This indicates that, at times, LLaVA perceives the image as a “bag of patches”, failing to grasp the complex semantics within the image. We hope LLaVA serves as a solid baseline on the benchmarks, on which our findings can inspire future work in developing more capable LMMs.\\n\\n7\\n\\nChallenging examples from LLaVA-Bench (In-the-Wild):'),\n",
              " Element(type='text', text='ICHIRAN Ramen [source]\\n\\nFilled fridge [source]\\n\\nAnnotation A close-up photo of a meal at ICHI- RAN. The chashu ramen bowl with a spoon is placed in the center. The ramen is seasoned with chili sauce, chopped scallions, and served with two pieces of chashu. Chopsticks are placed to the right of the bowl, still in their paper wrap, not yet opened. The ramen is also served with nori on the left. On top, from left to right, the fol- lowing sides are served: a bowl of or- ange spice (possibly garlic sauce), a plate of smoke-flavored stewed pork with chopped scallions, and a cup of matcha green tea.\\n\\n_ _\\n\\n_ _\\n\\n_\\n\\nAn open refrigerator filled with a variety of food items. In the left part of the compartment, towards the front, there is a plastic box of strawberries with a small bag of baby carrots on top. Towards the back, there is a stack of sauce containers. In the middle part of the compartment, towards the front, there is a green plastic box, and there is an unidentified plastic bag placed on it. Towards the back, there is a carton of milk. In the right part of the compartment, towards the front, there is a box of blueberries with three yogurts stacked on top. The large bottle of yogurt is Fage non-fat yogurt, and one of the smaller cups is Fage blueberry yogurt. The brand and flavor of the other smaller cup are unknown. Towards the back, there is a container with an unknown content.\\n\\nQuestion 1 What’s the name of the restaurant?\\n\\nWhat is the brand of the blueberry-flavored yogurt?\\n\\nQuestion 2 Describe this photo in detail.\\n\\nIs there strawberry-flavored yogurt in the fridge?\\n\\nTable 6: Challenging examples from LLaVA-Bench (In-the-Wild), we provide extremely-detailed annotation for each image for an accurate evaluation. Some questions require the model to extract details from high resolution image and to have a broad knowledge coverage.'),\n",
              " Element(type='text', text='5.2 ScienceQA\\n\\nScienceQA [34] contains 21k multimodal multiple choice questions with rich domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills. The benchmark dataset is split into training, validation, and test splits with 12726, 4241, and 4241 examples, respectively. We consider two representative methods, including GPT-3.5 model (text-davinci-002) with and without chain- of-thought (CoT), LLaMA-Adapter [59], as well as multimodal chain-of-thought (MM-CoT) [61], which is the current SoTA method on this dataset. For more baseline numbers, please see [34].\\n\\nThe results are reported in Table 7. For LLaVA, we use the visual features before the last layer, ask the model to first predict reasons and then the answer, and train it for 12 epochs. It yields 90.92% accuracy, which is quite close to the SoTA 91.68%. To explore the limit of LLMs, we also prompt GPT-4 using 2-shot in-context-learning and achieve 82.69% accuracy, which is a 7.52% absolute gain compared with 75.17% from GPT-3.5. For a substantial number of questions, we note that GPT-4 fails simply because it reports that there is insufficient context such as images or plots. We consider two schemes to combine the outcomes from our model and GPT-4. (i) A GPT-4 complement. Whenever GPT-4 fails to provide answers, we use the prediction from our method. This schemes yields 90.97% accuracy, which is almost the same as applying our method alone. (ii) GPT-4 as the judge. Whenever GPT-4 and LLaVA produce different answers, we prompt GPT-4 again, asking it to provide its own final answer based on the question and two outcomes. The spirit is similar with CoT, but with the external knowledge from the other model. Surprisingly, this scheme is able to provide consistent improvement over all question classes, and achieves a new SoTA accuracy of 92.53%. Interestingly, the text-only GPT-4, which cannot process images, improves the overall performance of the model on questions that have an image as context. This is because some of these questions do not actually require the image context for a correct answer. The GPT-4 judge can identify such cases and correct some of the errors that LLaVA makes. See the example in Appendix. To the best of our knowledge,\\n\\n8'),\n",
              " Element(type='text', text='Table 7: Accuracy (%) on Science QA dataset. Question categories: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context, NO = no context, G1-6 = grades 1-6, G7-12 = grades 7-12. †Text-only GPT-4, our eval. Our novel model ensembling with the text-only GPT-4 consistently improves the model’s performance under all categories, setting the new SoTA performance.\\n\\nthis is the first time that GPT-4 is used for model ensembling. We hope this finding can encourage future research to explore more effective methods to leverage LLMs for model ensembling.\\n\\nAblations. We ablate several design choices on ScienceQA in Table 8. (i) Visual features. We tried using the last layer feature from CLIP vision encoder, which yields 89.96% and is 0.96% lower than the feature before the last layer. We hypothesize that this is because CLIP’s last layer features may focus more on global and abstract image properties compared to the layer before it, which can focus more on localized properties that are useful for under- standing specific image details. (ii) Chain-of-thought. To decide the order between the answer and reasoning process in the model prediction, we run both variants and observe that answer-first reports the best number 89.77% accuracy in 12 epochs, while reasoning-first can quickly reach 89.77% accuracy in 6 epochs, but no further improvement with more training. Training the model for 24 epochs does not improve the performance. We conclude that CoT-like reasoning-first strategy can largely improve convergence, but contributes relatively little to the final performance. (iii) Pre-training. We skip pre-training and directly train on Science QA from scratch – performance drops to 85.81% accuracy. The 5.11% absolute degradation indicates the importance of our pre-training stage, in aligning multimodal features while preserving the vast pre-trained knowledge. (iv) Model size. We keep all configurations the same as our best 13B model, and train a 7B model. This yields 89.84% accuracy, which is 1.08% lower than 90.92%, demonstrating the importance of model scale.'),\n",
              " Element(type='text', text='6 Conclusion\\n\\nThis paper demonstrated the effectiveness of visual instruction tuning. We presented an automatic pipeline to create language-image instruction-following data, based on which we train LLaVA, a multimodal model to follow human intent to complete visual tasks. It achieves the new SoTA accuracy when fine-tuned on ScienceQA, and excellent visual chat capabilities when fine-tuned on multimodal chat data. Besides, we present the first benchmark to study multimodal instruction- following capability. This paper is an initial step in visual instruction tuning, and mainly focuses on real-life tasks. For more quantitative results of LLaVA on academic benchmarks, please refer to the improved baselines with visual instruction tuning [32]. We hope our work can inspire future research on building more capable multimodal models.\\n\\nAcknowledgements. We thank Baolin Peng and Pan Lu for valuable discussions on instruction-tuning language models and Science QA, respectively. We thank the LLaMA team for giving us access\\n\\n9\\n\\nto their models, and open-source projects, including Alpaca and Vicuna. This work was supported in part by NSF CAREER IIS2150012, and Institute of Information & communications Technology Planning & Evaluation(IITP) grants funded by the Korea government(MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) and (No. RS-2022-00187238, Development of Large Korean Language Model Technology for Efficient Pre-training).'),\n",
              " Element(type='text', text='References\\n\\n[1] Langchain. https://github.com/hwchase17/langchain, 2022. 2\\n\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022. 2, 4\\n\\n[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. 2\\n\\n[4] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. 1\\n\\n[5] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 2, 6, 7\\n\\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instruct pix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 2\\n\\n[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. 2\\n\\n[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 2\\n\\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. 1, 2, 4, 5, 6\\n\\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 2\\n\\n[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. 2\\n\\n[12] CVinW. Computer vision in the wild. https://github.com/ Computer-Vision-in-the-Wild/CVinW_Readings, 2022. 1\\n\\n[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 2\\n\\n[14] Fartash Faghri, Hadi Pouransari, Sachin Mehta, Mehrdad Farajtabar, Ali Farhadi, Mohammad Rastegari, and Oncel Tuzel. Reinforce data, multiply impact: Improved model accuracy and robustness with dataset reinforcement. arXiv preprint arXiv:2303.08983, 2023. 2\\n\\n[15] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. ArXiv, abs/2203.13131, 2022. 1\\n\\n10\\n\\n[16] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision- language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision, 2022. 1\\n\\n[17] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056, 2023. 3\\n\\n[18] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. arXiv preprint arXiv:2211.11559, 2022. 2'),\n",
              " Element(type='text', text='[19] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In CVPR, 2020. 2\\n\\n[20] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv preprint arXiv:2302.14045, 2023. 2\\n\\n[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip. July 2021. If you use this software, please cite it as below. 1\\n\\n[22] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 2\\n\\n[23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022. 2\\n\\n[24] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal generation. arXiv preprint arXiv:2301.13823, 2023. 2\\n\\n[25] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and René Ranftl. Language- driven semantic segmentation. ICLR, 2022. 1\\n\\n[26] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023. 1\\n\\n[27] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. ELEVATER: A bench- mark and toolkit for evaluating language-augmented visual models. In NeurIPS Track on Datasets and Benchmarks, 2022. 1\\n\\n[28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language- image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 1, 2, 4, 6, 7\\n\\n[29] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In CVPR, 2022. 1\\n\\n[30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint arXiv:2301.07093, 2023. 1\\n\\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 3\\n\\n[32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 9, 14\\n\\n11\\n\\n[33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 1\\n\\n[34] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. 2, 5, 8, 9\\n\\n[35] OpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023. 1, 2\\n\\n[36] OpenAI. Gpt-4 technical report, 2023. 1, 5, 6, 15\\n\\n[37] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022. 2'),\n",
              " Element(type='text', text='[38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. arXiv preprint arXiv:2304.03277, 2023. 1, 4\\n\\n[39] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary image classification. arXiv preprint arXiv: 2111.10050, 2021. 1\\n\\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 1, 2, 4\\n\\n[41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 2020. 2\\n\\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125, 2022. 1\\n\\n[43] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. CVPR, pages 10674–10685, 2022. 1\\n\\n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv, abs/2205.11487, 2022. 1\\n\\n[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion- 5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 2\\n\\n[46] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 2\\n\\n[47] Andrew Szot, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat. In Advances in Neural Information Processing Systems (NeurIPS), 2021. 2\\n\\n[48] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. 1, 4\\n\\n12\\n\\n[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1\\n\\n[50] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 1\\n\\n[51] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc- tions. arXiv preprint arXiv:2212.10560, 2022. 2\\n\\n[52] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705, 2022. 2'),\n",
              " Element(type='text', text='[53] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2\\n\\n[54] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Lu Yuan, Ce Liu, and Jianfeng Gao. Unified contrastive learning in image-text-label space. CVPR, 2022. 1\\n\\n[55] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2\\n\\n[56] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Benton C. Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. ArXiv, abs/2206.10789, 2022. 1\\n\\n[57] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 1\\n\\n[58] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. arXiv preprint arXiv:2303.08131, 2023. 1\\n\\n[59] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023. 2, 8, 9\\n\\n[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 2\\n\\n[61] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi- modal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. 8, 9\\n\\n[62] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16793–16803, 2022. 1\\n\\n[63] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. arXiv preprint arXiv:2212.11270, 2022. 1, 2\\n\\n13'),\n",
              " Element(type='text', text='A Broader Impact\\n\\nThe broader impact of LLaVA, a general-purpose visual assistant, has potential benefits and risks associated with its deployment and release. Some considerations are unique to LLaVA due to its visual nature, while others share similarities with existing instruction-following LLMs (e.g., Alpaca, Vicuna, etc.). As LLaVA is built upon LLaMA, Vicuna, and CLIP, it inherits some of the issues associated with LLMs and vision encoders. In the following, we outline both the risks and mitigation strategies in place for the release of this model.\\n\\nMalicious input. To minimize potential misuse and harmful consequences, we employ two pre- cautionary measures for LLaVA: (1) OpenAI Filter API for user input text to prevent harmful or inappropriate text instructions from being processed by the model, and (2) NSFW Filter for uploaded user images to detect and block Not Safe For Work (NSFW) content or any other potentially harmful image inputs.\\n\\nHallucination. Similar to LLMs, LLaVA might generate outputs that aren’t grounded in facts or input data. This raises concerns about inferences made, especially in critical applications (e.g., medical).\\n\\nBiases. Bias can be transferred from the base models to LLaVA, both from the vision encoder (CLIP) and the language decoder (LLaMA/Vicuna). This may lead to biased outcomes or unfair representations of diverse content.\\n\\nEnergy consumption. Though energy consumption is not a primary concern for LLaVA due to a smaller pretraining dataset (see details in Sec. C), it may become a concern when scaling up the pretraining dataset or increasing the model size, e.g., to a larger LLaMA version like the 65B model.\\n\\nEvaluation complexities. Assessing the performance of LLaVA is challenging as it involves both language and visual tasks. Our evaluation benchmark covers several aspects, including accuracy, concept coverage, reasoning ability, and creativity. However, additional aspects need consideration, such as the degree of visual content hallucination and fine-grained understanding of visual content. While text-only GPT-4 based multimodal evaluation is consistent and accurate in our study, its robustness in different situations and capability to evaluate other unexplored aspects are subjects for future work.\\n\\nDespite these risks, we believe that the benefits of releasing LLaVA to the research community outweigh the potential harm. It allows for ongoing investigation and improvement of the model and engages the community in developing better mitigation strategies to address these concerns. Moreover, the release of LLaVA can spur the development of new applications and research directions, ultimately contributing to the progress and responsible deployment of foundation models in vision-language tasks.'),\n",
              " Element(type='text', text='B More Results\\n\\nWe present more qualitative results of LLaVA to analyze its emergent behaviors and observed weaknesses. For more quantitative results of LLaVA on academic benchmarks, please refer to the improved baselines with visual instruction tuning [32]. In Table 9, LLaVA demonstrates a similar behavior as GPT-4 in another example from its paper. Similar to the GPT-4 live demo by OpenAI, LLaVA is capable of generating the HTML/JS/CSS code for an interactive joke website based on a simplified user input sketch in Fig. 2, despite a minor error. As shown in Fig. 3, LLaVA can follow user’s instructions in a conversational style and provide detailed responses or creative writings. Furthermore, LLaVA is able to relate the visual content to the textual knowledge from the pretrained LLM, as demonstrated in Fig. 4 and Fig. 5.\\n\\nOne interesting emergent behavior of LLaVA is that it is able to understand visual contents that are not covered in the training. For example, in Fig. 6, it is able to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge, even though Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of\\n\\n14\\n\\nLLaVA. LLaVA also demonstrates impressive OCR (optical character recognition) ability in Table 9 and Fig. 2, which is rarely covered in our training data.\\n\\nWe hope these additional results and observations showcase the potential of LLaVA in various application areas. In future work, it is important to investigate these emergent behaviors more thoroughly and to understand the underlying mechanisms that enable LLaVA to demonstrate such generalization abilities. This will pave the way towards building better LMMs, including enhancing robustness, reducing biases, and improving the alignment and the scope of the learned vision-language representations.\\n\\nVisual input example, Chicken Nugget Map:\\n\\n‘Sometimes | just look at pictures of the earth from space and | marvel at how beautiful it alls.'),\n",
              " Element(type='text', text='User LLaVA\\n\\nCan you explain this meme in detail? The meme in the image is a creative and humorous take on food, with a focus on chicken nuggets, as the center of the universe. The meme begins with the phrase \"Sometimes I just look at pictures of the Earth from space and I marvel at how beautiful it all is...\" The main image features a tray of cooked chicken nuggets, with the chicken pieces positioned to resemble the continents and islands on a world map. The meme playfully suggests that the chicken nuggets represent the Earth, and the various locations depicted in the photo are actually chicken nugget versions of different places. The punchline of the meme appears in smaller text at the bottom, reading \"I mean, it’s not the real Earth, but how beautiful it is all is.\" This lighthearted and entertaining meme combines food and humor, using the image of chicken nuggets to create a funny and imaginative representation of the world.\\n\\nUser GPT-4 [36] Can you explain this meme? This meme is a joke that combines two unrelated things: pictures of the earth from space and chicken nuggets. The text of the meme suggests that the image below is a beautiful picture of the earth from space. However, the image is actually of chicken nuggets arranged to vaguely resemble a map of the world. The humor in this meme comes from the unexpected juxtaposition of the text and the image. The text sets up an expectation of a majestic image of the earth, but the image is actually something mundane and silly.\\n\\nUser GPT-4 [36]\\n\\nUser Can you explain this meme?\\n\\nUser BLIP-2\\n\\nCan you explain this meme in detail? sometimes i just look at pictures of the earth from space and marvel how beautiful it is\\n\\nUser OpenFlamingo\\n\\nCan you explain this meme in detail? It’s a picture of a chicken nugget on the International Space Station.\\n\\nTable 9: Example prompt comparing LLaVA, GPT-4, BLIP-2, and OpenFlamingo’s visual reasoning capabilities in understanding the humor. BLIP-2 and OpenFlamingo fail to follow the user’s instructions. LLaVA and GPT-4 both explain the meme and its humor, while GPT-4 produces a more concise answer.\\n\\n15\\n\\nAi LLaVA Turn this mock-up it the joke with a real joke. <!DOCTYPE html> <htmi> <head> <title>My Joke Website</title> <style> body { background-color: white; font-family: Arial, sans-serif; } container { display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; } 1 { font-size: 36px; margin-bottom: 20px; } p{ font-size: 20px; } button { background-color: blue; color: white; border: none; padding: 10px 20px; font-size: 18px; cursor: pointer; } </style> </head> <body> <div class=\"container\"> <h1>My Joke Website</h1> <p id=\"punchline\">Welcome to my joke website. Push the button to reveal the punchline!</p> <button onclick=\"revealPunchline()\">Push me!</button> </div> <script> function revealPunchline() { document. getElementByld(\"punchline\").innerHTML = \"The punchline is: ‘Why was the math book sad? Because it had too many problems.\"; } </script> </body> </html> Place the generated HTML code into an HTML renderer to render the web page. My Joke Website My Joke Website Welcome to my joke website. Push the The punchline is: \\'Why was the math book button to reveal the punchline! sad? Because it had too many problems.’ Rendered website Rendered website (clicked the button)\\n\\nFigure 2: LLaVA generates HTML/JS code for an interactive website based on user sketch inputs. The interactive interface works after fixing a minor error (in red) in the generated output. There is room for improvement in LLaVA’s output, such as splitting the joke and punchline into two rows, and only revealing the punchline upon button click, to better reflect the user’s intent.\\n\\n16'),\n",
              " Element(type='text', text=\"What are the meals that | can cook with these? With the variety of food items stored in the refrigerator, you can prepare several meals. For example, you can create a fruit salad using the strawberries, blueberries, and carrots. Additionally, you can make a delicious smoothie using the fruits and vegetables, which would be a healthy and nutritious snack or breakfast option. The milk and eggs can be used to create various dishes, such as a milk-based smoothie, a LLaVA | baked casserole, or scrambled eggs. The juice can also be used to make a fresh fruit and juice mix or as a refreshing homemade popsicle. The possibilities are vast, and the meals you can create will depend on your preferences and dietary needs. | would like to do the fruit salad. Show me the recipe. | & Certainly! Here's a simple fruit salad recipe using the items in your refrigerator: User Ingredients: LLaVA| -2 cups fresh strawberries, sliced - 1 cup fresh blueberries - 1 large carrot, grated - Juice of half a lemon - A handful of chopped fresh parsley or mint - A few ice cubes - Optional: honey or maple syrup (adjust to taste) Instructions: 1. Ina large bowl, combine the sliced strawberries, blueberries, and carrot. Gently toss them together to mix. 2. Drizzle the juice of half a lemon over the fruit and vegetables to prevent them from oxidizing and add a refreshing citrus flavor. 3. Sprinkle the chopped fresh parsley or mint over the fruit and gently toss once more. 4. |f desired, add a drizzle of honey or maple syrup for a touch of sweetness. Adjust the amount according to your taste preferences. 5. Cover the bowl and refrigerate the fruit salad for at least 30 minutes or up to a few hours. This allows the flavors to meld and creates a more refreshing salad. 6. Serve the fruit salad in cups or bowls, garnished with a sprinkle of fresh parsley or mint, if desired. This fruit salad recipe is perfect for a healthy snack or side dish, and with the variety of ingredients available in your refrigerator, it should be easy to put together quickly.\"),\n",
              " Element(type='text', text=\"What are the things | should be cautious about when | visit here? When visiting this peaceful location with a pier extending over a serene lake, surrounded by mountains, there are several things to be cautious about. First, pay attention to the weather conditions, as sudden changes in weather, especially strong winds or storms, can cause the water to become rough and pose risks to the stability of the pier. Second, be mindful of the structural integrity of the pier; avoid overloading it with too LLaVA many people or excessive weight, as this could potentially damage the pier or cause it to collapse. Additionally, be aware of the surrounding wildlife and respect their space; do not harm or disturb them in any way. Finally, if you decide to go fishing, ensure that you adhere to any local fishing regulations and use appropriate fishing techniques to keep the lake ecosystem healthy and maintain the balance of the food chain. Compose an engaging travel blog post about my trip to this place, highlighting the beautiful sceneries, joyful experiences, and include possible anecdotes. oS oF Traveling to this picturesque location with a serene lake and mountains in the background was a refreshing and rejuvenating experience. The User pier extending over the calm water provided an excellent vantage point from which to appreciate the breathtaking scenery. As | stood on the pier, | couldn't help but feel a deep connection with nature, taking a moment to soak in the peaceful atmosphere. LLaVA One of the highlights of my trip was undoubtedly the fishing experience. The area is teeming with life, and | was thrilled to land a sizeable catch, despite the modest size of the lake. The joy of fishing in such a beautiful setting was truly rewarding, and it allowed me to take a break from my busy life and truly immerse myself in the tranquility of nature. While exploring the surrounding area, | came across a family of birds that had built a nest close to the pier, adding to the charm of the location. Interacting with the wildlife and observing their behavior was an enriching experience that I'll cherish for years to come. In conclusion, this idyllic destination provided me with an opportunity to unwind and recharge, surrounded by mesmerizing natural beauty and the gentle sounds of water and wildlife. If you're looking for a serene escape from the hustle and bustle of everyday life, this pier overlooking the lake is the perfect place to find solace and reconnect with nature.\\n\\nFigure 3: LLaVA is capable of recognizing the visual content following the user’s intent, without directly prompting for visual recognition. It also provides a detailed response when prompted with a follow-up request, and the generated response is closely related to the provided visual content.\\n\\n17\\n\\nLLaVA G User What's the ending of this movie? In the image, a young woman is leaning on a man in front of a beautiful sunset, with both of them standing on a boat. They appear to be enjoying a romantic moment together. The scene suggests that they could be depicted as the characters Jack and Rose from the popular movie Titanic. In the movie, their romance blossoms aboard the fictional ocean liner Titanic, with the pair ultimately sharing a passionate kiss as the ship sinks into the ocean. The ending of the movie is known for its emotional and heartfelt scenes, where the characters are forced to say goodbye to each other as they are separated by the sinking ship. The image evokes a sense of romance and nostalgia, paying tribute to the iconic love story portrayed in the movie Titanic.\\n\\nFigure 4: LLaVA relates the movie scenes to the textual knowledge from the pretrained LLM.\"),\n",
              " Element(type='text', text=\"LLaVA G User Do you know who drew this painting? The painting depicts a woman, commonly believed to be Mona Lisa, the famous artwork by Leonardo da Vinci. It is a portrait painting that showcases the woman's enigmatic smile and has become one of the most famous and iconic art pieces in the world. The original work is displayed in the Louvre Museum in Paris, and it is known for its intricate details, use of oil paint, and the artist's innovative techniques that contributed to its enduring appeal and mystery. Start a new conversation, and the history is cleared. Do you know who drew this painting? The painting depicts a dog in a humorous situation, where it appears to be wearing a woman's dress and standing in front of a landscape. Although the artist of the painting is not explicitly identified, it could be a reference to the famous artist Da Vinci, known for his artistic works, including the Mona Lisa. The painting is likely a creative and comical take on the traditional portrait style, replacing a human figure with a dog dressed in a woman's clothing, making it an interesting and amusing visual piece.\\n\\nFigure 5: LLaVA recognizes the famous art work, Mona Lisa, by Leonardo da Vinci. When we start a new conversation, it also explains the humourous artwork created on the web, mimicking the Mona Lisa.\\n\\n18\\n\\na ° | What is the name of the man in the photo? is the name of the man in the | What is the name of the man in the photo? LLaVA__ The name of the man in the photo is Elon Musk. Start a new conversation, and the history is cleared. mo | Whatis the name of the man in the photo? What is the name of the man in the photo? The name of the man in the photo is Elon Musk. LLaVA p\\n\\nFigure 6: An interesting emergent behavior of LLaVA is its ability to recognize Elon Musk both in a headshot and in a humorous meme where he is dressed as a doge. This implies that the pre-trained CLIP vision encoder may have seen images of Elon Musk. However, it is still surprising because Elon Musk never appears in the training data for either the visual feature alignment or visual instruction tuning stages of LLaVA, which indicates that the base language model generalizes to unseen visual concepts.\\n\\nQuestion: Which material is this rocking chair made of? Options: (A) wood (B) silk LLaVA answer: LECTURE: A material is a type of matter. Wood, glass, metal, and plastic are common materials. Some objects are made of just one material. Most nails are made of metal. Other objects are made of more than one material. This hammer is made of metal and wood. SOLUTION: Look at the picture of the rocking chair. The rocking chair is made of two different materials. The legs are made of wood, and the back and seat are made of silk. The answer is B. GPT-4 (text-only) answer: The answer is A. BECAUSE: Rocking chairs are typically made of sturdy materials that can support the weight of a person and withstand the rocking motion. Wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance.\"),\n",
              " Element(type='text', text='GPT-4 (text-only) judge: Since there is no context or image to refer to, I cannot independently analyze the rocking chair in question. However, I can try to analyze the reasoning given by the assistants. Assistant 1, who claims to have seen the image, mentions that the legs of the rocking chair are made of wood, and the back and seat are made of silk, implying that it’s a combination of two materials. However, this response seems less plausible, as silk is not a common material used for the construction of rocking chairs due to issues with stability and durability. Typically, one would expect the frame, legs, and seat or back to be made of wood, with cushions and padding made of fabrics, such as cotton or polyester. Assistant 2 argues that rocking chairs are typically made of sturdy materials, and wood is a common material used for making rocking chairs due to its strength, durability, and natural appearance. This reasoning stands true and is based on common knowledge. Given the information available, the most plausible answer is that the rocking chair is made of wood, as suggested by Assistant 1. The answer is A.\\n\\nTable 10: One example on how the text-only GPT-4 acts as a judge to ensemble the predictions from LLaVA and a text-only GPT-4, and gives a correct final answer.\\n\\n19\\n\\nC Training Details\\n\\nWe pre-train our model on the filtered CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, and fine-tune on the proposed LLaVA-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and a batch size of 32. Following Vicuna, we use the Adam optimizer with no weight decay and a cosine learning rate with a warmup ratio of 3%. During finetuning, FSDP (Full Shard Data Parallel) and gradient checkpointing is used to save GPU memory, and offloading is not used. BF16 and TF32 are enabled to achieve a balance between speed and precision.\\n\\nWe train all models with 8× A100s. Pretraining on CC-595K completes within 4 hours. Finetuning on Instruct-158K completes within 10 hours. Finetuning on ScienceQA completes within 4 hours.'),\n",
              " Element(type='text', text='D Assets\\n\\nOur source code, generated instruction-tuning data, proposed benchmark are uploaded to the anonymized GitHub repository: LLaVA-Annonymous/LLaVA.\\n\\n1. Source Code: link 2. README: link 3. Instructions to launch the demo: link 4. All prompts and few shot examples for querying GPT-4: link 5. LLaVA-Instruct-158K: link 6. LLaVA-Bench: COCO, In-The-Wild 7. Model checkpoints. The size of the model checkpoints after compression is 25GB, which exceeds the 5GB limit of GitHub LFS (Large File Storage). We’ll release the checkpoint to the public, or upon request with reviewers for this submission.\\n\\nE Data\\n\\nInstructions for brief image description. The list of instructions used to briefly describe the image content are shown in Table 11. They present the same meaning with natural language variance.\\n\\n\"Describe the image concisely.\" • \"Provide a brief description of the given image.\" • \"Offer a succinct explanation of the picture presented.\" • \"Summarize the visual content of the image.\" • \"Give a short and clear explanation of the subsequent image.\" • \"Share a concise interpretation of the image provided.\" • \"Present a compact description of the photo’s key features.\" • \"Relay a brief, clear account of the picture shown.\" • \"Render a clear and concise summary of the photo.\" • \"Write a terse but informative summary of the picture.\" • \"Create a compact narrative representing the image presented.\"\\n\\nTable 11: The list of instructions for brief image description.\\n\\nInstructions for detailed image description. The list of instructions used to describe the image content in detail are shown in Table 12. They present the same meaning with natural language variance.\\n\\nCC3M. We extract noun-phrases using Spacy for each caption over the whole CC3M dataset, and count the frequency of each unique noun-phrase. We skip noun-phrases whose frequency is smaller than 3, as they are usually rare combinations concept and attributes that has already been covered\\n\\n20\\n\\n• \"Describe the following image in detail\" • \"Provide a detailed description of the given image\" • \"Give an elaborate explanation of the image you see\" • \"Share a comprehensive rundown of the presented image\" • \"Offer a thorough analysis of the image\" • \"Explain the various aspects of the image before you\" • \"Clarify the contents of the displayed image with great detail\" • \"Characterize the image using a well-detailed description\" • \"Break down the elements of the image in a detailed manner\" • \"Walk through the important details of the image\" • \"Portray the image with a rich, descriptive narrative\" • \"Narrate the contents of the image with precision\" • \"Analyze the image in a comprehensive and detailed manner\" • \"Illustrate the image through a descriptive explanation\" • \"Examine the image closely and share its details\" • \"Write an exhaustive depiction of the given image\" \\n\\nTable 12: The list of instructions for detailed image description.\\n\\nby other captions. We then start from the noun-phrases with lowest remaining frequency, add the captions that contain this noun-phrase to the candidate pool. If the frequency of the noun-phrase is larger than 100, we randomly choose a subset of size 100 out of all its captions. This results in around 595K image-text pairs.\\n\\nThe comparison of noun-phrase statistics before and after filtering CC3M is shown in Figure 7. The filtered dataset shows a good coverage of concepts whose frequency is higher from 3, but with a smaller number of image-text pairs.\\n\\n10° — CC3M: 108182 . : g — CC3M (Filtered): 31423 Ss 103 g 10 oa @ E 10} 0 10000 20000 30000 40000 50000 Unique noun-phrases (ordered by frequency in the descending order)\\n\\nFigure 7: Comparison of noun-phrase statistics before and after filtering CC3M. The total number of unique noun-phrases are reported in the legend.'),\n",
              " Element(type='text', text='F Prompts\\n\\nThe prompt used to generate image-based conversation from ChatGPT/GPT-4 is shown in Table 13.\\n\\n21\\n\\nmessages = [ {\"role\":\"system\", \"content\": f\"\"\"You are an AI visual assistant, and you are seeing a single image. What you see are provided with five sentences, describing the same image you are looking at. Answer all questions as you are seeing the image.\\n\\nDesign a conversation between you and a person asking about this photo. The answers should be in a tone that a visual AI assistant is seeing the image and answering the question. Ask diverse questions and give corresponding answers.\\n\\nInclude questions asking about the visual content of the image, including the object types, counting the objects, object actions, object locations, relative positions between objects, etc. Only include questions that have definite answers: (1) one can see the content in the image that the question asks about and can answer confidently; (2) one can determine confidently from the image that it is not in the image. Do not ask any question that cannot be answered confidently.\\n\\nAlso include complex questions that are relevant to the content in the image, for example, asking about background knowledge of the objects in the image, asking to discuss about events happening in the image, etc. Again, do not ask about uncertain details. Provide detailed answers when answering complex questions. For example, give detailed examples or reasoning steps to make the content more convincing and well-organized. You can include multiple paragraphs if necessary.\"\"\"} ] for sample in fewshot_samples:\\n\\nmessages.append({\"role\":\"user\", \"content\":sample[‘context’]}) messages.append({\"role\":\"assistant\", \"content\":sample[‘response’]} ) messages.append({\"role\":\"user\", \"content\":‘\\\\n’.join(query)})\\n\\nTable 13: For each query, we illustrate the prompt construction process for ChatGPT/GPT-4 to collect query[‘response’] from query[‘context’], using few-shot in-context-learning, where examples are from fewshot_samples, each example including input sample[‘context’] and output sample[‘response’]. Note that messages is the final prompt. In this example, we provide the prompt used to generate the conversation response, please see also see its in-context-learning examples in Table 15 and Table 16 for details. We recommend readers to check out the codebase for the prompts to generated two other types of responses, including detailed decription and complex reasoning.\\n\\n22\\n\\nContext type 1: Captions A group of people standing outside of a black vehicle with various luggage. Luggage surrounds a vehicle in an underground parking area People try to fit all of their luggage in an SUV. The sport utility vehicle is parked in the public garage, being packed for a trip Some people with luggage near a van that is transporting it. Context type 2: Boxes person: [0.681, 0.242, 0.774, 0.694], person: [0.63, 0.222, 0.686, 0.516], person: [0.444, 0.233, 0.487, 0.34], backpack: [0.384, 0.696, 0.485, 0.914], backpack: [0.755, 0.413, 0.846, 0.692], suitcase: [0.758, 0.413, 0.845, 0.69], suitcase: [0.1, 0.497, 0.173, 0.579], bicycle: [0.282, 0.363, 0.327, 0.442], car: [0.786, 0.25, 0.848, 0.322], car: [0.783, 0.27, 0.827, 0.335], car: [0.86, 0.254, 0.891, 0.3], car: [0.261, 0.101, 0.787, 0.626]\\n\\nfap Lee'),\n",
              " Element(type='text', text='Response type 1: conversation Question: What type of vehicle is featured in the image? Answer: The image features a black sport utility vehicle (SUV). Question: Where is the vehicle parked? Answer: The vehicle is parked in an underground parking area, likely in a public garage. Question: What are the people in the image doing? Answer: The people are trying to fit all of their luggage into the SUV, likely preparing for a trip. Response type 2: detailed description The image is an underground parking area with a black sport utility vehicle (SUV) parked. There are three people in the scene, with one person standing closer to the left side of the vehicle, another person in the middle, and the third person on the right side. They are all working together to pack their luggage into the SUV for a trip. In the parking area, various luggage items are scattered around the vehicle. There are two backpacks, one located near the left rear wheel and the other closer to the right side of the vehicle. Additionally, there are two suitcases, one on the right side of the car and another further away near the center of the parking area. A bicycle can also be seen on the left side of the vehicle. Other cars are parked around the main SUV, with one car positioned behind it and slightly to the left, another behind and slightly to the right, and the third car further behind on the right side. Response type 3: complex reasoning Question: What challenges do these people face? Answer: In the image, a group of people is standing outside a black SUV in a parking area, surrounded by various pieces of luggage, including suitcases and backpacks. They are facing the challenge of fitting all their luggage into the black SUV. There are multiple suitcases and backpacks to be packed, which suggests that the group has a significant amount of belongings to accommodate. They might have to strategize and arrange the luggage efficiently to ensure that everything fits properly into the vehicle. Additionally, they need to consider the comfort of the passengers and visibility while driving, so the placement of the luggage must not obstruct the driver’s view or make the passengers uncomfortable during the trip.\\n\\nTable 14: One example to illustrate the instruction-following data. The top block shows the contexts such as captions and boxes used to prompt GPT, and the bottom block shows the three types of responses. Note that the visual image is not used to prompt GPT, we only show it here as a reference.\\n\\n23'),\n",
              " Element(type='text', text='Captions There is a movie theater that displays the show times above the doors. A red fire hydrant is deep in the snow. The fire hydrant is in the snow near a recently plowed sidewalk. This city has had a very hard winter with snow. A hotel for dogs in the snow in winter. Conversation Question: What color is the fire hydrant in the image? === Answer: The fire hydrant in the image is red. === Question: Is the fire hydrant fully covered by snow or partially visible? === Answer: The fire hydrant is partially visible, with a significant portion of it covered by the deep snow. === Question: What is the condition of the sidewalk near the fire hydrant? === Answer: The sidewalk near the fire hydrant has been recently plowed, so it is relatively clear of snow compared to the surrounding area. === Question: What does the image suggest about the winter season in the city? === Answer: The image suggests that the winter season in the city has been quite harsh and challenging. The significant snow accumulation seen in the image indicates that the city has experienced heavy snowfall, which can lead to various difficulties and disruptions for its residents. For instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city’s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts. Furthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages. In conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure.\\n\\nCaptions There is a movie theater that displays the show times above the doors. A red fire hydrant is deep in the snow. The fire hydrant is in the snow near a recently plowed sidewalk. This city has had a very hard winter with snow. A hotel for dogs in the snow in winter.\\n\\nFor instance, the deep snow covering the fire hydrant could potentially hinder its accessibility in case of emergencies, which poses safety concerns. The recently plowed sidewalk implies that the city’s maintenance crews have been actively working to keep the walkways clear and safe for pedestrians, but the sheer amount of snowfall might make it difficult to keep up with the continuous snow removal efforts.\\n\\nFurthermore, such extreme winter conditions can impact transportation, leading to traffic issues, delays in public transit, and increased risks of accidents due to slippery roads. It can also cause problems for businesses and schools, as people might face challenges commuting to work or attending classes. Additionally, the heavy snow can put extra strain on infrastructure, such as roofs and power lines, increasing the likelihood of structural damage or power outages.\\n\\nIn conclusion, the image of the red fire hydrant deep in the snow and the recently plowed sidewalk suggest that the city has faced a particularly severe winter season, with substantial snowfall that has likely caused various challenges and disruptions for its residents and infrastructure.\\n\\nTable 15: One example used in in-context-learning to construct visual conversation data.'),\n",
              " Element(type='text', text='24\\n\\nCaptions A man is skiing in the open snow covered hills A skier is making his way into the snow. A skier on the foothills of a huge mountain range. A skier looks at mountains while standing near a trail sign. a single person skiing by an area with a lot of bushes\\n\\nConversation Question: What is the skier doing? === Answer: The skier is skiing in the snow-covered hills, making his way through the snow and enjoying the beautiful mountain scenery. They are also standing near a trail sign, which indicates their location on the mountain. === Question: Are there any other people or skiers visible in the image? === Answer: No, there is only a single person skiing in the image, and they are surrounded by an area with a lot of bushes. === Question: What kind of skiing is the person doing? Cross-country or downhill? === Answer: Based on what I can see in the photo, it is not possible to definitively determine whether the skier is doing cross-country or downhill skiing. However, given the presence of a trail sign and the fact that they are skiing in the foothills, it is more likely that the skier is engaged in cross-country skiing. Downhill skiing typically takes place on more steep slopes and involves ski lifts, whereas cross-country skiing often occurs on relatively flatter terrain with marked trails. === Question: Can you explain cross-country and downhill skiing a bit more? === Answer: Cross-country skiing and downhill skiing are two distinct types of skiing, each with its own techniques, equipment, and terrain preferences.\\n\\n1. Cross-Country Skiing: Also known as Nordic skiing, cross-country skiing involves traversing relatively flat terrain or gentle slopes using skis and poles. It is a popular recreational and competitive sport that focuses on endurance, balance, and aerobic fitness. Cross-country skiers use their own locomotion to move across snow-covered terrain, rather than relying on ski lifts or other forms of assistance. There are two main techniques in cross-country skiing: - Classic: This technique involves skiing in parallel tracks, using a forward-and-backward gliding motion while keeping the skis parallel to each other. The skier uses their poles to propel themselves forward. - Skate skiing: This technique is similar to ice skating, where the skier pushes off one ski at an angle and glides on the other. It requires a smooth, groomed surface and is typically faster than the classic technique. Cross-country ski equipment consists of lightweight skis, boots, bindings, and poles. The boots are more flexible than those used in downhill skiing, allowing for better ankle movement and control.\\n\\n2. Downhill Skiing: Also known as alpine skiing, downhill skiing involves descending slopes at high speeds using skis and poles for balance and control. This sport is more focused on speed, technique, and navigating challenging terrain, including steep slopes, moguls, and even jumps. Downhill skiing can be further categorized into several disciplines, such as slalom, giant slalom, super-G, and downhill racing. Each discipline has its own set of rules, courses, and ski equipment. Downhill ski equipment includes heavier and stiffer skis, boots, bindings, and poles than those used in cross-country skiing. The boots are more rigid to provide better support and control during high-speed descents and sharp turns.\\n\\nIn summary, cross-country skiing is an endurance-based sport that involves traveling across flat or gently sloping terrain, while downhill skiing is focused on speed and technique as skiers navigate steeper slopes and challenging terrain. Both sports require specialized equipment and techniques, but they offer different experiences and challenges to participants.\\n\\nTable 16: One example used in in-context-learning to construct visual conversation data.'),\n",
              " Element(type='text', text='25')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jh6HrMUi1z3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-vector retriever"
      ],
      "metadata": {
        "id": "RonEkmIzJIKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "multi-vector-retriever를 사용하여 표와 선택적으로 **텍스트의 요약(summaries)을 생성**합니다.\n",
        "\n",
        "요약과 함께 원본 표 요소(raw tables)도 저장합니다.\n",
        "\n",
        "**요약은 검색의 품질을 향상시키는 데 사용**되며, 이는 multi-vector retriever 공식문서에서 설명됩니다.\n",
        "\n",
        "**원본 표는 LLM에 전달되어, LLM이 답변을 생성하는 데 전체 표 맥락을 제**공합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I7s7Ic3M2d5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summaries"
      ],
      "metadata": {
        "id": "Xb66Pb4t2sOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "BNfMrGTY2uZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "각 요소에 대해 간단한 요약 체인을 생성합니다."
      ],
      "metadata": {
        "id": "3g9L9PuC3GkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
        "Give a concise summary of the table or text. Table or text chunk: {element}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\", openai_api_key=OPENAI_KEY)\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "IG8CQYke3Iv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ],
      "metadata": {
        "id": "W6RZAbyO3Itl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAgyU5z03Iq_",
        "outputId": "df32b27e-8c8f-4f7e-87a4-ea310efe8372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The table presents data on conversation detail description and complex reasoning. The data is divided into five categories: Full data, Detail + Complex Conv + 5%, Detail + 10% Complex, Conversation, and No Instruction Tuning. In all categories, there is a decrease in values as the complexity increases. The most significant decrease is observed in the No Instruction Tuning category, with drops ranging from -51.3 to -78.0. The smallest decrease is seen in the Full data category, with drops ranging from -1.6 to -6.6.',\n",
              " 'The table presents the performance of different models (OpenFlamingo, BLIP-2, LLaVA, and LLaVA†) in terms of complex reasoning in conversation detail description. The results are presented as percentages with a margin of error. LLaVA† shows the highest performance in all categories, with scores ranging from 58.8 ± 0.6 to 81.4 ± 0.3. OpenFlamingo consistently scores the lowest, with results ranging from 19.0 ± 0.5 to 19.3 ± 0.5.',\n",
              " \"The table presents the performance of various methods in a certain task, with results reported in the literature and from the author's own experiments. The methods include Human, GPT-3.5, GPT-3.5 w/ CoT, LLaMA-Adapter, MM-CoTBase, MM-CoTLarge, GPT-4†, LLaVA, LLaVA+GPT-4† (complement), and LLaVA+GPT-4† (judge). The performance is measured across different contexts and modalities, including NAT, SOC, LAN, IMG, TXT, NO, and grades from G1-6 to G7-12. The highest performance reported in the literature is 95.91 by MM-CoTLarge, while the highest performance from the author's experiments is 96.74 by LLaVA+GPT-4† (complement).\",\n",
              " 'The table compares the performance of different models based on visual features. The \"Last Before\" model had a decrease of 5.11 points from 90.92 to 85.81, and a further decrease of 1.08 to 89.84. The \"Best variant\" model, which predicts the answer first and is trained from scratch with a 7B model size, had a decrease of 0.96 to 89.96 and a further decrease of 1.15 to 89.77.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply to texts\n",
        "texts = [i.text for i in text_elements]\n",
        "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})"
      ],
      "metadata": {
        "id": "v1i2hTxM3p_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hilkqxKC3p9T",
        "outputId": "18daad5b-f971-4898-d4c9-71ff867f6ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The text discusses a study on instruction tuning large language models (LLMs) using machine-generated instruction-following data to improve zero-shot capabilities on new tasks. The researchers used language-only GPT-4 to generate multimodal language-image instruction-following data. They introduced LLaVA: Large Language and Vision Assistant, a large multimodal model trained end-to-end that connects a vision encoder and an LLM for general-purpose visual and language understanding. They also constructed two evaluation benchmarks for future research on visual instruction following. The experiments showed that LLaVA demonstrated impressive multimodal chat abilities and achieved a new state-of-the-art accuracy of 92.53% when fine-tuned on Science QA. The researchers made GPT-4 generated visual instruction tuning data, their model, and code publicly available.',\n",
              " 'The text discusses the development of artificial intelligence, specifically focusing on creating a general-purpose assistant that can follow multi-modal vision-and-language instructions. It highlights the growing interest in language-augmented vision models and large language models (LLMs) that can perform various tasks based on explicit language instructions. The text also mentions the success of models like ChatGPT, GPT-4, and LLaMA, which have shown the potential of LLMs in following human instructions. The authors introduce visual instruction-tuning, a method to extend instruction-tuning to the language-image multimodal space. They present a data reformation perspective and pipeline to convert image-text pairs into an instruction-following format, develop a large multimodal model (LMM), and present LLaVA-Bench, a multimodal instruction-following benchmark. They also plan to release their generated multimodal instruction-following data and models to the public.',\n",
              " 'The text discusses the development of multimodal instruction-following agents in computer vision and natural language processing (NLP). Two types of models are identified: end-to-end trained models, which are specific to each research topic, and systems that coordinate various models. The text also discusses instruction tuning in NLP, which allows models like GPT-3, T5, PaLM, and OPT to follow natural language instructions and complete tasks. This approach has improved the generalization abilities of these models. The text also mentions the development of open-source multimodal models, which have shown promising task transfer generalization performance. However, these models are not explicitly tuned with vision-language instruction data, and their performance in multimodal tasks usually falls short compared to language-only tasks. The text concludes by differentiating between visual instruction tuning and visual prompt tuning.',\n",
              " 'The text discusses the use of GPT-4 and ChatGPT for generating multimodal instruction-following data, which is currently limited in availability. The authors propose using image and caption pairs to prompt GPT-4 to create a list of questions that instruct an assistant to describe the image content. However, this method lacks diversity and in-depth reasoning. To address this, they use GPT-4 or ChatGPT to create instruction-following data involving visual content, using captions and bounding boxes as symbolic representations of the image. They use COCO images to generate three types of instruction-following data, with a few manually designed examples used as seed examples in in-context-learning to query GPT-4.',\n",
              " 'The text discusses a method for creating a language-image instruction-following model. The model is trained using three types of data: conversation, detailed description, and complex reasoning. The conversation data involves a simulated conversation between an AI assistant and a person asking questions about an image. The detailed description data involves the AI generating a comprehensive description of an image. The complex reasoning data involves the AI answering in-depth reasoning questions about an image. The model was trained on 158K unique samples, with 58K in conversations, 23K in detailed descriptions, and 77K in complex reasoning. The model uses the Vicuna language model and the CLIP visual encoder. The visual features are converted into language embedding tokens using a trainable projection matrix. The text suggests that more sophisticated methods for connecting image and language representations could be explored in the future.',\n",
              " \"The text describes a two-stage training process for a multimodal instruction-following sequence. The first stage involves pre-training for feature alignment, where the CC3M dataset is filtered to 595K image-text pairs. These pairs are converted into instruction-following data using a naive expansion method. The image features are then aligned with the pre-trained language model's word embedding. The second stage involves fine-tuning the model end-to-end, keeping the visual encoder weights frozen and updating the pre-trained weights of the projection layer and language model. Two specific use case scenarios are considered: a multimodal chatbot and a Science QA. The chatbot is fine-tuned on 158K language-image instruction-following data, while the Science QA is trained on a large-scale multimodal science question dataset.\",\n",
              " \"The performance of LLaVA, a model designed for instruction-following and visual reasoning, was assessed through two primary experimental settings: a multimodal chatbot and the ScienceQA dataset. The model was trained using Vicuna's hyperparameters and pre-trained on the filtered CC-595K subset. It was then fine-tuned on the LLaVA-Instruct-158K dataset. A chatbot demo was developed to demonstrate LLaVA's image understanding and conversation abilities. The model's performance was compared with multimodal GPT-4, BLIP-2, and OpenFlamingo models. An example of a visual input was provided, where LLaVA was able to accurately describe an unusual scene of a man ironing clothes on the back of a moving vehicle.\",\n",
              " \"The text discusses the performance of a user model called LLaVA in comparison to other models like GPT-4, BLIP-2, and OpenFlamingo. LLaVA is noted for its ability to accurately follow user instructions and provide comprehensive responses, even when asked to describe unusual aspects of an image. Despite being trained on a small multimodal instruction-following dataset, LLaVA demonstrates similar reasoning results to multimodal GPT-4. In contrast, BLIP-2 and OpenFlamingo tend to describe the image rather than follow user instructions. A quantitative evaluation method is proposed to measure LLaVA's instruction-following capability on multimodal data, using GPT-4 to measure the quality of generated responses. The model's responses are evaluated on helpfulness, relevance, accuracy, and level of detail, with a higher score indicating better performance.\",\n",
              " 'Table 4 presents an ablation study on LLaVA-Bench (COCO) using different training data. The results are compared relative to a text-only GPT-4 model that uses actual image captions and bounding boxes as visual input. The GPT-4 model is prompted with answers from the tested model outputs and its own text-only responses, then asked to compare and rate both sets of responses, providing an explanation for its rating.',\n",
              " \"The text discusses the performance of the LLaVA model on two benchmarks: LLaVA-Bench (COCO) and LLaVA-Bench (In-the-Wild). On the COCO benchmark, the model's ability to follow instructions improved significantly with instruction tuning, and adding a small amount of detailed description and complex reasoning questions led to a considerable improvement in the model's overall capability. The best performance was achieved when all three types of data were used. On the In-the-Wild benchmark, LLaVA performed significantly better than other models, achieving an impressive 81.7% performance on complex reasoning questions. However, the model showed limitations in understanding complex semantics within images. The authors hope that LLaVA will serve as a baseline for future work in developing more capable language and multimodal models (LMMs).\",\n",
              " 'The text provides a detailed description of a meal at ICHIRAN Ramen, featuring a chashu ramen bowl with chili sauce, chopped scallions, two pieces of chashu, and nori. The meal also includes sides of orange spice (possibly garlic sauce), smoke-flavored stewed pork with chopped scallions, and a cup of matcha green tea. \\n\\nAdditionally, the text describes the contents of a refrigerator filled with various food items, including strawberries, baby carrots, sauce containers, a green plastic box, an unidentified plastic bag, milk, blueberries, and yogurts. The yogurts are from the brand Fage, with one being non-fat and another being blueberry-flavored. The contents of some containers are unknown. \\n\\nThe text also includes questions about the name of the restaurant and the brand of the blueberry-flavored yogurt, as well as requests for detailed descriptions of the photo and whether there is strawberry-flavored yogurt in the fridge.',\n",
              " 'The ScienceQA dataset contains 21k multimodal multiple choice questions across various subjects and topics. The dataset is divided into training, validation, and test splits. Several methods were used to analyze the data, including GPT-3.5 model, LLaMA-Adapter, and multimodal chain-of-thought (MM-CoT), the latter being the current state-of-the-art method. LLaVA achieved 90.92% accuracy, close to the state-of-the-art 91.68%. GPT-4 achieved 82.69% accuracy, a 7.52% gain compared to GPT-3.5. Two schemes were considered to combine outcomes from the model and GPT-4, one of which achieved a new state-of-the-art accuracy of 92.53%. Interestingly, GPT-4, which cannot process images, improved the overall performance on questions that have an image as context.',\n",
              " 'The table and text discuss the performance of a novel model ensembled with GPT-4 on the Science QA dataset. The model shows improved performance across all categories, setting a new standard of performance. The text also discusses the results of several design choices on the ScienceQA. Using the last layer feature from the CLIP vision encoder yielded slightly lower results than the feature before the last layer. The chain-of-thought strategy showed that an answer-first approach had the best accuracy, but a reasoning-first approach reached the same accuracy faster. Skipping pre-training and training from scratch resulted in a significant drop in accuracy, highlighting the importance of pre-training. Lastly, a smaller model size resulted in slightly lower accuracy, demonstrating the importance of model scale.',\n",
              " 'The paper discusses the effectiveness of visual instruction tuning, presenting an automatic pipeline to create language-image instruction-following data. This data is used to train LLaVA, a multimodal model designed to follow human intent and complete visual tasks. The model achieves state-of-the-art accuracy when fine-tuned on ScienceQA and exhibits excellent visual chat capabilities when fine-tuned on multimodal chat data. The paper also introduces the first benchmark for studying multimodal instruction-following capability. The work is primarily focused on real-life tasks and aims to inspire future research on building more capable multimodal models. The research was supported by NSF CAREER IIS2150012, and IITP grants funded by the Korea government.',\n",
              " 'The references include a variety of academic papers and online resources related to computer vision, language models, and machine learning. Topics covered include few-shot learning, visually-grounded navigation, image-text pre-training, and language model scaling. Some notable works include \"Flamingo: a visual language model for few-shot learning\", \"A general language assistant as a laboratory for alignment\", and \"Language models are few-shot learners\". There are also references to online resources such as Langchain and Computer Vision in the Wild.',\n",
              " 'The text provides a list of references from various authors, primarily focusing on the intersection of language and visual models. Topics include pre-training for vision-and-language navigation, aligning perception with language models, scaling language model instruction meta learning, visual prompt tuning, grounding language models to images, language-driven semantic segmentation, and multimodal foundation models. Several references also discuss specific software and tools such as Openclip, ELEVATER, and ChatGPT. The references span from 2014 to 2023, indicating a range of developments in the field over time.',\n",
              " 'The provided text includes a list of academic papers and preprints, primarily from the field of machine learning and artificial intelligence. The topics covered include instruction tuning with GPT-4, open-vocabulary image classification, transferable visual models from natural language supervision, text-to-text transformers, text-conditional image generation, high-resolution image synthesis, photorealistic text-to-image diffusion models, large-scale datasets for image-text models, visual inference via python execution, training home assistants, instruction-following models, foundation language models, generative image-to-text transformers, and aligning language models with self-generated instructions. The works are from various years, with the most recent being from 2023.',\n",
              " 'The text provides a list of academic papers and preprints, primarily from the field of computer science and artificial intelligence. The topics covered include visual foundation models, contrastive learning in image-text-label space, multimodal reasoning, text-to-image generation, computer vision, open-vocabulary segmentation and detection, fine-tuning of language models, pre-trained transformer language models, multi-modal reasoning in language models, region-based language-image pretraining, and generalized decoding for pixel, image, and language. The works are dated from 2021 to 2023.',\n",
              " \"LLaVA, a general-purpose visual assistant, has potential benefits and risks. Risks include malicious input, hallucination, biases, energy consumption, and evaluation complexities. To mitigate these, measures such as OpenAI Filter API and NSFW Filter are used to prevent harmful inputs. The model may generate outputs not based on facts or data, and biases can be transferred from base models. Energy consumption could become a concern when scaling up the model. Evaluating LLaVA's performance is challenging due to its dual nature. Despite these risks, the benefits of releasing LLaVA to the research community are believed to outweigh potential harm, as it allows for ongoing model improvement and encourages the development of new applications and research directions.\",\n",
              " \"The text discusses additional results of LLaVA, a language model. LLaVA exhibits similar behaviors to GPT-4, including generating HTML/JS/CSS code for an interactive website and providing detailed responses in a conversational style. It can also relate visual content to textual knowledge. Interestingly, LLaVA can understand visual content not covered in training, such as recognizing Elon Musk in different images and demonstrating impressive OCR ability. These results highlight LLaVA's potential in various applications and the need for further investigation into its emergent behaviors and generalization abilities. The goal is to improve LMMs by enhancing robustness, reducing biases, and improving the alignment and scope of learned vision-language representations.\",\n",
              " \"The text discusses the performance of different AI models (LLaVA, GPT-4, BLIP-2, and OpenFlamingo) in understanding and explaining a humorous meme. Both LLaVA and GPT-4 successfully explain the meme, with GPT-4 providing a more concise explanation. BLIP-2 and OpenFlamingo fail to follow the user's instructions. The text also presents an example of LLaVA generating HTML/JS code for an interactive website based on user inputs. The generated website works after fixing a minor error in the output. However, there is room for improvement in LLaVA’s output, such as splitting the joke and punchline into two rows and only revealing the punchline upon button click.\",\n",
              " 'The text provides a variety of meal options that can be prepared using the food items available in the refrigerator. These include a fruit salad, a smoothie, a milk-based smoothie, a baked casserole, scrambled eggs, a fresh fruit and juice mix, and homemade popsicles. A detailed recipe for a fruit salad using strawberries, blueberries, and carrots is also provided. The recipe includes steps for preparation and suggests serving the salad as a healthy snack or side dish.',\n",
              " \"The text discusses the precautions to take when visiting a serene location with a pier extending over a lake, surrounded by mountains. These include being aware of weather conditions, not overloading the pier, respecting wildlife, and adhering to fishing regulations. The text also describes a travel blog post about a trip to this location, highlighting the beautiful scenery, fishing experiences, and interactions with wildlife. The text then introduces LLaVA, a tool capable of recognizing visual content and providing detailed responses related to the visual content. Lastly, it describes an image of a romantic scene from the movie Titanic, and LLaVA's ability to relate the scene to the movie's plot.\",\n",
              " \"The text discusses LLaVA, a system that can recognize and describe various artworks and images, including the Mona Lisa by Leonardo da Vinci and a humorous painting of a dog. It can also identify individuals in photos, such as Elon Musk, even in humorous memes. Interestingly, Elon Musk never appears in the training data for LLaVA, indicating the system's ability to generalize to unseen visual concepts. The text also presents a question about the material of a rocking chair, with LLaVA correctly identifying it as made of wood and silk, while GPT-4 (text-only) incorrectly identifies it as only made of wood.\",\n",
              " 'The text discusses the use of GPT-4 as a judge to analyze the responses of two assistants regarding the material of a rocking chair. It concludes that the chair is likely made of wood. The text also provides details about the training process for their model. The model is pre-trained on the CC-595K subset for 1 epoch with a learning rate of 2e-3 and a batch size of 128, then fine-tuned on the LLaVA-Instruct-158K dataset for 3 epochs, with a learning rate of 2e-5 and a batch size of 32. The Adam optimizer is used with no weight decay and a cosine learning rate with a warmup ratio of 3%. FSDP and gradient checkpointing are used to save GPU memory, and BF16 and TF32 are enabled for a balance between speed and precision. The model is trained with 8 A100s, with pretraining and fine-tuning taking 4 and 10 hours respectively.',\n",
              " 'The text provides information about the assets and data related to a project. The assets, including source code, instruction-tuning data, and a proposed benchmark, are available on a GitHub repository. The data section provides instructions for both brief and detailed image descriptions, with each set of instructions offering the same meaning with natural language variance. The text also discusses the process of extracting noun-phrases from the CC3M dataset for image descriptions, and the filtering process to ensure good coverage of concepts. A comparison of noun-phrase statistics before and after filtering is shown in Figure 7.',\n",
              " \"The text provides instructions for an AI visual assistant to generate a conversation based on an image. The AI is to ask and answer questions about the image's visual content, including object types, counts, actions, locations, and relative positions. It should only ask questions with definite answers and include complex questions relevant to the image's content. The AI should provide detailed answers, including examples or reasoning steps. The text also includes an example of a prompt construction process for ChatGPT/GPT-4 and two types of context: captions describing a scene with people, luggage, and a vehicle, and boxes indicating the positions of various objects in the image.\",\n",
              " \"The text describes an image of a black SUV parked in an underground garage with three people trying to fit their luggage into the vehicle. The luggage includes two backpacks and two suitcases scattered around the vehicle. There's also a bicycle on the left side of the SUV. Other cars are parked around the SUV. The text also discusses the challenges the group might face in fitting all their luggage into the SUV without obstructing the driver's view or making the passengers uncomfortable. The text is presented in three response types: conversation, detailed description, and complex reasoning.\",\n",
              " \"The text describes a city experiencing a harsh winter, with significant snowfall causing various challenges. A red fire hydrant is partially covered in snow, posing potential safety concerns. The sidewalk near the hydrant has been recently plowed, indicating the city's efforts to keep walkways clear. However, the heavy snowfall is causing difficulties in transportation, business operations, and infrastructure maintenance. The text suggests that the city's residents and infrastructure are facing disruptions due to the severe winter conditions.\",\n",
              " \"The text is a conversation about a skier, discussing what they are doing and the type of skiing they might be engaged in. The skier is alone, skiing in snow-covered hills and standing near a trail sign. It's suggested that they might be cross-country skiing due to the presence of a trail sign and the foothills terrain. The text also provides detailed explanations of cross-country and downhill skiing, highlighting their differences in terms of techniques, equipment, and terrain preferences. Cross-country skiing is endurance-based, involving flat or gently sloping terrain, while downhill skiing focuses on speed and technique, navigating steeper slopes and challenging terrain.\",\n",
              " 'As an AI, I need more context to provide a summary. Please provide the table or text you want summarized.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add to vectorstore"
      ],
      "metadata": {
        "id": "Q5lE3UXL31kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "요약과 함께 Multi Vector Retriever를 사용하세요:\n",
        "\n",
        "*   InMemoryStore는 원본 텍스트와 표를 저장합니다.\n",
        "*   vectorstore는 임베딩된 요약을 저장합니다."
      ],
      "metadata": {
        "id": "rABGjT274F3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings(openai_api_key=OPENAI_KEY))\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [\n",
        "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
        "    for i, s in enumerate(table_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_tables)\n",
        "retriever.docstore.mset(list(zip(table_ids, tables)))"
      ],
      "metadata": {
        "id": "xPsf_VVe5KN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "rj3zKoVJ6xTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Prompt template\n",
        "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# LLM\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\", openai_api_key=OPENAI_KEY)\n",
        "\n",
        "# RAG pipeline\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "8pLBeksu6v2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What is the performance of the GPT-4 model on the Science QA dataset?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "MBTCoKGF6v0c",
        "outputId": "139e94e6-479e-4587-8f68-a24b1b3e373f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GPT-4 model achieves 82.69% accuracy on the Science QA dataset. When used in combination with the LLaVA model, the accuracy increases. Specifically, when GPT-4 is used as a complement (providing answers when GPT-4 fails), the accuracy is 90.97%. When GPT-4 is used as a judge (providing a final answer when GPT-4 and LLaVA produce different answers), the accuracy further increases to 92.53%.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4lkdzNl7ZiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TbiUvl_6vxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWcflX1j31Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "su6_Wuvf_uu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1W7kgWHf_Iw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}